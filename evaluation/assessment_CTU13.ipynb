{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from supportFunctions import *\n",
    "\n",
    "root_folder = \"..\\\\data\\\\CTU13\\\\flows\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PARAMETERS #####\n",
    "\n",
    "## Generic parameters\n",
    "temporal = False # used to determine if this evaluation assumes a \"temporal\" dependency among its samples\n",
    "base_clf = 'dt' # name of the classifier used for this \"run\". Available names: ['dt', 'rf', 'hgb', 'lr']. You can add more by editing the supportFunctions file\n",
    "test_size = 0.2 # proportion of the dataset used for testing. We always kept it fixed to 0.2 for our paper\n",
    "train_size = 100 # proportion of the REMAINING data that are used for training (if >1, then it will take that exact amount). To reproduce the results of the paper, use: 100 (for \"limited\" training data) or 0.2 or 0.5 or 0.99 (for scarce, moderate, abundant training data, respectively) \n",
    "agreement = 0.5 # from 0 to 1. Proportion of classifiers that must agree on an attack (for the ensemble). This is fixed in our paper.\n",
    "max_size = 500000 ## maximum amount of samples to include when creating the initial dataframes. This is fixed in our paper\n",
    "max_size_atk = int(max_size / 3) # maximum amount of malicious samples per class. This is fixed in our paper\n",
    "\n",
    "## Adversarial Attacks parameters\n",
    "atk_intensity = 1 # 10 packets of 100B each over 10 seconds. The response is of one pkt of 1 byte\n",
    "byt_intensity = atk_intensity * 102 # \n",
    "pkt_intensity = atk_intensity * 10 # \n",
    "dur_intensity = atk_intensity * 10 # consider duration in SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading input data\n",
    "\n",
    "malicious_folder = root_folder + \"malicious/\"\n",
    "\n",
    "benign_file = root_folder + \"benign.csv\"\n",
    "benign_df = pd.read_csv(benign_file, header='infer', index_col=0)\n",
    "benign_df = benign_df.sample(min(max_size, len(benign_df)))\n",
    "#sort by timestamp\n",
    "if temporal == True:\n",
    "    pass # it should be already sorted\n",
    "    \n",
    "benign_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "attack_names = [\"neris\", \"rbot\", \"nsis\", \"virut\", \"donbot\", \"murlo\"]\n",
    "\n",
    "neris_file = malicious_folder + \"neris.csv\"\n",
    "rbot_file = malicious_folder + \"rbot.csv\"\n",
    "nsis_file = malicious_folder + \"nsis.csv\"\n",
    "virut_file = malicious_folder + \"virut.csv\"\n",
    "donbot_file = malicious_folder + \"donbot.csv\"\n",
    "sogou_file = malicious_folder + \"sogou.csv\" #only 63 samples: discard\n",
    "murlo_file = malicious_folder + \"murlo.csv\"\n",
    "\n",
    "\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_df = pd.read_csv({a}_file, header='infer', index_col=0)\")\n",
    "    exec(f\"{a}_df = {a}_df.sample(min(max_size_atk, len({a}_df)))\")\n",
    "    # sort by timestamp\n",
    "    if temporal == True:\n",
    "        pass # it should be already sorted\n",
    "    exec(f\"{a}_df.reset_index(inplace=True, drop=True)\")\n",
    "    exec(f\"{a}_df['Label'] = a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining Train and Test sets for each class\n",
    "\n",
    "df_list = [benign_df]\n",
    "for a in attack_names:\n",
    "    exec(f\"df_list.append({a}_df)\")\n",
    "\n",
    "if temporal == True:\n",
    "    for dummy_df in df_list:\n",
    "        if train_size <=1:\n",
    "            train_threshold = int(((1-test_size) * train_size) * len(dummy_df))\n",
    "        else:\n",
    "            train_threshold = int(100)\n",
    "        test_threshold = len(dummy_df) - int(test_size * len(dummy_df))\n",
    "        dummy_df['index'] = dummy_df.index\n",
    "        dummy_df['is_test'] = np.where(dummy_df['index'] >= test_threshold , True, False)\n",
    "        dummy_df['is_train'] = np.where(dummy_df['index'] <= train_threshold , True, False)\n",
    "else:\n",
    "    for dummy_df in df_list:\n",
    "        if train_size <= 1:\n",
    "            train_threshold = test_size + (1-test_size)*train_size\n",
    "        else:\n",
    "            train_threshold = test_size + ((train_size * 100) / (len(dummy_df)) / 100)       \n",
    "        dummy_df['seed'] = (np.random.uniform(0,1,len(dummy_df)))\n",
    "        dummy_df['is_test'] = np.where(dummy_df['seed'] <= test_size, True, False)\n",
    "        dummy_df['is_train'] = np.where((dummy_df['seed'] <= train_threshold) & (dummy_df['is_test']==False), True, False)\n",
    "\n",
    "# get all together\n",
    "all_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 0 & \\textit{Benign} & 500000 & \\\\ \\cline{2-4}\n",
      "& 1 & \\textit{neris} & 166666 \\\\ \\cline{2-4}\n",
      "& 2 & \\textit{rbot} & 143918 \\\\ \\cline{2-4}\n",
      "& 3 & \\textit{nsis} & 2168 \\\\ \\cline{2-4}\n",
      "& 4 & \\textit{virut} & 40904 \\\\ \\cline{2-4}\n",
      "& 5 & \\textit{donbot} & 4630 \\\\ \\cline{2-4}\n",
      "& 6 & \\textit{murlo} & 6127 \\\\ \\cline{2-4}\n"
     ]
    }
   ],
   "source": [
    "def handle_categorical(df):\n",
    "    ## Handling categorical data\n",
    "    df_dummy = df.copy(deep=True)\n",
    "    df_dummy['Nature'] = np.where(df_dummy['Label'].str.contains('BENIGN'),0,1)\n",
    "    \n",
    "    for column_name in df_dummy.columns:\n",
    "        if column_name == ('SrcPort_type'):\n",
    "            df_dummy[column_name+\"-f\"] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('DstPort_type'):\n",
    "            df_dummy[column_name+\"-f\"] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('Proto'):\n",
    "            df_dummy[column_name+\"-f\"] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('State'):\n",
    "            df_dummy[column_name+\"-f\"] = pd.factorize(df_dummy[column_name])[0]\n",
    "        else:\n",
    "            pass\n",
    "    return df_dummy\n",
    "\n",
    "all_df = handle_categorical(all_df)\n",
    "all_df['Label_cat'] = pd.factorize(all_df['Label'])[0]\n",
    "\n",
    "all_df['int2int'] = np.where( ((all_df['SrcIP_internal']==True) & (all_df['DstIP_internal']==True)), True, False)\n",
    "\n",
    "all_train, all_test = all_df[all_df['is_train']==True], all_df[all_df['is_test']==True]\n",
    "\n",
    "### SPLITTING ALL BACK ####\n",
    "benign_df = all_df[all_df['Label']=='BENIGN']\n",
    "benign_train = benign_df[benign_df['is_train']==True]\n",
    "benign_test = benign_df[benign_df['is_test']==True]\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_df = all_df[all_df['Label']=='{a}']\")\n",
    "    \n",
    "malicious_df = all_df[all_df['Label']!='BENIGN']\n",
    "malicious_train, malicious_test = malicious_df[malicious_df['is_train']==True], malicious_df[malicious_df['is_test']==True]\n",
    "\n",
    "print(\"& 0 & \\\\textit{{Benign}} & {} & \\\\\\\\ \\\\cline{{2-4}}\".format(len(benign_df)))\n",
    "\n",
    "\n",
    "for i,a in enumerate(attack_names):\n",
    "    exec(f\"print('& {i+1} & \\\\\\\\textit{{{{{a}}}}} & {{}} \\\\\\\\\\\\\\\\ \\\\\\\\cline{{{{2-4}}}}'.format(len({a}_df)))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature sets\n",
    "\n",
    "# the following is the \"complete\" feature set\n",
    "\n",
    "features = ['sTos', 'dTos', 'SrcWin', 'DstWin',\n",
    "       'sHops', 'dHops', 'sTtl', 'dTtl', 'TcpRtt',\n",
    "       'SynAck', 'AckDat', 'SrcPkts', 'DstPkts', 'SrcBytes', 'DstBytes',\n",
    "       'SAppBytes', 'DAppBytes', 'Dur', 'TotPkts', 'TotBytes', 'TotAppByte',\n",
    "       'Rate', 'SrcRate', 'DstRate', 'DstIP_internal',\n",
    "       'SrcIP_internal', 'Proto-f',\n",
    "       'State-f', 'SrcPort_type-f', 'DstPort_type-f'\n",
    "       ]\n",
    "\n",
    "# this is for the \"essential\" feature set\n",
    "small_features = ['sTos', 'dTos',       \n",
    "       'SrcPkts', 'DstPkts', 'SrcBytes', 'DstBytes',\n",
    "       'Dur', 'TotPkts', 'TotBytes',\n",
    "        'Proto-f', \n",
    "       'State-f', 'SrcPort_type-f', 'DstPort_type-f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19694\n"
     ]
    }
   ],
   "source": [
    "# creating adversarial dataset\n",
    "mal_base = malicious_df[((malicious_df['Proto']=='udp')) & (malicious_df['is_test']==True)]\n",
    "print(len(mal_base))\n",
    "mal_adv = mal_base.copy(deep=True)\n",
    "# support\n",
    "\n",
    "# attacking\n",
    "max_dur = mal_adv['Dur'].max()\n",
    "min_dur = mal_adv['Dur'].min()\n",
    "mal_adv['Dur'] = mal_adv['Dur'] + dur_intensity\n",
    "mal_adv['Dur'] = np.where(mal_adv['Dur'] > max_dur, max_dur, mal_adv['Dur'])\n",
    "mal_adv['Dur'] = np.where(mal_adv['Dur'] < min_dur, min_dur, mal_adv['Dur'])\n",
    "\n",
    "mal_adv['SrcBytes'] = mal_adv['SrcBytes'] + byt_intensity\n",
    "mal_adv['SrcPkts'] = mal_adv['SrcPkts'] + pkt_intensity\n",
    "mal_adv['DstBytes'] = mal_adv['DstBytes'] + byt_intensity / 1024\n",
    "mal_adv['DstPkts'] = mal_adv['DstPkts'] + pkt_intensity\n",
    "\n",
    "mal_adv['TotPkts'] = mal_adv['SrcPkts'] + mal_adv['DstPkts']\n",
    "mal_adv['TotBytes'] = mal_adv['SrcBytes'] + mal_adv['DstBytes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROM NOW ON, THE CODE IS ALWAYS THE SAME FOR EVERY DATASET!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Assessment on \"Complete\" feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BINARY CLASSIFIER (Complete features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing bin......done! Training time: 0.006977s\tInference time: 0.034920s\n",
      "Total Misclassifications: 7046 out of 173220 (Recall: 0.991456\tFPR: 0.064165)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93649</td>\n",
       "      <td>6421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>625</td>\n",
       "      <td>72525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     93649   6421\n",
       "1       625  72525"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bClf, bPred, bResult = develop_clf(all_train, all_test, features, clf_name='bin', label='Nature', clf_type=base_clf, verbose=1)\n",
    "\n",
    "if (bResult.acc == 0):\n",
    "    bErr = int(len(all_test) * (1-bResult.acc_multi))\n",
    "else:\n",
    "    bErr = int(len(all_test) * (1-bResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(bErr, len(all_test), bResult.rec, bResult.fpr))\n",
    "pd.crosstab(all_test['Nature'], bPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-CLASS CLASSIFIER - cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing mc......done! Training time: 0.005978s\tInference time: 0.019933s\n",
      "Total Misclassifications: 22153 out of 73150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17739</td>\n",
       "      <td>1051</td>\n",
       "      <td>3058</td>\n",
       "      <td>6433</td>\n",
       "      <td>3258</td>\n",
       "      <td>2058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>944</td>\n",
       "      <td>25485</td>\n",
       "      <td>68</td>\n",
       "      <td>1356</td>\n",
       "      <td>806</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>353</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1583</td>\n",
       "      <td>225</td>\n",
       "      <td>266</td>\n",
       "      <td>5710</td>\n",
       "      <td>289</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>52</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>679</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>105</td>\n",
       "      <td>1030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      1      2     3     4     5     6\n",
       "True                                      \n",
       "1     17739   1051  3058  6433  3258  2058\n",
       "2       944  25485    68  1356   806   191\n",
       "3        23      0   353    16     0    18\n",
       "4      1583    225   266  5710   289    71\n",
       "5        52     39     2    55   679   114\n",
       "6        25      0    39     9   105  1030"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the classifier that analyzes ONLY the malicious samples that \"receives\" from the initial binary classifier\n",
    "# It is trained on the same training set---but without using the benign samples\n",
    "# It is tested on the malicious samples in the test set that are flagged as malicious by the binary classifier\n",
    "\n",
    "\n",
    "mcClf, mcPred, mcResult = develop_clf(malicious_train, malicious_test, features, clf_name='mc', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "mcErr = int(len(malicious_test) * (1-mcResult.acc_multi))\n",
    "print(\"Total Misclassifications: {} out of {}\".format(mcErr, len(malicious_test)))\n",
    "pd.crosstab(malicious_test['Label_cat'], mcPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications (among the malicious samples): 21856 out of 72525\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17514</td>\n",
       "      <td>1051</td>\n",
       "      <td>2918</td>\n",
       "      <td>6398</td>\n",
       "      <td>3249</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>944</td>\n",
       "      <td>25416</td>\n",
       "      <td>59</td>\n",
       "      <td>1351</td>\n",
       "      <td>804</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>337</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1560</td>\n",
       "      <td>225</td>\n",
       "      <td>241</td>\n",
       "      <td>5694</td>\n",
       "      <td>288</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>52</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>678</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>105</td>\n",
       "      <td>1030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      1      2     3     4     5     6\n",
       "True                                      \n",
       "1     17514   1051  2918  6398  3249  2023\n",
       "2       944  25416    59  1351   804   191\n",
       "3        18      0   337    12     0    15\n",
       "4      1560    225   241  5694   288    69\n",
       "5        52     39     2    55   678   114\n",
       "6        25      0    39     9   105  1030"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We select the samples flagged as malicious by the initial classifier.\n",
    "# Of course, samples flagged as malicious that are NOT actually malicious will always be misclassified\n",
    "\n",
    "all_test['bPred'] = bPred\n",
    "mc_test = all_test[(all_test['bPred']==1) & (all_test['Nature']==1)]\n",
    "if (len(mc_test)==0):\n",
    "    # in this case, this classifier receives nothing\n",
    "    print(\"There is no malicious sample flagged as malicious to analyze!\")\n",
    "\n",
    "mcPred_m = mcClf.predict(mc_test[features])\n",
    "mcResult.acc_multic = accuracy_score(mc_test['Label_cat'], mcPred_m, normalize=True, sample_weight=None)\n",
    "mcErr_m = int((1-mcResult.acc_multic) * len(mc_test))\n",
    "print(\"Total Misclassifications (among the malicious samples): {} out of {}\".format(mcErr_m, len(mc_test)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.crosstab(mc_test['Label_cat'], mcPred_m, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This classifier also analyzed 6421 benign samples that were incorrectly labelled as 'malicious' by the binary classifier\n",
      "Hence, this classifier was tested on 78946 samples, of which 28277 have been misclassified\n"
     ]
    }
   ],
   "source": [
    "## Note: We also accounted for the false positives of the first binary classifier (all of which have been considered as misclassifications)\n",
    "bin_falsePositives = int(bResult.fpr * len(benign_test))\n",
    "print(\"This classifier also analyzed {} benign samples that were incorrectly labelled as 'malicious' by the binary classifier\".format(bin_falsePositives))\n",
    "\n",
    "print(\"Hence, this classifier was tested on {} samples, of which {} have been misclassified\".format(len(mc_test)+bin_falsePositives, bin_falsePositives+mcErr_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-CLASS CLASSIFIER - stand-alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing m......done! Training time: 0.007973s\tInference time: 0.040874s\n",
      "Total Misclassifications: 29816 out of 173220\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92819</td>\n",
       "      <td>936</td>\n",
       "      <td>450</td>\n",
       "      <td>2993</td>\n",
       "      <td>1067</td>\n",
       "      <td>1274</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>418</td>\n",
       "      <td>17212</td>\n",
       "      <td>1073</td>\n",
       "      <td>2761</td>\n",
       "      <td>6522</td>\n",
       "      <td>3560</td>\n",
       "      <td>2051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>917</td>\n",
       "      <td>25466</td>\n",
       "      <td>141</td>\n",
       "      <td>1266</td>\n",
       "      <td>870</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>321</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>1370</td>\n",
       "      <td>224</td>\n",
       "      <td>299</td>\n",
       "      <td>5851</td>\n",
       "      <td>322</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>700</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>105</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1      2     3     4     5     6\n",
       "True                                             \n",
       "0     92819    936    450  2993  1067  1274   531\n",
       "1       418  17212   1073  2761  6522  3560  2051\n",
       "2        53    917  25466   141  1266   870   137\n",
       "3        23     30      0   321    20     0    16\n",
       "4        28   1370    224   299  5851   322    50\n",
       "5         0     52     31     2    48   700   108\n",
       "6         3     25      0    32     8   105  1035"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first assess its multiclassification performance, and then its binary classification performance\n",
    "\n",
    "mClf, mPred, mResult = develop_clf(all_train, all_test, features, clf_name='m', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "mErr = int(len(all_test) * (1-mResult.acc_multi))\n",
    "print(\"Total Misclassifications: {} out of {}\".format(mErr, len(all_test)))\n",
    "mResult.ctab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 7775 out of 173220 (Recall: 0.992823\tFPR: 0.072459)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92819</td>\n",
       "      <td>7251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>525</td>\n",
       "      <td>72625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     92819   7251\n",
       "1       525  72625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the binary classification performance, we use the previous predictions\n",
    "mPred_bin = np.copy(mPred)\n",
    "mPred_bin[mPred_bin > 0] = 1\n",
    "mResult.bin_results(all_test['Nature'], mPred_bin)\n",
    "mErr_bin = int(len(all_test) * (1-mResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(mErr_bin, len(all_test), mResult.rec, mResult.fpr))\n",
    "mResult.ctab_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSEMBLE CLASSIFIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \"individual\" binary classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing neris......done! Training time: 0.003979s\tInference time: 0.024917s\n",
      "Testing neris...\n",
      "...done! \tInference time: 0.031893s\n",
      "Training and testing rbot......done! Training time: 0.003990s\tInference time: 0.024913s\n",
      "Testing rbot...\n",
      "...done! \tInference time: 0.029908s\n",
      "Training and testing nsis......done! Training time: 0.005980s\tInference time: 0.022838s\n",
      "Testing nsis...\n",
      "...done! \tInference time: 0.029900s\n",
      "Training and testing virut......done! Training time: 0.004981s\tInference time: 0.020931s\n",
      "Testing virut...\n",
      "...done! \tInference time: 0.029897s\n",
      "Training and testing donbot......done! Training time: 0.004154s\tInference time: 0.016979s\n",
      "Testing donbot...\n",
      "...done! \tInference time: 0.030894s\n",
      "Training and testing murlo......done! Training time: 0.004014s\tInference time: 0.019442s\n",
      "Testing murlo...\n",
      "...done! \tInference time: 0.029903s\n",
      "Total training time: 0.027098s\t AvgFPR: 0.030910\t AvgTPR: 0.983882\tTotal inference time: 0.182396s (fake: 0.130020s)\n"
     ]
    }
   ],
   "source": [
    "ensemble_df = pd.DataFrame()\n",
    "\n",
    "benign_train = benign_df[benign_df['is_train']==True]\n",
    "benign_test = benign_df[benign_df['is_test']==True]\n",
    "\n",
    "\n",
    "ens_time = 0\n",
    "ens_avgFPR = 0\n",
    "tot_TP = 0\n",
    "tot_P = 0\n",
    "ens_infer_time = 0\n",
    "fakeEns_infer_time = 0\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_train = {a}_df[{a}_df['is_train']==True]\")\n",
    "    exec(f\"{a}_test = {a}_df[{a}_df['is_test']==True]\")\n",
    "\n",
    "    exec(f\"train = pd.concat([benign_train, {a}_train])\")\n",
    "    exec(f\"test = pd.concat([benign_test, {a}_test])\")\n",
    "    \n",
    "    # We first train a classifier only on \"benign\" or on malicious samples of a specific attack. \n",
    "    # Afterwards, we immediately test it on a test-set having ONLY malicious samples of this specific attack\n",
    "    # Note: such \"testing\" is redundant, because it assumes that the classifier only receives the samples of the attack it is trained on!\n",
    "    exec(f\"{a}Clf, {a}Pred, {a}Result = develop_clf(train, test, features, clf_name='{a}', clf_type=base_clf, verbose=1)\")\n",
    "    exec(f\"fakeEns_infer_time += {a}Result.infer_time\")\n",
    "    # We now test the specific classifier on the ENTIRE test-set, thereby allowing to assess its performance also against malicious samples of different attacks\n",
    "    exec(f\"{a}_allPred, {a}_allResults, {a}Result.infer_time = evaluate_clf({a}Clf, all_test, features, clf_name='{a}', time={a}Result.time, verbose=1)\")\n",
    "\n",
    "    exec(f\"ensemble_df['{a}'] = {a}_allPred\")\n",
    "\n",
    "    exec(f\"tot_TP += ({a}Result.rec * len({a}_test))\")\n",
    "    exec(f\"tot_P += len({a}_test)\")\n",
    "\n",
    "\n",
    "    exec(f\"ens_avgFPR += {a}Result.fpr\")\n",
    "    exec(f\"ens_time += {a}Result.time\")\n",
    "    exec(f\"ens_infer_time +={a}Result.infer_time\")\n",
    "\n",
    "ens_avgFPR = ens_avgFPR / len(attack_names)\n",
    "ens_avgREC = tot_TP/tot_P\n",
    "\n",
    "print(\"Total training time: {:5f}s\\t AvgFPR: {:5f}\\t AvgTPR: {:5f}\\tTotal inference time: {:5f}s (fake: {:5f}s)\".\n",
    "      format(ens_time, ens_avgFPR, ens_avgREC, ens_infer_time, fakeEns_infer_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble (real assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we measure the combined performance of the entire ensemble\n",
    "# This is done with a logical or, or for majority voting (regulated by the \"agreement\" variable)\n",
    "\n",
    "ensemble_df[\"sum\"] = ensemble_df.sum(axis=1)\n",
    "#calculating \n",
    "ensemble_df[\"LOR\"] = (ensemble_df[\"sum\"]>0)\n",
    "\n",
    "#Appending Ground Truth\n",
    "temp = all_test['Nature'] #> 0)\n",
    "ensemble_df['True'] = ((temp.reset_index(drop=True)) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Logical OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 11754 out of 173220 (Recall: 0.998988\tFPR: 0.116718)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>88390</td>\n",
       "      <td>11680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>74</td>\n",
       "      <td>73076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  88390  11680\n",
       "True      74  73076"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enslorResult = Result(ensemble_df['True'], ensemble_df['LOR'], ens_time, ens_infer_time)\n",
    "enslorErr= int(len(all_test) * (1-enslorResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(enslorErr, len(all_test), enslorResult.rec, enslorResult.fpr))\n",
    "enslorResult.ctab_bin # you can also try with enslorResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting: at least 3 out of 6 classifiers must agree that a sample is malicious.\n",
      "Total Misclassifications: 3673 out of 173220 (Recall: 0.974532\tFPR: 0.018097)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>98259</td>\n",
       "      <td>1811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1863</td>\n",
       "      <td>71287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  98259   1811\n",
       "True    1863  71287"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_agree = math.ceil(agreement * len(attack_names))\n",
    "print(\"Voting: at least {} out of {} classifiers must agree that a sample is malicious.\".format(min_agree, len(attack_names)))\n",
    "ensemble_df[\"MAJV\"] = (ensemble_df[\"sum\"]>=min_agree)\n",
    "ensvotResult = Result(ensemble_df['True'], ensemble_df['MAJV'], ens_time, ens_infer_time)\n",
    "ensvotErr = int(len(all_test) * (1-ensvotResult.acc))\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(ensvotErr, len(all_test), ensvotResult.rec, ensvotResult.fpr))\n",
    "ensvotResult.ctab_bin # you can also try with ensvotResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Stacked Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 10226 out of 173220 (Recall: 0.996364\tFPR: 0.099540)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90109</td>\n",
       "      <td>9961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266</td>\n",
       "      <td>72884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     90109   9961\n",
       "1       266  72884"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "clf_list = []\n",
    "for a in attack_names:\n",
    "    exec(f\"clf_list.append({a}Clf)\")\n",
    "\n",
    "meta = choose_clf(clf_type=base_clf)\n",
    "sClf = StackingClassifier(classifiers=clf_list, meta_classifier=meta, fit_base_estimators=False, use_probas = False)\n",
    "s_timeStart = time.time()\n",
    "sClf.fit(all_train[features], all_train['Nature'])\n",
    "s_time = time.time() - s_timeStart + ens_time\n",
    "s_timeStart = time.time()\n",
    "sPred = sClf.predict(all_test[features])\n",
    "s_infer_time = time.time()-s_timeStart\n",
    "sResult = Result(all_test['Nature'], sPred, s_time, s_infer_time)\n",
    "if sResult.acc < sResult.acc_multi:\n",
    "    sResult.acc = sResult.acc_multi\n",
    "sErr = int(len(all_test) * (1-sResult.acc))\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sErr, len(all_test), sResult.rec, sResult.fpr))\n",
    "\n",
    "sResult.ctab_bin # you can also try with sResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open World Assessment: One attack against all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
      "      Binary CLF: TPR=0.937699\tFPR=0.058364\n",
      "      Multiclass (binarized) CLF: TPR=0.900330\tFPR=0.069526\n",
      "      EnsLOR CLF: TPR=0.972680\tFPR=0.104795\n",
      "      EnsVOT CLF: TPR=0.614395\tFPR=0.011307\n",
      "      EnsSTK CLF: TPR=0.967791\tFPR=0.092237\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "### The following code is a mixture of everything described insofar.\n",
    "### We are only focused on TPR and FPR here. We do not care about accuracy, adversarial robustness, or runtime.\n",
    "### These experiments are also done only on the \"Complete\" feature set\n",
    "\n",
    "\n",
    "oaac_bin_rec = 0\n",
    "oaac_bin_fpr = 0\n",
    "oaac_multi_rec = 0\n",
    "oaac_multi_fpr = 0\n",
    "oaac_enslor_rec = 0\n",
    "oaac_enslor_fpr = 0\n",
    "oaac_ensvot_rec = 0\n",
    "oaac_ensvot_fpr = 0\n",
    "oaac_ensstk_rec = 0\n",
    "oaac_ensstk_fpr = 0\n",
    "\n",
    "for u in attack_names: # u is the unknown attack\n",
    "    #print(u) # this is the unknown attack\n",
    "    #exec(f\"{u}_test = {u}_df[{u}_df['is_train']==False]\") # create test set by putting the \"test\" samples of u\n",
    "    exec(f\"{u}_test = pd.concat([benign_test, {u}_test])\") # add to the test set the \"benign\" test samples\n",
    "    exec(f\"{u}_train = benign_df[benign_df['is_train']==True]\") # compose the \"training\" set: start by putting the benign \"training\" samples\n",
    "    for a in attack_names: \n",
    "        # for every attack that is not u, add its training samples to the training set of u\n",
    "        if a==u:\n",
    "            continue\n",
    "        exec(f\"{u}_train = pd.concat([{u}_train, {a}_df[{a}_df['is_train']==True]])\")\n",
    "\n",
    "\n",
    "    # We have created the training and testing set. Now we must train and test a binary classifier by following the standard procedure\n",
    "    ########## BINARY CLASSIFIER ##########\n",
    "    exec(f\"{u}_oaac_bClf, {u}_oaac_bPred, {u}_oaac_bResult = develop_clf({u}_train, {u}_test, features, clf_name='{u}_oaac_bin', label='Nature', clf_type=base_clf)\")\n",
    "\n",
    "\n",
    "    ########## Multiclass CLASSIFIER ########## --> Train, then test only on binary\n",
    "    exec(f\"{u}_oaac_mClf, {u}_oaac_mPred, {u}_oaac_mResult = develop_clf({u}_train, {u}_test, features, clf_name='{u}_oaac_multi', label='Label_cat', clf_type=base_clf)\")\n",
    "    exec(f\"{u}_oaac_mPred_bin = np.copy({u}_oaac_mPred)\")\n",
    "    exec(f\"{u}_oaac_mPred_bin[{u}_oaac_mPred_bin > 0] = 1\")\n",
    "    exec(f\"{u}_oaac_mResult.bin_results({u}_test['Nature'], {u}_oaac_mPred_bin)\")\n",
    "\n",
    "\n",
    "    ######### Ensemble ##############\n",
    "    # send the samples in TEST to all the classifiers of the ensemble (which are already trained), aside from the one focusing on u\n",
    "    exec(f\"{u}_oaac_ens_df = pd.DataFrame()\")\n",
    "    for a in attack_names:    \n",
    "        if a==u:\n",
    "                continue\n",
    "        exec(f\"{a}_{u}Pred, {a}_{u}Results, {a}_{u}_infer_time = evaluate_clf({a}Clf, {u}_test, features, clf_name='{a}_{u}', time={a}Result.time)\")\n",
    "        exec(f\"{u}_oaac_ens_df['{a}'] = {a}_{u}Pred\")\n",
    "\n",
    "    # now we have the dataframe with all the predictions, let's see the aggregate results\n",
    "    exec(f\"{u}_oaac_ens_df['sum'] = {u}_oaac_ens_df.sum(axis=1)\")\n",
    "    exec(f\"{u}_oaac_ens_df['LOR'] = ({u}_oaac_ens_df['sum']>0)\")\n",
    "    exec(f\"temp = {u}_test['Nature'] #> 0)\")\n",
    "    exec(f\"{u}_oaac_ens_df['True'] = ((temp.reset_index(drop=True)) > 0)\")\n",
    "    exec(f\"{u}_oaac_enslorResult = Result({u}_oaac_ens_df['True'], {u}_oaac_ens_df['LOR'], (ens_time-{u}Result.time), (ens_infer_time-{u}Result.infer_time))\")\n",
    "\n",
    "    # now we consider the majority voting of the ensemble\n",
    "    exec(f\"{u}_oaac_ens_df['MAJV'] = ({u}_oaac_ens_df['sum']>=min_agree)\")\n",
    "    exec(f\"{u}_oaac_ensvotResult = Result({u}_oaac_ens_df['True'], {u}_oaac_ens_df['MAJV'], (ens_time-{u}Result.time), (ens_infer_time-{u}Result.infer_time))\")\n",
    "\n",
    "    # finally, let's use the stacking ensemble\n",
    "    exec(f\"{u}_clf_list = []\")\n",
    "    for a in attack_names:\n",
    "        if a==u:\n",
    "                continue\n",
    "        exec(f\"{u}_clf_list.append({a}Clf)\")\n",
    "    exec(f\"{u}_oaac_sClf = StackingClassifier(classifiers={u}_clf_list, meta_classifier=meta, fit_base_estimators=False, use_probas = False)\")\n",
    "    exec(f\"{u}_oaac_sClf.fit({u}_train[features], {u}_train['Nature'])\")\n",
    "    exec(f\"{u}_oaac_sPred = {u}_oaac_sClf.predict({u}_test[features])\")\n",
    "    exec(f\"{u}_oaac_sResult = Result({u}_test['Nature'], {u}_oaac_sPred, (ens_time-{u}Result.time), (ens_infer_time-{u}Result.infer_time))\")\n",
    "\n",
    "    # Updating results\n",
    "    exec(f\"oaac_bin_rec += {u}_oaac_bResult.rec\")\n",
    "    exec(f\"oaac_bin_fpr += {u}_oaac_bResult.fpr\")\n",
    "    exec(f\"oaac_multi_rec += {u}_oaac_mResult.rec\")\n",
    "    exec(f\"oaac_multi_fpr += {u}_oaac_mResult.fpr\")\n",
    "    exec(f\"oaac_enslor_rec += {u}_oaac_enslorResult.rec\")\n",
    "    exec(f\"oaac_enslor_fpr += {u}_oaac_enslorResult.fpr\")\n",
    "    exec(f\"oaac_ensvot_rec += {u}_oaac_ensvotResult.rec\")\n",
    "    exec(f\"oaac_ensvot_fpr += {u}_oaac_ensvotResult.fpr\")\n",
    "    exec(f\"oaac_ensstk_rec += {u}_oaac_sResult.rec\")\n",
    "    exec(f\"oaac_ensstk_fpr += {u}_oaac_sResult.fpr\")\n",
    "\n",
    "# Finalizing averages\n",
    "oaac_bin_rec /= len(attack_names)\n",
    "oaac_bin_fpr /= len(attack_names)\n",
    "oaac_multi_rec /= len(attack_names)\n",
    "oaac_multi_fpr /= len(attack_names)\n",
    "oaac_enslor_rec /= len(attack_names)\n",
    "oaac_enslor_fpr /= len(attack_names)\n",
    "oaac_ensvot_rec /= len(attack_names)\n",
    "oaac_ensvot_fpr /= len(attack_names)\n",
    "oaac_ensstk_rec /= len(attack_names)\n",
    "oaac_ensstk_fpr /= len(attack_names)\n",
    "\n",
    "\n",
    "print('''Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
    "      Binary CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      Multiclass (binarized) CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      EnsLOR CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      EnsVOT CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      EnsSTK CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      '''.format(oaac_bin_rec, oaac_bin_fpr,\n",
    "                 oaac_multi_rec, oaac_multi_fpr,\n",
    "                 oaac_enslor_rec, oaac_enslor_fpr,\n",
    "                 oaac_ensvot_rec, oaac_ensvot_fpr,\n",
    "                 oaac_ensstk_rec, oaac_ensstk_fpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment on Essential feature set (and Adversarial attacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BINARY CLASSIFIER (Essential features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing adv_bin......done! Training time: 0.004975s\tInference time: 0.024429s\n",
      "Total Misclassifications: 23222 out of 173220 (Recall: 0.966658\tFPR: 0.207685)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79287</td>\n",
       "      <td>20783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2439</td>\n",
       "      <td>70711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     79287  20783\n",
       "1      2439  70711"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_bClf, sma_bPred, sma_bResult = develop_clf(all_train, all_test, small_features, clf_name='adv_bin', label='Nature', clf_type=base_clf, verbose=1)\n",
    "sma_bErr = int(len(all_test) * (1-sma_bResult.acc))\n",
    "if (sma_bResult.acc == 0):\n",
    "    sma_bErr = int(len(all_test) * (1-sma_bResult.acc_multi))\n",
    "else:\n",
    "    sma_bErr = int(len(all_test) * (1-sma_bResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_bErr, len(all_test), sma_bResult.rec, sma_bResult.fpr))\n",
    "pd.crosstab(all_test['Nature'], sma_bPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial Attack against Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.958\n",
      "Adversarial Recall (attack): 0.972\n"
     ]
    }
   ],
   "source": [
    "# Note that the adversarial attacks only affect a subset of the initial set of malicious samples\n",
    "# Hence, we compute the classification performance also on this subset for a far comparison\n",
    "\n",
    "\n",
    "\n",
    "adv_bPred_base = sma_bClf.predict(mal_base[small_features])\n",
    "adv_bPred_adv = sma_bClf.predict(mal_adv[small_features])\n",
    "adv_bin_base_rec =  recall_score(mal_base['Nature'], adv_bPred_base, pos_label=1)\n",
    "adv_bin_adv_rec = recall_score(mal_adv['Nature'], adv_bPred_adv, pos_label=1)\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_bin_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_bin_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classifier - cascade (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing sma_mc......done! Training time: 0.003987s\tInference time: 0.013962s\n",
      "Total Misclassifications: 22714 out of 73150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17571</td>\n",
       "      <td>976</td>\n",
       "      <td>3587</td>\n",
       "      <td>5799</td>\n",
       "      <td>1413</td>\n",
       "      <td>4251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>988</td>\n",
       "      <td>25368</td>\n",
       "      <td>58</td>\n",
       "      <td>1363</td>\n",
       "      <td>833</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>338</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1571</td>\n",
       "      <td>233</td>\n",
       "      <td>474</td>\n",
       "      <td>5560</td>\n",
       "      <td>270</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>470</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      1      2     3     4     5     6\n",
       "True                                      \n",
       "1     17571    976  3587  5799  1413  4251\n",
       "2       988  25368    58  1363   833   240\n",
       "3        45      0   338    22     1     4\n",
       "4      1571    233   474  5560   270    36\n",
       "5        32     30    10    43   470   356\n",
       "6         5      0    32     8    34  1129"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_mcClf, sma_mcPred, sma_mcResult = develop_clf(malicious_train, malicious_test, small_features, clf_name='sma_mc', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "sma_mcErr = int(len(malicious_test) * (1-sma_mcResult.acc_multi))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {}\".format(sma_mcErr, len(malicious_test)))\n",
    "pd.crosstab(malicious_test['Label_cat'], sma_mcPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications (among the malicious samples): 21209 out of 70711\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16866</td>\n",
       "      <td>976</td>\n",
       "      <td>3146</td>\n",
       "      <td>5207</td>\n",
       "      <td>1405</td>\n",
       "      <td>4167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>988</td>\n",
       "      <td>25290</td>\n",
       "      <td>58</td>\n",
       "      <td>1359</td>\n",
       "      <td>831</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>281</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1484</td>\n",
       "      <td>233</td>\n",
       "      <td>387</td>\n",
       "      <td>5468</td>\n",
       "      <td>270</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>469</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      1      2     3     4     5     6\n",
       "True                                      \n",
       "1     16866    976  3146  5207  1405  4167\n",
       "2       988  25290    58  1359   831   125\n",
       "3        29      0   281    16     0     4\n",
       "4      1484    233   387  5468   270    24\n",
       "5        32     30    10    43   469   356\n",
       "6         2      0    23     3     1  1128"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We select the samples flagged as malicious by the initial classifier.\n",
    "# Of course, samples flagged as malicious that are NOT actually malicious will always be misclassified\n",
    "\n",
    "all_test['sma_bPred'] = sma_bPred\n",
    "sma_mc_test = all_test[(all_test['sma_bPred']==1) & (all_test['Nature']==1)]\n",
    "if (len(sma_mc_test)==0):\n",
    "    # in this case, this classifier receives nothing\n",
    "    print(\"There is no malicious sample flagged as malicious to analyze!\")\n",
    "\n",
    "sma_mcPred_m = sma_mcClf.predict(sma_mc_test[small_features])\n",
    "sma_mcResult.acc_multic = accuracy_score(sma_mc_test['Label_cat'], sma_mcPred_m, normalize=True, sample_weight=None)\n",
    "sma_mcErr_m = int((1-sma_mcResult.acc_multic) * len(sma_mc_test))\n",
    "print(\"Total Misclassifications (among the malicious samples): {} out of {}\".format(sma_mcErr_m, len(sma_mc_test)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.crosstab(sma_mc_test['Label_cat'], sma_mcPred_m, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This classifier also analyzed 20782 benign samples that were incorrectly labelled as 'malicious' by the (small) binary classifier\n",
      "Hence, this (small) classifier was tested on 91493 samples, of which 41991 have been misclassified\n"
     ]
    }
   ],
   "source": [
    "## Note: We also accounted for the false positives of the first binary classifier (all of which have been considered as misclassifications)\n",
    "sma_bin_falsePositives = int(sma_bResult.fpr * len(benign_test))\n",
    "print(\"This classifier also analyzed {} benign samples that were incorrectly labelled as 'malicious' by the (small) binary classifier\".format(sma_bin_falsePositives))\n",
    "\n",
    "print(\"Hence, this (small) classifier was tested on {} samples, of which {} have been misclassified\".format(len(sma_mc_test)+sma_bin_falsePositives, sma_bin_falsePositives+sma_mcErr_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classifier - stand-alone (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing sma_m......done! Training time: 0.004984s\tInference time: 0.027906s\n",
      "Total Misclassifications: 46326 out of 173220\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76927</td>\n",
       "      <td>7428</td>\n",
       "      <td>449</td>\n",
       "      <td>9010</td>\n",
       "      <td>3665</td>\n",
       "      <td>1174</td>\n",
       "      <td>1417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1700</td>\n",
       "      <td>17167</td>\n",
       "      <td>975</td>\n",
       "      <td>2840</td>\n",
       "      <td>5562</td>\n",
       "      <td>1260</td>\n",
       "      <td>4093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169</td>\n",
       "      <td>991</td>\n",
       "      <td>25368</td>\n",
       "      <td>58</td>\n",
       "      <td>1339</td>\n",
       "      <td>822</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>326</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>182</td>\n",
       "      <td>1491</td>\n",
       "      <td>233</td>\n",
       "      <td>344</td>\n",
       "      <td>5513</td>\n",
       "      <td>304</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>453</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>1140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1      2     3     4     5     6\n",
       "True                                             \n",
       "0     76927   7428    449  9010  3665  1174  1417\n",
       "1      1700  17167    975  2840  5562  1260  4093\n",
       "2       169    991  25368    58  1339   822   103\n",
       "3        39     26      0   326    15     1     3\n",
       "4       182   1491    233   344  5513   304    77\n",
       "5        10     32     30     4    56   453   356\n",
       "6         1      6      0    24     3    34  1140"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_mClf, sma_mPred, sma_mResult = develop_clf(all_train, all_test, small_features, clf_name='sma_m', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "sma_mErr = int(len(all_test) * (1-sma_mResult.acc_multi))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {}\".format(sma_mErr, len(all_test)))\n",
    "sma_mResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Classifier: Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 25243 out of 173220 (Recall: 0.971278\tFPR: 0.231268)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76927</td>\n",
       "      <td>23143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2101</td>\n",
       "      <td>71049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     76927  23143\n",
       "1      2101  71049"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## MULTI-CLASS CLASSIFIER - BINARY ##########\n",
    "sma_mPred_bin = np.copy(sma_mPred)\n",
    "sma_mPred_bin[sma_mPred_bin > 0] = 1\n",
    "sma_mResult.bin_results(all_test['Nature'], sma_mPred_bin)\n",
    "sma_mErr_bin = int(len(all_test) * (1-sma_mResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_mErr_bin, len(all_test), sma_mResult.rec, sma_mResult.fpr))\n",
    "sma_mResult.ctab_bin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial Attack against the Multiclass Classifier (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.969\n",
      "Adversarial Recall (attack): 0.998\n"
     ]
    }
   ],
   "source": [
    "adv_mPred_base = sma_mClf.predict(mal_base[small_features])\n",
    "adv_mPred_adv = sma_mClf.predict(mal_adv[small_features])\n",
    "\n",
    "adv_mPred_base_bin = np.copy(adv_mPred_base)\n",
    "adv_mPred_base_bin[adv_mPred_base_bin > 0] = 1\n",
    "\n",
    "adv_mPred_adv_bin = np.copy(adv_mPred_adv)\n",
    "adv_mPred_adv_bin[adv_mPred_adv_bin > 0] = 1\n",
    "\n",
    "\n",
    "adv_multi_base_rec =  recall_score(mal_base['Nature'], adv_mPred_base_bin)\n",
    "adv_multi_adv_rec = recall_score(mal_adv['Nature'], adv_mPred_adv_bin)\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_multi_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_multi_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Classifiers (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing sma_neris......done! Training time: 0.004002s\tInference time: 0.016961s\n",
      "Testing sma_neris...\n",
      "...done! \tInference time: 0.020947s\n",
      "Training and testing sma_rbot......done! Training time: 0.003003s\tInference time: 0.014978s\n",
      "Testing sma_rbot...\n",
      "...done! \tInference time: 0.018503s\n",
      "Training and testing sma_nsis......done! Training time: 0.003630s\tInference time: 0.013991s\n",
      "Testing sma_nsis...\n",
      "...done! \tInference time: 0.021855s\n",
      "Training and testing sma_virut......done! Training time: 0.003965s\tInference time: 0.015960s\n",
      "Testing sma_virut...\n",
      "...done! \tInference time: 0.020929s\n",
      "Training and testing sma_donbot......done! Training time: 0.002984s\tInference time: 0.013000s\n",
      "Testing sma_donbot...\n",
      "...done! \tInference time: 0.018898s\n",
      "Training and testing sma_murlo......done! Training time: 0.003992s\tInference time: 0.011962s\n",
      "Testing sma_murlo...\n",
      "...done! \tInference time: 0.018937s\n",
      "Total training time: 0.021575s\t AvgFPR: 0.081058\t AvgTPR: 0.943117\tTotal inference time: 0.120069s (fake: 0.086852s)\n"
     ]
    }
   ],
   "source": [
    "sma_ensemble_df = pd.DataFrame()\n",
    "adv_ensemble_df_base = pd.DataFrame()\n",
    "adv_ensemble_df_adv = pd.DataFrame()\n",
    "\n",
    "benign_train = benign_df[benign_df['is_train']==True]\n",
    "benign_test = benign_df[benign_df['is_test']==True]\n",
    "\n",
    "\n",
    "sma_ens_time = 0\n",
    "sma_ens_avgFPR = 0\n",
    "sma_tot_TP = 0\n",
    "sma_tot_P = 0\n",
    "sma_ens_infer_time = 0\n",
    "sma_fakeEns_infer_time = 0\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_train = {a}_df[{a}_df['is_train']==True]\")\n",
    "    exec(f\"{a}_test = {a}_df[{a}_df['is_test']==True]\")\n",
    "\n",
    "    exec(f\"train = pd.concat([benign_train, {a}_train])\")\n",
    "    exec(f\"test = pd.concat([benign_test, {a}_test])\")\n",
    "\n",
    "    exec(f\"sma_{a}Clf, sma_{a}Pred, sma_{a}Result = develop_clf(train, test, small_features, clf_name='sma_{a}', clf_type=base_clf, verbose=1)\")\n",
    "    exec(f\"sma_fakeEns_infer_time += sma_{a}Result.infer_time\")\n",
    "    \n",
    "    exec(f\"sma_{a}_allPred, sma_{a}_allResults, sma_{a}Result.infer_time = evaluate_clf(sma_{a}Clf, all_test, small_features, clf_name='sma_{a}', time=sma_{a}Result.time, verbose=1)\")\n",
    "    exec(f\"sma_ensemble_df['{a}'] = sma_{a}_allPred\")\n",
    "\n",
    "    exec(f\"adv_{a}Pred_base = sma_{a}Clf.predict(mal_base[small_features])\")\n",
    "    exec(f\"adv_{a}Pred_adv = sma_{a}Clf.predict(mal_adv[small_features])\")\n",
    "    exec(f\"adv_ensemble_df_base['{a}'] = adv_{a}Pred_base\")\n",
    "    exec(f\"adv_ensemble_df_adv['{a}'] = adv_{a}Pred_adv\")\n",
    "\n",
    "\n",
    "\n",
    "    exec(f\"sma_tot_TP += (sma_{a}Result.rec * len({a}_test))\")\n",
    "    exec(f\"sma_tot_P += len({a}_test)\")\n",
    "\n",
    "\n",
    "    exec(f\"sma_ens_avgFPR += sma_{a}Result.fpr\")\n",
    "    exec(f\"sma_ens_time += sma_{a}Result.time\")\n",
    "    exec(f\"sma_ens_infer_time +=sma_{a}Result.infer_time\")\n",
    "\n",
    "\n",
    "\n",
    "sma_ens_avgFPR = sma_ens_avgFPR / len(attack_names)\n",
    "sma_ens_avgREC = sma_tot_TP/sma_tot_P\n",
    "\n",
    "\n",
    "print(\"Total training time: {:5f}s\\t AvgFPR: {:5f}\\t AvgTPR: {:5f}\\tTotal inference time: {:5f}s (fake: {:5f}s)\".\n",
    "      format(sma_ens_time, sma_ens_avgFPR, sma_ens_avgREC, sma_ens_infer_time, sma_fakeEns_infer_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing real Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma_ensemble_df[\"sum\"] = sma_ensemble_df.sum(axis=1)\n",
    "sma_ensemble_df[\"LOR\"] = (sma_ensemble_df[\"sum\"]>0)\n",
    "temp = all_test['Nature'] \n",
    "sma_ensemble_df['True'] = ((temp.reset_index(drop=True)) > 0)\n",
    "\n",
    "adv_ensemble_df_base[\"sum\"] = adv_ensemble_df_base.sum(axis=1)\n",
    "adv_ensemble_df_base[\"LOR\"] = (adv_ensemble_df_base[\"sum\"]>0)\n",
    "temp = mal_base['Nature']\n",
    "adv_ensemble_df_base['True'] = ((temp.reset_index(drop=True)) > 0)\n",
    "\n",
    "\n",
    "adv_ensemble_df_adv[\"sum\"] = adv_ensemble_df_adv.sum(axis=1)\n",
    "adv_ensemble_df_adv[\"LOR\"] = (adv_ensemble_df_adv[\"sum\"]>0)\n",
    "temp = mal_adv['Nature']\n",
    "adv_ensemble_df_adv['True'] = ((temp.reset_index(drop=True)) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Logical OR (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 31898 out of 173220 (Recall: 0.982775\tFPR: 0.306166)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>69432</td>\n",
       "      <td>30638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1260</td>\n",
       "      <td>71890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  69432  30638\n",
       "True    1260  71890"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_enslorResult = Result(sma_ensemble_df['True'], sma_ensemble_df['LOR'], sma_ens_time, sma_ens_infer_time)\n",
    "sma_enslorErr= int(len(all_test) * (1-sma_enslorResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_enslorErr, len(all_test), sma_enslorResult.rec, sma_enslorResult.fpr))\n",
    "sma_enslorResult.ctab_bin # you can also try with sma_enslorResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logical OR: Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.978\n",
      "Adversarial Recall (attack): 0.014\n"
     ]
    }
   ],
   "source": [
    "adv_enslor_base_rec = recall_score(mal_base['Nature'], adv_ensemble_df_base[\"LOR\"])\n",
    "adv_enslor_adv_rec = recall_score(mal_adv['Nature'], adv_ensemble_df_adv[\"LOR\"])\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_enslor_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_enslor_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Majority Voting (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 44282 out of 173220 (Recall: 0.449364\tFPR: 0.040012)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>96066</td>\n",
       "      <td>4004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>40279</td>\n",
       "      <td>32871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  96066   4004\n",
       "True   40279  32871"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_ensemble_df[\"MAJV\"] = (sma_ensemble_df[\"sum\"]>=min_agree)\n",
    "sma_ensvotResult = Result(sma_ensemble_df['True'], sma_ensemble_df['MAJV'], sma_ens_time, sma_ens_infer_time)\n",
    "sma_ensvotErr = int(len(all_test) * (1-sma_ensvotResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_ensvotErr, len(all_test), sma_ensvotResult.rec, sma_ensvotResult.fpr))\n",
    "sma_ensvotResult.ctab_bin # you can also try with sma_ensvotResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Majority Voting: Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.329\n",
      "Adversarial Recall (attack): 0.000\n"
     ]
    }
   ],
   "source": [
    "adv_ensemble_df_base[\"MAJV\"] = (adv_ensemble_df_base[\"sum\"]>=min_agree)\n",
    "adv_ensemble_df_adv[\"MAJV\"] = (adv_ensemble_df_adv[\"sum\"]>=min_agree)\n",
    "\n",
    "adv_ensvot_base_rec = recall_score(mal_base['Nature'], adv_ensemble_df_base[\"MAJV\"])\n",
    "adv_ensvot_adv_rec = recall_score(mal_adv['Nature'], adv_ensemble_df_adv[\"MAJV\"])\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_ensvot_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_ensvot_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Stacked Classifier (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 30091 out of 173220 (Recall: 0.980383\tFPR: 0.286360)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71414</td>\n",
       "      <td>28656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1435</td>\n",
       "      <td>71715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     71414  28656\n",
       "1      1435  71715"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_list = []\n",
    "for a in attack_names:\n",
    "    exec(f\"clf_list.append(sma_{a}Clf)\")\n",
    "\n",
    "sma_sClf = StackingClassifier(classifiers=clf_list, meta_classifier=meta, fit_base_estimators=False, use_probas = False)\n",
    "s_timeStart = time.time()\n",
    "sma_sClf.fit(all_train[small_features], all_train['Nature'])\n",
    "sma_s_time = time.time() - s_timeStart + sma_ens_time\n",
    "s_timeStart = time.time()\n",
    "sma_sPred = sma_sClf.predict(all_test[small_features])\n",
    "sma_s_infer_time = time.time()-s_timeStart\n",
    "sma_sResult = Result(all_test['Nature'], sma_sPred, sma_s_time, sma_s_infer_time)\n",
    "if sma_sResult.acc < sma_sResult.acc_multi:\n",
    "    sma_sResult.acc = sma_sResult.acc_multi\n",
    "sma_sErr = int(len(all_test) * (1-sma_sResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_sErr, len(all_test), sma_sResult.rec, sma_sResult.fpr))\n",
    "\n",
    "sma_sResult.ctab_bin # you can also try with sma_sResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked Classifier: Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.977\n",
      "Adversarial Recall (attack): 0.012\n"
     ]
    }
   ],
   "source": [
    "adv_sPred_base = sma_sClf.predict(mal_base[small_features])\n",
    "adv_sPred_adv = sma_sClf.predict(mal_adv[small_features])\n",
    "\n",
    "adv_ensstk_base_rec =  recall_score(mal_base['Nature'], adv_sPred_base)\n",
    "adv_ensstk_adv_rec = recall_score(mal_adv['Nature'], adv_sPred_adv)\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_ensstk_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_ensstk_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now inspect the results by referring to the \"Result\" variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR 0.991455912508544 0.992822966507177 0.9838824333561176 0.9989883800410116 0.9745317840054682 0.9963636363636363 \n",
      "FPR 0.06416508444089142 0.07245927850504652 0.03091002964591451 0.11671829719196558 0.01809733186769258 0.09954032177475769 \n",
      "Training Time 0.006976604461669922 0.007972955703735352 0.02709794044494629 0.02709794044494629 0.02709794044494629 0.03905892372131348 \n",
      "Inference Time 0.03491997718811035 0.04087400436401367 0.1300203800201416 0.18239569664001465 0.18239569664001465 0.1435544490814209 \n",
      "Accuracy 0.827872070199746 0.9551091098025632 0.6986418476387453 0.6971428571428572\n"
     ]
    }
   ],
   "source": [
    "## BASELINE RESULTS (on Complete Feature Set)\n",
    "print(\n",
    "    \"TPR\",\n",
    "    bResult.rec,\n",
    "    mResult.rec,\n",
    "    ens_avgREC,\n",
    "    enslorResult.rec,\n",
    "    ensvotResult.rec,\n",
    "    sResult.rec,\n",
    "    \"\\nFPR\",\n",
    "    bResult.fpr,\n",
    "    mResult.fpr,\n",
    "    ens_avgFPR,\n",
    "    enslorResult.fpr,\n",
    "    ensvotResult.fpr,\n",
    "    sResult.fpr,\n",
    "    \"\\nTraining Time\",\n",
    "    bResult.time,\n",
    "    mResult.time,\n",
    "    ens_time,\n",
    "    enslorResult.time,\n",
    "    ensvotResult.time,\n",
    "    sResult.time,\n",
    "    \"\\nInference Time\",\n",
    "    bResult.infer_time,\n",
    "    mResult.infer_time,\n",
    "    fakeEns_infer_time,\n",
    "    enslorResult.infer_time,\n",
    "    ensvotResult.infer_time,\n",
    "    sResult.infer_time, \n",
    "    \"\\nAccuracy\",\n",
    "    mResult.acc_multi,   # This is the accuracy on the multiclassification\n",
    "    mResult.acc,         # This is the accuracy on the binary classification\n",
    "    mcResult.acc_multic, # This is the accuracy on the multiclassification AFTER the output of the binary classifier (it does not account for benign samples, which are false positives)\n",
    "    mcResult.acc_multi   # This is the accuracy on the multiclassification on the whole test portion of the malicious dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR 0.9666575529733424 0.9712781954887219 0.9431168831168831 0.9827751196172249 0.4493643198906357 0.9803827751196172 \n",
      "FPR 0.20768462076546412 0.231268112321375 0.08105825921854702 0.30616568402118516 0.040011991605875874 0.2863595483161787 \n",
      "Training Time 0.004975318908691406 0.004984140396118164 0.021575212478637695 0.021575212478637695 0.021575212478637695 0.03154921531677246 \n",
      "Inference Time 0.024428844451904297 0.027906179428100586 0.08685183525085449 0.12006878852844238 0.12006878852844238 0.11062240600585938 \n",
      "Accuracy 0.7325597506061655 0.854266251010276 0.7000608109063653 0.6894873547505126\n"
     ]
    }
   ],
   "source": [
    "## BASELINE RESULTS (on Essential Feature Set)\n",
    "print(\n",
    "    \"TPR\",\n",
    "    sma_bResult.rec,\n",
    "    sma_mResult.rec,\n",
    "    sma_ens_avgREC,\n",
    "    sma_enslorResult.rec,\n",
    "    sma_ensvotResult.rec,\n",
    "    sma_sResult.rec,\n",
    "    \"\\nFPR\",\n",
    "    sma_bResult.fpr,\n",
    "    sma_mResult.fpr,\n",
    "    sma_ens_avgFPR,\n",
    "    sma_enslorResult.fpr,\n",
    "    sma_ensvotResult.fpr,\n",
    "    sma_sResult.fpr,\n",
    "    \"\\nTraining Time\",\n",
    "    sma_bResult.time,\n",
    "    sma_mResult.time,\n",
    "    sma_ens_time,\n",
    "    sma_enslorResult.time,\n",
    "    sma_ensvotResult.time,\n",
    "    sma_sResult.time,\n",
    "    \"\\nInference Time\",\n",
    "    sma_bResult.infer_time,\n",
    "    sma_mResult.infer_time,\n",
    "    sma_fakeEns_infer_time,\n",
    "    sma_enslorResult.infer_time,\n",
    "    sma_ensvotResult.infer_time,\n",
    "    sma_sResult.infer_time, \n",
    "    \"\\nAccuracy\",\n",
    "    sma_mResult.acc_multi,   # This is the accuracy on the multiclassification\n",
    "    sma_mResult.acc,         # This is the accuracy on the binary classification\n",
    "    sma_mcResult.acc_multic, # This is the accuracy on the multiclassification AFTER the output of the binary classifier (it does not account for benign samples, which are false positives)\n",
    "    sma_mcResult.acc_multi   # This is the accuracy on the multiclassification on the whole test portion of the malicious dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
      "      BD: TPR=0.937699\tFPR=0.058364\n",
      "      MD (binarized) CLF: TPR=0.900330\tFPR=0.069526\n",
      "      ED-o: TPR=0.972680\tFPR=0.104795\n",
      "      ED-v: TPR=0.614395\tFPR=0.011307\n",
      "      ED-s: TPR=0.967791\tFPR=0.092237\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "## Open World: One attack against all (averaged results)\n",
    "\n",
    "print('''Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
    "      BD: TPR={:5f}\\tFPR={:5f}\n",
    "      MD (binarized) CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      ED-o: TPR={:5f}\\tFPR={:5f}\n",
    "      ED-v: TPR={:5f}\\tFPR={:5f}\n",
    "      ED-s: TPR={:5f}\\tFPR={:5f}\n",
    "      '''.format(oaac_bin_rec, oaac_bin_fpr,\n",
    "                 oaac_multi_rec, oaac_multi_fpr,\n",
    "                 oaac_enslor_rec, oaac_enslor_fpr,\n",
    "                 oaac_ensvot_rec, oaac_ensvot_fpr,\n",
    "                 oaac_ensstk_rec, oaac_ensstk_fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BD (before, after): 0.9578044074337362 0.9723265969330761 \n",
      "MD  (before, after): 0.9686706611150604 0.9977658170001016 \n",
      "ED-o  (before, after): 0.9781151619782675 0.01365898243119732 \n",
      "ED-v  (before, after): 0.32862800853051694 0.00035543820452929825 \n",
      "ED-s  (before, after): 0.9768457398192343 0.012237229613080125\n"
     ]
    }
   ],
   "source": [
    "## Adversarial Attacks \n",
    "\n",
    "print(\n",
    "    \"BD (before, after):\",\n",
    "    adv_bin_base_rec,\n",
    "    adv_bin_adv_rec,\n",
    "    \"\\nMD  (before, after):\",\n",
    "    adv_multi_base_rec,\n",
    "    adv_multi_adv_rec,\n",
    "    \"\\nED-o  (before, after):\",\n",
    "    adv_enslor_base_rec,\n",
    "    adv_enslor_adv_rec, \n",
    "    \"\\nED-v  (before, after):\",\n",
    "    adv_ensvot_base_rec,\n",
    "    adv_ensvot_adv_rec, \n",
    "    \"\\nED-s  (before, after):\",\n",
    "    adv_ensstk_base_rec, \n",
    "    adv_ensstk_adv_rec,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
