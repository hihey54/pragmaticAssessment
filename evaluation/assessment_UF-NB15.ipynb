{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from supportFunctions import *\n",
    "\n",
    "root_folder = \"..\\\\data\\\\UF-NB15\\\\flows\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PARAMETERS #####\n",
    "\n",
    "## Generic parameters\n",
    "temporal = True # used to determine if this evaluation assumes a \"temporal\" dependency among its samples\n",
    "base_clf = 'dt' # name of the classifier used for this \"run\". Available names: ['dt', 'rf', 'hgb', 'lr']. You can add more by editing the supportFunctions file\n",
    "test_size = 0.2 # proportion of the dataset used for testing. We always kept it fixed to 0.2 for our paper\n",
    "train_size = 100 # proportion of the REMAINING data that are used for training (if >1, then it will take that exact amount). To reproduce the results of the paper, use: 100 (for \"limited\" training data) or 0.2 or 0.5 or 0.99 (for scarce, moderate, abundant training data, respectively) \n",
    "agreement = 0.5 # from 0 to 1. Proportion of classifiers that must agree on an attack (for the ensemble). This is fixed in our paper.\n",
    "max_size = 500000 ## maximum amount of samples to include when creating the initial dataframes. This is fixed in our paper\n",
    "max_size_atk = int(max_size / 3) # maximum amount of malicious samples per class. This is fixed in our paper\n",
    "\n",
    "## Adversarial Attacks parameters\n",
    "atk_intensity = 1\n",
    "pkt_intensity = atk_intensity * 10 # \n",
    "byt_intensity = pkt_intensity * 100 # \n",
    "dur_intensity = atk_intensity * 100 # consider seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading input data\n",
    "\n",
    "malicious_folder = root_folder + \"malicious/\"\n",
    "\n",
    "benign_file = root_folder + \"benign.csv\"\n",
    "benign_df = pd.read_csv(benign_file, header='infer', index_col=0)\n",
    "benign_df = benign_df.sample(min(max_size, len(benign_df)))\n",
    "#sort by timestamp\n",
    "if temporal == True:\n",
    "    pass # already sorted\n",
    "benign_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# attack_names = ['expl', 'recon', 'dos', 'shell', 'fuzz', 'worm', 'bdoor', 'ana', 'other']\n",
    "attack_names = ['expl', 'recon', 'dos', 'shell', 'fuzz', 'bdoor', 'ana']\n",
    "\n",
    "expl_file = malicious_folder + \"expl.csv\"\n",
    "recon_file = malicious_folder + \"recon.csv\"\n",
    "dos_file = malicious_folder + \"dos.csv\"\n",
    "shell_file = malicious_folder + \"shell.csv\"\n",
    "fuzz_file = malicious_folder + \"fuzz.csv\"\n",
    "worm_file = malicious_folder + \"worm.csv\" # excluded, only 164 samples\n",
    "bdoor_file = malicious_folder + \"bdoor.csv\"\n",
    "ana_file = malicious_folder + \"ana.csv\"\n",
    "other_file = malicious_folder + \"other.csv\" # excluded due to mismatch\n",
    "\n",
    "\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_df = pd.read_csv({a}_file, header='infer', index_col=0)\")\n",
    "    exec(f\"{a}_df = {a}_df.sample(min(max_size_atk, len({a}_df)))\")\n",
    "    # sort by timestamp\n",
    "    if temporal == True:\n",
    "        pass\n",
    "    exec(f\"{a}_df.reset_index(inplace=True, drop=True)\")\n",
    "    exec(f\"{a}_df['Label'] = a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining Train and Test sets for each class\n",
    "\n",
    "df_list = [benign_df]\n",
    "for a in attack_names:\n",
    "    exec(f\"df_list.append({a}_df)\")\n",
    "\n",
    "if temporal == True:\n",
    "    for dummy_df in df_list:\n",
    "        if train_size <=1:\n",
    "            train_threshold = int(((1-test_size) * train_size) * len(dummy_df))\n",
    "        else:\n",
    "            train_threshold = int(100)\n",
    "        test_threshold = len(dummy_df) - int(test_size * len(dummy_df))\n",
    "        dummy_df['index'] = dummy_df.index\n",
    "        dummy_df['is_test'] = np.where(dummy_df['index'] >= test_threshold , True, False)\n",
    "        dummy_df['is_train'] = np.where(dummy_df['index'] <= train_threshold , True, False)\n",
    "else:\n",
    "    for dummy_df in df_list:\n",
    "        if train_size <= 1:\n",
    "            train_threshold = test_size + (1-test_size)*train_size\n",
    "        else:\n",
    "            train_threshold = test_size + ((train_size * 100) / (len(dummy_df)) / 100)       \n",
    "        dummy_df['seed'] = (np.random.uniform(0,1,len(dummy_df)))\n",
    "        dummy_df['is_test'] = np.where(dummy_df['seed'] <= test_size, True, False)\n",
    "        dummy_df['is_train'] = np.where((dummy_df['seed'] <= train_threshold) & (dummy_df['is_test']==False), True, False)\n",
    "\n",
    "# get all together\n",
    "all_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 0 & \\textit{Benign} & 500000 & \\\\ \\cline{2-4}\n",
      "& 1 & \\textit{expl} & 31551 \\\\ \\cline{2-4}\n",
      "& 2 & \\textit{recon} & 12779 \\\\ \\cline{2-4}\n",
      "& 3 & \\textit{dos} & 5794 \\\\ \\cline{2-4}\n",
      "& 4 & \\textit{shell} & 1427 \\\\ \\cline{2-4}\n",
      "& 5 & \\textit{fuzz} & 22310 \\\\ \\cline{2-4}\n",
      "& 6 & \\textit{bdoor} & 2169 \\\\ \\cline{2-4}\n",
      "& 7 & \\textit{ana} & 2299 \\\\ \\cline{2-4}\n"
     ]
    }
   ],
   "source": [
    "def handle_categorical(df):\n",
    "    ## Handling categorical data\n",
    "    df_dummy = df.copy(deep=True)\n",
    "    df_dummy['Nature'] = np.where(df_dummy['Label'].str.contains('BENIGN'),0,1)\n",
    "    \n",
    "    for column_name in df_dummy.columns:\n",
    "        if column_name == ('SrcPort_type'):\n",
    "            df_dummy[column_name+'-f'] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('DstPort_type'):\n",
    "            df_dummy[column_name+'-f'] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('Proto'):\n",
    "            df_dummy[column_name+\"-f\"] = pd.factorize(df_dummy[column_name])[0]\n",
    "        else:\n",
    "            pass\n",
    "    return df_dummy\n",
    "\n",
    "all_df = handle_categorical(all_df)\n",
    "all_df['Label_cat'] = pd.factorize(all_df['Label'])[0]\n",
    "all_df['int2int'] = np.where( ((all_df['SrcIP_internal']==True) & (all_df['DstIP_internal']==True)), True, False)\n",
    "\n",
    "all_df['totPkt'] = all_df['Pkts_in'] + all_df['Pkts_out']\n",
    "all_df['totByt'] = all_df['Bytes_in'] + all_df['Bytes_out']\n",
    "\n",
    "all_train, all_test = all_df[all_df['is_train']==True], all_df[all_df['is_test']==True]\n",
    "\n",
    "### SPLITTING ALL BACK ####\n",
    "benign_df = all_df[all_df['Label']=='BENIGN']\n",
    "benign_train = benign_df[benign_df['is_train']==True]\n",
    "benign_test = benign_df[benign_df['is_test']==True]\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_df = all_df[all_df['Label']=='{a}']\")\n",
    "    \n",
    "malicious_df = all_df[all_df['Label']!='BENIGN']\n",
    "malicious_train, malicious_test = malicious_df[malicious_df['is_train']==True], malicious_df[malicious_df['is_test']==True]\n",
    "\n",
    "\n",
    "\n",
    "print(\"& 0 & \\\\textit{{Benign}} & {} & \\\\\\\\ \\\\cline{{2-4}}\".format(len(benign_df)))\n",
    "\n",
    "\n",
    "for i,a in enumerate(attack_names):\n",
    "    exec(f\"print('& {i+1} & \\\\\\\\textit{{{{{a}}}}} & {{}} \\\\\\\\\\\\\\\\ \\\\\\\\cline{{{{2-4}}}}'.format(len({a}_df)))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature sets\n",
    "\n",
    "# the following is the \"complete\" feature set\n",
    "\n",
    "features = ['Proto_l7', 'Bytes_in', 'Pkts_in',\n",
    "       'Bytes_out', 'Pkts_out', 'TCP_Flag', 'Client_TCP_Flag',\n",
    "       'Server_TCP_Flag', 'Duration(ms)', 'Duration_in', 'Duration_out',\n",
    "       'TTL_min', 'TTL_max', 'MaxPkts', 'MinPkts', 'MinLen', 'MaxLen',\n",
    "        'RetrBytes_in', 'RetrPkts_in',\n",
    "       'RetrBytes_out', 'RetrPkts_out', 'Throughput_SrcDst',\n",
    "       'Throughput_DstSrc', 'Pkt<128Byt', 'Pkt128<Byt<256', 'Pkt256<Byt<512',\n",
    "       'Pkt512<Byt<1024', 'Pkt1024<Byt<1514', 'MaxTCPWin_in', 'MaxTCPWin_out',\n",
    "       'ICMP', 'ICMP_IPV4', 'DNS_QUERY_ID', 'DNS_QUERY_TYPE', 'DNS_TTL_ANSWER',\n",
    "       'FTP_COMMAND_RET_CODE',\n",
    "       'SrcPort_type-f', 'DstPort_type-f',\n",
    "       'Proto-f', 'int2int'\n",
    "       ]\n",
    "\n",
    "\n",
    "# this is for the \"essential\" feature set\n",
    "small_features = ['Proto-f', 'Bytes_in',\n",
    "       'Pkts_in', 'Bytes_out', 'Pkts_out', 'TCP_Flag', 'Client_TCP_Flag',\n",
    "        'totPkt', 'totByt',\n",
    "        'Duration(ms)',\n",
    "       'ICMP', 'ICMP_IPV4', 'DNS_QUERY_ID', 'DNS_QUERY_TYPE', 'DNS_TTL_ANSWER',\n",
    "       'FTP_COMMAND_RET_CODE',\n",
    "        'DstIP_internal', 'int2int',\n",
    "       'DstPort_type-f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2310\n"
     ]
    }
   ],
   "source": [
    "# creating adversarial dataset\n",
    "mal_base = malicious_df[((malicious_df['Proto']==17)) & (malicious_df['is_test']==True)]\n",
    "print(len(mal_base))\n",
    "mal_adv = mal_base.copy(deep=True)\n",
    "\n",
    "# attacking\n",
    "max_dur = mal_adv['Duration(ms)'].max()\n",
    "min_dur = mal_adv['Duration(ms)'].min()\n",
    "mal_adv['Duration(ms)'] = mal_adv['Duration(ms)'] + (dur_intensity * 1000)\n",
    "\n",
    "mal_adv['Duration(ms)'] = np.where(mal_adv['Duration(ms)'] > max_dur, max_dur, mal_adv['Duration(ms)'])\n",
    "mal_adv['Duration(ms)'] = np.where(mal_adv['Duration(ms)'] < min_dur, min_dur, mal_adv['Duration(ms)'])\n",
    "\n",
    "\n",
    "#mal_adv['Pkts_in'] = mal_adv['Pkts_in'] + pkt_intensity\n",
    "mal_adv['Pkts_out'] = mal_adv['Pkts_out'] + pkt_intensity\n",
    "#mal_adv['Bytes_in'] = mal_adv['Bytes_in'] + byt_intensity\n",
    "mal_adv['Bytes_out'] = mal_adv['Bytes_out'] + byt_intensity \n",
    "\n",
    "\n",
    "mal_adv['totPkt'] = mal_adv['Pkts_in'] + mal_adv['Pkts_out']\n",
    "mal_adv['totByt'] = mal_adv['Bytes_in'] + mal_adv['Bytes_out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROM NOW ON, THE CODE IS ALWAYS THE SAME FOR EVERY DATASET!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Assessment on \"Complete\" feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BINARY CLASSIFIER (Complete features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing bin......done! Training time: 0.005981s\tInference time: 0.031964s\n",
      "Total Misclassifications: 913 out of 115662 (Recall: 0.998340\tFPR: 0.008880)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99112</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>15636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     99112    888\n",
       "1        26  15636"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bClf, bPred, bResult = develop_clf(all_train, all_test, features, clf_name='bin', label='Nature', clf_type=base_clf, verbose=1)\n",
    "\n",
    "if (bResult.acc == 0):\n",
    "    bErr = int(len(all_test) * (1-bResult.acc_multi))\n",
    "else:\n",
    "    bErr = int(len(all_test) * (1-bResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(bErr, len(all_test), bResult.rec, bResult.fpr))\n",
    "pd.crosstab(all_test['Nature'], bPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-CLASS CLASSIFIER - cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing mc......done! Training time: 0.007957s\tInference time: 0.007967s\n",
      "Total Misclassifications: 5763 out of 15662\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3362</td>\n",
       "      <td>288</td>\n",
       "      <td>1462</td>\n",
       "      <td>193</td>\n",
       "      <td>375</td>\n",
       "      <td>368</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78</td>\n",
       "      <td>2026</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>148</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>242</td>\n",
       "      <td>61</td>\n",
       "      <td>344</td>\n",
       "      <td>38</td>\n",
       "      <td>114</td>\n",
       "      <td>174</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>264</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>251</td>\n",
       "      <td>78</td>\n",
       "      <td>136</td>\n",
       "      <td>31</td>\n",
       "      <td>3561</td>\n",
       "      <td>246</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>151</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>115</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred     1     2     3    4     5    6    7\n",
       "True                                       \n",
       "1     3362   288  1462  193   375  368  262\n",
       "2       78  2026   131    0    47  148  125\n",
       "3      242    61   344   38   114  174  185\n",
       "4        8     0    10  264     3    0    0\n",
       "5      251    78   136   31  3561  246  159\n",
       "6       29    33    63    9    19  151  129\n",
       "7       10    40    94    0     9  115  191"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the classifier that analyzes ONLY the malicious samples that \"receives\" from the initial binary classifier\n",
    "# It is trained on the same training set---but without using the benign samples\n",
    "# It is tested on the malicious samples in the test set that are flagged as malicious by the binary classifier\n",
    "\n",
    "\n",
    "mcClf, mcPred, mcResult = develop_clf(malicious_train, malicious_test, features, clf_name='mc', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "mcErr = int(len(malicious_test) * (1-mcResult.acc_multi))\n",
    "print(\"Total Misclassifications: {} out of {}\".format(mcErr, len(malicious_test)))\n",
    "pd.crosstab(malicious_test['Label_cat'], mcPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications (among the malicious samples): 5754 out of 15636\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3347</td>\n",
       "      <td>288</td>\n",
       "      <td>1459</td>\n",
       "      <td>192</td>\n",
       "      <td>375</td>\n",
       "      <td>368</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78</td>\n",
       "      <td>2026</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>148</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>241</td>\n",
       "      <td>61</td>\n",
       "      <td>342</td>\n",
       "      <td>38</td>\n",
       "      <td>112</td>\n",
       "      <td>174</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>264</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>251</td>\n",
       "      <td>78</td>\n",
       "      <td>136</td>\n",
       "      <td>31</td>\n",
       "      <td>3561</td>\n",
       "      <td>246</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>151</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>115</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred     1     2     3    4     5    6    7\n",
       "True                                       \n",
       "1     3347   288  1459  192   375  368  262\n",
       "2       78  2026   130    0    46  148  125\n",
       "3      241    61   342   38   112  174  185\n",
       "4        8     0    10  264     3    0    0\n",
       "5      251    78   136   31  3561  246  159\n",
       "6       29    33    63    9    19  151  129\n",
       "7       10    40    94    0     9  115  191"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We select the samples flagged as malicious by the initial classifier.\n",
    "# Of course, samples flagged as malicious that are NOT actually malicious will always be misclassified\n",
    "\n",
    "all_test['bPred'] = bPred\n",
    "mc_test = all_test[(all_test['bPred']==1) & (all_test['Nature']==1)]\n",
    "if (len(mc_test)==0):\n",
    "    # in this case, this classifier receives nothing\n",
    "    print(\"There is no malicious sample flagged as malicious to analyze!\")\n",
    "\n",
    "mcPred_m = mcClf.predict(mc_test[features])\n",
    "mcResult.acc_multic = accuracy_score(mc_test['Label_cat'], mcPred_m, normalize=True, sample_weight=None)\n",
    "mcErr_m = int((1-mcResult.acc_multic) * len(mc_test))\n",
    "print(\"Total Misclassifications (among the malicious samples): {} out of {}\".format(mcErr_m, len(mc_test)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.crosstab(mc_test['Label_cat'], mcPred_m, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This classifier also analyzed 887 benign samples that were incorrectly labelled as 'malicious' by the binary classifier\n",
      "Hence, this classifier was tested on 16523 samples, of which 6641 have been misclassified\n"
     ]
    }
   ],
   "source": [
    "## Note: We also accounted for the false positives of the first binary classifier (all of which have been considered as misclassifications)\n",
    "bin_falsePositives = int(bResult.fpr * len(benign_test))\n",
    "print(\"This classifier also analyzed {} benign samples that were incorrectly labelled as 'malicious' by the binary classifier\".format(bin_falsePositives))\n",
    "\n",
    "print(\"Hence, this classifier was tested on {} samples, of which {} have been misclassified\".format(len(mc_test)+bin_falsePositives, bin_falsePositives+mcErr_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-CLASS CLASSIFIER - stand-alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing m......done! Training time: 0.008982s\tInference time: 0.035876s\n",
      "Total Misclassifications: 7591 out of 115662\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98121</td>\n",
       "      <td>385</td>\n",
       "      <td>4</td>\n",
       "      <td>1146</td>\n",
       "      <td>40</td>\n",
       "      <td>206</td>\n",
       "      <td>23</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>3398</td>\n",
       "      <td>215</td>\n",
       "      <td>1492</td>\n",
       "      <td>189</td>\n",
       "      <td>355</td>\n",
       "      <td>356</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>1978</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>188</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>262</td>\n",
       "      <td>55</td>\n",
       "      <td>354</td>\n",
       "      <td>38</td>\n",
       "      <td>97</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>264</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>83</td>\n",
       "      <td>169</td>\n",
       "      <td>31</td>\n",
       "      <td>3613</td>\n",
       "      <td>161</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>65</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>157</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>40</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>120</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0     1     2     3    4     5    6    7\n",
       "True                                              \n",
       "0     98121   385     4  1146   40   206   23   75\n",
       "1        54  3398   215  1492  189   355  356  251\n",
       "2         1    71  1978   161    0    33  188  123\n",
       "3         9   262    55   354   38    97  164  179\n",
       "4         0     8     0    10  264     3    0    0\n",
       "5         0   260    83   169   31  3613  161  145\n",
       "6         0    28    33    65    9    19  157  122\n",
       "7         0    15    40    90    0     9  120  185"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first assess its multiclassification performance, and then its binary classification performance\n",
    "\n",
    "mClf, mPred, mResult = develop_clf(all_train, all_test, features, clf_name='m', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "mErr = int(len(all_test) * (1-mResult.acc_multi))\n",
    "print(\"Total Misclassifications: {} out of {}\".format(mErr, len(all_test)))\n",
    "mResult.ctab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 1943 out of 115662 (Recall: 0.995914\tFPR: 0.018790)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98121</td>\n",
       "      <td>1879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>15598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     98121   1879\n",
       "1        64  15598"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the binary classification performance, we use the previous predictions\n",
    "mPred_bin = np.copy(mPred)\n",
    "mPred_bin[mPred_bin > 0] = 1\n",
    "mResult.bin_results(all_test['Nature'], mPred_bin)\n",
    "mErr_bin = int(len(all_test) * (1-mResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(mErr_bin, len(all_test), mResult.rec, mResult.fpr))\n",
    "mResult.ctab_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSEMBLE CLASSIFIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \"individual\" binary classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing expl......done! Training time: 0.003998s\tInference time: 0.029844s\n",
      "Testing expl...\n",
      "...done! \tInference time: 0.031895s\n",
      "Training and testing recon......done! Training time: 0.002996s\tInference time: 0.032922s\n",
      "Testing recon...\n",
      "...done! \tInference time: 0.029931s\n",
      "Training and testing dos......done! Training time: 0.003985s\tInference time: 0.027873s\n",
      "Testing dos...\n",
      "...done! \tInference time: 0.029886s\n",
      "Training and testing shell......done! Training time: 0.002990s\tInference time: 0.028909s\n",
      "Testing shell...\n",
      "...done! \tInference time: 0.030430s\n",
      "Training and testing fuzz......done! Training time: 0.003983s\tInference time: 0.028914s\n",
      "Testing fuzz...\n",
      "...done! \tInference time: 0.031921s\n",
      "Training and testing bdoor......done! Training time: 0.003988s\tInference time: 0.027884s\n",
      "Testing bdoor...\n",
      "...done! \tInference time: 0.029894s\n",
      "Training and testing ana......done! Training time: 0.004064s\tInference time: 0.027867s\n",
      "Testing ana...\n",
      "...done! \tInference time: 0.029869s\n",
      "Total training time: 0.026004s\t AvgFPR: 0.006981\t AvgTPR: 0.998085\tTotal inference time: 0.213825s (fake: 0.204211s)\n"
     ]
    }
   ],
   "source": [
    "ensemble_df = pd.DataFrame()\n",
    "\n",
    "benign_train = benign_df[benign_df['is_train']==True]\n",
    "benign_test = benign_df[benign_df['is_test']==True]\n",
    "\n",
    "\n",
    "ens_time = 0\n",
    "ens_avgFPR = 0\n",
    "tot_TP = 0\n",
    "tot_P = 0\n",
    "ens_infer_time = 0\n",
    "fakeEns_infer_time = 0\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_train = {a}_df[{a}_df['is_train']==True]\")\n",
    "    exec(f\"{a}_test = {a}_df[{a}_df['is_test']==True]\")\n",
    "\n",
    "    exec(f\"train = pd.concat([benign_train, {a}_train])\")\n",
    "    exec(f\"test = pd.concat([benign_test, {a}_test])\")\n",
    "    \n",
    "    # We first train a classifier only on \"benign\" or on malicious samples of a specific attack. \n",
    "    # Afterwards, we immediately test it on a test-set having ONLY malicious samples of this specific attack\n",
    "    # Note: such \"testing\" is redundant, because it assumes that the classifier only receives the samples of the attack it is trained on!\n",
    "    exec(f\"{a}Clf, {a}Pred, {a}Result = develop_clf(train, test, features, clf_name='{a}', clf_type=base_clf, verbose=1)\")\n",
    "    exec(f\"fakeEns_infer_time += {a}Result.infer_time\")\n",
    "    # We now test the specific classifier on the ENTIRE test-set, thereby allowing to assess its performance also against malicious samples of different attacks\n",
    "    exec(f\"{a}_allPred, {a}_allResults, {a}Result.infer_time = evaluate_clf({a}Clf, all_test, features, clf_name='{a}', time={a}Result.time, verbose=1)\")\n",
    "\n",
    "    exec(f\"ensemble_df['{a}'] = {a}_allPred\")\n",
    "\n",
    "    exec(f\"tot_TP += ({a}Result.rec * len({a}_test))\")\n",
    "    exec(f\"tot_P += len({a}_test)\")\n",
    "\n",
    "\n",
    "    exec(f\"ens_avgFPR += {a}Result.fpr\")\n",
    "    exec(f\"ens_time += {a}Result.time\")\n",
    "    exec(f\"ens_infer_time +={a}Result.infer_time\")\n",
    "\n",
    "ens_avgFPR = ens_avgFPR / len(attack_names)\n",
    "ens_avgREC = tot_TP/tot_P\n",
    "\n",
    "print(\"Total training time: {:5f}s\\t AvgFPR: {:5f}\\t AvgTPR: {:5f}\\tTotal inference time: {:5f}s (fake: {:5f}s)\".\n",
    "      format(ens_time, ens_avgFPR, ens_avgREC, ens_infer_time, fakeEns_infer_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble (real assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we measure the combined performance of the entire ensemble\n",
    "# This is done with a logical or, or for majority voting (regulated by the \"agreement\" variable)\n",
    "\n",
    "ensemble_df[\"sum\"] = ensemble_df.sum(axis=1)\n",
    "#calculating \n",
    "ensemble_df[\"LOR\"] = (ensemble_df[\"sum\"]>0)\n",
    "\n",
    "#Appending Ground Truth\n",
    "temp = all_test['Nature'] #> 0)\n",
    "ensemble_df['True'] = ((temp.reset_index(drop=True)) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Logical OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 921 out of 115662 (Recall: 0.999808\tFPR: 0.009180)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>99082</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>3</td>\n",
       "      <td>15659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  99082    918\n",
       "True       3  15659"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enslorResult = Result(ensemble_df['True'], ensemble_df['LOR'], ens_time, ens_infer_time)\n",
    "enslorErr= int(len(all_test) * (1-enslorResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(enslorErr, len(all_test), enslorResult.rec, enslorResult.fpr))\n",
    "enslorResult.ctab_bin # you can also try with enslorResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting: at least 4 out of 7 classifiers must agree that a sample is malicious.\n",
      "Total Misclassifications: 1558 out of 115662 (Recall: 0.952496\tFPR: 0.008150)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>99185</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>744</td>\n",
       "      <td>14918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  99185    815\n",
       "True     744  14918"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_agree = math.ceil(agreement * len(attack_names))\n",
    "print(\"Voting: at least {} out of {} classifiers must agree that a sample is malicious.\".format(min_agree, len(attack_names)))\n",
    "ensemble_df[\"MAJV\"] = (ensemble_df[\"sum\"]>=min_agree)\n",
    "ensvotResult = Result(ensemble_df['True'], ensemble_df['MAJV'], ens_time, ens_infer_time)\n",
    "ensvotErr = int(len(all_test) * (1-ensvotResult.acc))\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(ensvotErr, len(all_test), ensvotResult.rec, ensvotResult.fpr))\n",
    "ensvotResult.ctab_bin # you can also try with ensvotResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Stacked Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 887 out of 115662 (Recall: 0.999808\tFPR: 0.008850)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99115</td>\n",
       "      <td>885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>15659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     99115    885\n",
       "1         3  15659"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "clf_list = []\n",
    "for a in attack_names:\n",
    "    exec(f\"clf_list.append({a}Clf)\")\n",
    "\n",
    "meta = choose_clf(clf_type=base_clf)\n",
    "sClf = StackingClassifier(classifiers=clf_list, meta_classifier=meta, fit_base_estimators=False, use_probas = False)\n",
    "s_timeStart = time.time()\n",
    "sClf.fit(all_train[features], all_train['Nature'])\n",
    "s_time = time.time() - s_timeStart + ens_time\n",
    "s_timeStart = time.time()\n",
    "sPred = sClf.predict(all_test[features])\n",
    "s_infer_time = time.time()-s_timeStart\n",
    "sResult = Result(all_test['Nature'], sPred, s_time, s_infer_time)\n",
    "if sResult.acc < sResult.acc_multi:\n",
    "    sResult.acc = sResult.acc_multi\n",
    "sErr = int(len(all_test) * (1-sResult.acc))\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sErr, len(all_test), sResult.rec, sResult.fpr))\n",
    "\n",
    "sResult.ctab_bin # you can also try with sResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open World Assessment: One attack against all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
      "      Binary CLF: TPR=0.996689\tFPR=0.009289\n",
      "      Multiclass (binarized) CLF: TPR=0.996520\tFPR=0.009903\n",
      "      EnsLOR CLF: TPR=0.999932\tFPR=0.009093\n",
      "      EnsVOT CLF: TPR=0.837641\tFPR=0.006321\n",
      "      EnsSTK CLF: TPR=0.999580\tFPR=0.008903\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "### The following code is a mixture of everything described insofar.\n",
    "### We are only focused on TPR and FPR here. We do not care about accuracy, adversarial robustness, or runtime.\n",
    "### These experiments are also done only on the \"Complete\" feature set\n",
    "\n",
    "\n",
    "oaac_bin_rec = 0\n",
    "oaac_bin_fpr = 0\n",
    "oaac_multi_rec = 0\n",
    "oaac_multi_fpr = 0\n",
    "oaac_enslor_rec = 0\n",
    "oaac_enslor_fpr = 0\n",
    "oaac_ensvot_rec = 0\n",
    "oaac_ensvot_fpr = 0\n",
    "oaac_ensstk_rec = 0\n",
    "oaac_ensstk_fpr = 0\n",
    "\n",
    "for u in attack_names: # u is the unknown attack\n",
    "    #print(u) # this is the unknown attack\n",
    "    #exec(f\"{u}_test = {u}_df[{u}_df['is_train']==False]\") # create test set by putting the \"test\" samples of u\n",
    "    exec(f\"{u}_test = pd.concat([benign_test, {u}_test])\") # add to the test set the \"benign\" test samples\n",
    "    exec(f\"{u}_train = benign_df[benign_df['is_train']==True]\") # compose the \"training\" set: start by putting the benign \"training\" samples\n",
    "    for a in attack_names: \n",
    "        # for every attack that is not u, add its training samples to the training set of u\n",
    "        if a==u:\n",
    "            continue\n",
    "        exec(f\"{u}_train = pd.concat([{u}_train, {a}_df[{a}_df['is_train']==True]])\")\n",
    "\n",
    "\n",
    "    # We have created the training and testing set. Now we must train and test a binary classifier by following the standard procedure\n",
    "    ########## BINARY CLASSIFIER ##########\n",
    "    exec(f\"{u}_oaac_bClf, {u}_oaac_bPred, {u}_oaac_bResult = develop_clf({u}_train, {u}_test, features, clf_name='{u}_oaac_bin', label='Nature', clf_type=base_clf)\")\n",
    "\n",
    "\n",
    "    ########## Multiclass CLASSIFIER ########## --> Train, then test only on binary\n",
    "    exec(f\"{u}_oaac_mClf, {u}_oaac_mPred, {u}_oaac_mResult = develop_clf({u}_train, {u}_test, features, clf_name='{u}_oaac_multi', label='Label_cat', clf_type=base_clf)\")\n",
    "    exec(f\"{u}_oaac_mPred_bin = np.copy({u}_oaac_mPred)\")\n",
    "    exec(f\"{u}_oaac_mPred_bin[{u}_oaac_mPred_bin > 0] = 1\")\n",
    "    exec(f\"{u}_oaac_mResult.bin_results({u}_test['Nature'], {u}_oaac_mPred_bin)\")\n",
    "\n",
    "\n",
    "    ######### Ensemble ##############\n",
    "    # send the samples in TEST to all the classifiers of the ensemble (which are already trained), aside from the one focusing on u\n",
    "    exec(f\"{u}_oaac_ens_df = pd.DataFrame()\")\n",
    "    for a in attack_names:    \n",
    "        if a==u:\n",
    "                continue\n",
    "        exec(f\"{a}_{u}Pred, {a}_{u}Results, {a}_{u}_infer_time = evaluate_clf({a}Clf, {u}_test, features, clf_name='{a}_{u}', time={a}Result.time)\")\n",
    "        exec(f\"{u}_oaac_ens_df['{a}'] = {a}_{u}Pred\")\n",
    "\n",
    "    # now we have the dataframe with all the predictions, let's see the aggregate results\n",
    "    exec(f\"{u}_oaac_ens_df['sum'] = {u}_oaac_ens_df.sum(axis=1)\")\n",
    "    exec(f\"{u}_oaac_ens_df['LOR'] = ({u}_oaac_ens_df['sum']>0)\")\n",
    "    exec(f\"temp = {u}_test['Nature'] #> 0)\")\n",
    "    exec(f\"{u}_oaac_ens_df['True'] = ((temp.reset_index(drop=True)) > 0)\")\n",
    "    exec(f\"{u}_oaac_enslorResult = Result({u}_oaac_ens_df['True'], {u}_oaac_ens_df['LOR'], (ens_time-{u}Result.time), (ens_infer_time-{u}Result.infer_time))\")\n",
    "\n",
    "    # now we consider the majority voting of the ensemble\n",
    "    exec(f\"{u}_oaac_ens_df['MAJV'] = ({u}_oaac_ens_df['sum']>=min_agree)\")\n",
    "    exec(f\"{u}_oaac_ensvotResult = Result({u}_oaac_ens_df['True'], {u}_oaac_ens_df['MAJV'], (ens_time-{u}Result.time), (ens_infer_time-{u}Result.infer_time))\")\n",
    "\n",
    "    # finally, let's use the stacking ensemble\n",
    "    exec(f\"{u}_clf_list = []\")\n",
    "    for a in attack_names:\n",
    "        if a==u:\n",
    "                continue\n",
    "        exec(f\"{u}_clf_list.append({a}Clf)\")\n",
    "    exec(f\"{u}_oaac_sClf = StackingClassifier(classifiers={u}_clf_list, meta_classifier=meta, fit_base_estimators=False, use_probas = False)\")\n",
    "    exec(f\"{u}_oaac_sClf.fit({u}_train[features], {u}_train['Nature'])\")\n",
    "    exec(f\"{u}_oaac_sPred = {u}_oaac_sClf.predict({u}_test[features])\")\n",
    "    exec(f\"{u}_oaac_sResult = Result({u}_test['Nature'], {u}_oaac_sPred, (ens_time-{u}Result.time), (ens_infer_time-{u}Result.infer_time))\")\n",
    "\n",
    "    # Updating results\n",
    "    exec(f\"oaac_bin_rec += {u}_oaac_bResult.rec\")\n",
    "    exec(f\"oaac_bin_fpr += {u}_oaac_bResult.fpr\")\n",
    "    exec(f\"oaac_multi_rec += {u}_oaac_mResult.rec\")\n",
    "    exec(f\"oaac_multi_fpr += {u}_oaac_mResult.fpr\")\n",
    "    exec(f\"oaac_enslor_rec += {u}_oaac_enslorResult.rec\")\n",
    "    exec(f\"oaac_enslor_fpr += {u}_oaac_enslorResult.fpr\")\n",
    "    exec(f\"oaac_ensvot_rec += {u}_oaac_ensvotResult.rec\")\n",
    "    exec(f\"oaac_ensvot_fpr += {u}_oaac_ensvotResult.fpr\")\n",
    "    exec(f\"oaac_ensstk_rec += {u}_oaac_sResult.rec\")\n",
    "    exec(f\"oaac_ensstk_fpr += {u}_oaac_sResult.fpr\")\n",
    "\n",
    "# Finalizing averages\n",
    "oaac_bin_rec /= len(attack_names)\n",
    "oaac_bin_fpr /= len(attack_names)\n",
    "oaac_multi_rec /= len(attack_names)\n",
    "oaac_multi_fpr /= len(attack_names)\n",
    "oaac_enslor_rec /= len(attack_names)\n",
    "oaac_enslor_fpr /= len(attack_names)\n",
    "oaac_ensvot_rec /= len(attack_names)\n",
    "oaac_ensvot_fpr /= len(attack_names)\n",
    "oaac_ensstk_rec /= len(attack_names)\n",
    "oaac_ensstk_fpr /= len(attack_names)\n",
    "\n",
    "\n",
    "print('''Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
    "      Binary CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      Multiclass (binarized) CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      EnsLOR CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      EnsVOT CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      EnsSTK CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      '''.format(oaac_bin_rec, oaac_bin_fpr,\n",
    "                 oaac_multi_rec, oaac_multi_fpr,\n",
    "                 oaac_enslor_rec, oaac_enslor_fpr,\n",
    "                 oaac_ensvot_rec, oaac_ensvot_fpr,\n",
    "                 oaac_ensstk_rec, oaac_ensstk_fpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment on Essential feature set (and Adversarial attacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BINARY CLASSIFIER (Essential features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing adv_bin......done! Training time: 0.005980s\tInference time: 0.019049s\n",
      "Total Misclassifications: 7949 out of 115662 (Recall: 0.985379\tFPR: 0.077210)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92279</td>\n",
       "      <td>7721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>229</td>\n",
       "      <td>15433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     92279   7721\n",
       "1       229  15433"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_bClf, sma_bPred, sma_bResult = develop_clf(all_train, all_test, small_features, clf_name='adv_bin', label='Nature', clf_type=base_clf, verbose=1)\n",
    "sma_bErr = int(len(all_test) * (1-sma_bResult.acc))\n",
    "if (sma_bResult.acc == 0):\n",
    "    sma_bErr = int(len(all_test) * (1-sma_bResult.acc_multi))\n",
    "else:\n",
    "    sma_bErr = int(len(all_test) * (1-sma_bResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_bErr, len(all_test), sma_bResult.rec, sma_bResult.fpr))\n",
    "pd.crosstab(all_test['Nature'], sma_bPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial Attack against Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.990\n",
      "Adversarial Recall (attack): 0.994\n"
     ]
    }
   ],
   "source": [
    "# Note that the adversarial attacks only affect a subset of the initial set of malicious samples\n",
    "# Hence, we compute the classification performance also on this subset for a far comparison\n",
    "\n",
    "\n",
    "\n",
    "adv_bPred_base = sma_bClf.predict(mal_base[small_features])\n",
    "adv_bPred_adv = sma_bClf.predict(mal_adv[small_features])\n",
    "adv_bin_base_rec =  recall_score(mal_base['Nature'], adv_bPred_base, pos_label=1)\n",
    "adv_bin_adv_rec = recall_score(mal_adv['Nature'], adv_bPred_adv, pos_label=1)\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_bin_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_bin_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classifier - cascade (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing sma_mc......done! Training time: 0.006969s\tInference time: 0.004983s\n",
      "Total Misclassifications: 5633 out of 15662\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3374</td>\n",
       "      <td>285</td>\n",
       "      <td>1334</td>\n",
       "      <td>181</td>\n",
       "      <td>528</td>\n",
       "      <td>306</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>2030</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>142</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>246</td>\n",
       "      <td>55</td>\n",
       "      <td>301</td>\n",
       "      <td>43</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>265</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>198</td>\n",
       "      <td>78</td>\n",
       "      <td>139</td>\n",
       "      <td>20</td>\n",
       "      <td>3720</td>\n",
       "      <td>155</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>68</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>149</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>119</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred     1     2     3    4     5    6    7\n",
       "True                                       \n",
       "1     3374   285  1334  181   528  306  302\n",
       "2       75  2030   130    0    55  142  123\n",
       "3      246    55   301   43   161  161  191\n",
       "4        9     0     7  265     3    1    0\n",
       "5      198    78   139   20  3720  155  152\n",
       "6       21    33    68   19    19  149  124\n",
       "7       13    39    89    0    10  119  189"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_mcClf, sma_mcPred, sma_mcResult = develop_clf(malicious_train, malicious_test, small_features, clf_name='sma_mc', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "sma_mcErr = int(len(malicious_test) * (1-sma_mcResult.acc_multi))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {}\".format(sma_mcErr, len(malicious_test)))\n",
    "pd.crosstab(malicious_test['Label_cat'], sma_mcPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications (among the malicious samples): 5529 out of 15433\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3276</td>\n",
       "      <td>285</td>\n",
       "      <td>1282</td>\n",
       "      <td>181</td>\n",
       "      <td>521</td>\n",
       "      <td>306</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>2030</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>142</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>225</td>\n",
       "      <td>55</td>\n",
       "      <td>290</td>\n",
       "      <td>43</td>\n",
       "      <td>156</td>\n",
       "      <td>161</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>265</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>195</td>\n",
       "      <td>78</td>\n",
       "      <td>131</td>\n",
       "      <td>20</td>\n",
       "      <td>3704</td>\n",
       "      <td>155</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>65</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>149</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>119</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred     1     2     3    4     5    6    7\n",
       "True                                       \n",
       "1     3276   285  1282  181   521  306  301\n",
       "2       75  2030   130    0    55  142  123\n",
       "3      225    55   290   43   156  161  190\n",
       "4        9     0     7  265     3    1    0\n",
       "5      195    78   131   20  3704  155  150\n",
       "6       20    33    65   19    19  149  124\n",
       "7       13    39    89    0    10  119  189"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We select the samples flagged as malicious by the initial classifier.\n",
    "# Of course, samples flagged as malicious that are NOT actually malicious will always be misclassified\n",
    "\n",
    "all_test['sma_bPred'] = sma_bPred\n",
    "sma_mc_test = all_test[(all_test['sma_bPred']==1) & (all_test['Nature']==1)]\n",
    "if (len(sma_mc_test)==0):\n",
    "    # in this case, this classifier receives nothing\n",
    "    print(\"There is no malicious sample flagged as malicious to analyze!\")\n",
    "\n",
    "sma_mcPred_m = sma_mcClf.predict(sma_mc_test[small_features])\n",
    "sma_mcResult.acc_multic = accuracy_score(sma_mc_test['Label_cat'], sma_mcPred_m, normalize=True, sample_weight=None)\n",
    "sma_mcErr_m = int((1-sma_mcResult.acc_multic) * len(sma_mc_test))\n",
    "print(\"Total Misclassifications (among the malicious samples): {} out of {}\".format(sma_mcErr_m, len(sma_mc_test)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.crosstab(sma_mc_test['Label_cat'], sma_mcPred_m, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This classifier also analyzed 7721 benign samples that were incorrectly labelled as 'malicious' by the (small) binary classifier\n",
      "Hence, this (small) classifier was tested on 23154 samples, of which 13250 have been misclassified\n"
     ]
    }
   ],
   "source": [
    "## Note: We also accounted for the false positives of the first binary classifier (all of which have been considered as misclassifications)\n",
    "sma_bin_falsePositives = int(sma_bResult.fpr * len(benign_test))\n",
    "print(\"This classifier also analyzed {} benign samples that were incorrectly labelled as 'malicious' by the (small) binary classifier\".format(sma_bin_falsePositives))\n",
    "\n",
    "print(\"Hence, this (small) classifier was tested on {} samples, of which {} have been misclassified\".format(len(sma_mc_test)+sma_bin_falsePositives, sma_bin_falsePositives+sma_mcErr_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classifier - stand-alone (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing sma_m......done! Training time: 0.006987s\tInference time: 0.021924s\n",
      "Total Misclassifications: 15038 out of 115662\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90688</td>\n",
       "      <td>3199</td>\n",
       "      <td>21</td>\n",
       "      <td>4618</td>\n",
       "      <td>203</td>\n",
       "      <td>1082</td>\n",
       "      <td>106</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>313</td>\n",
       "      <td>3337</td>\n",
       "      <td>315</td>\n",
       "      <td>988</td>\n",
       "      <td>183</td>\n",
       "      <td>554</td>\n",
       "      <td>307</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>2032</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>143</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "      <td>235</td>\n",
       "      <td>55</td>\n",
       "      <td>267</td>\n",
       "      <td>47</td>\n",
       "      <td>159</td>\n",
       "      <td>158</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>262</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>86</td>\n",
       "      <td>135</td>\n",
       "      <td>24</td>\n",
       "      <td>3697</td>\n",
       "      <td>159</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>33</td>\n",
       "      <td>63</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>149</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>119</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0     1     2     3    4     5    6    7\n",
       "True                                              \n",
       "0     90688  3199    21  4618  203  1082  106   83\n",
       "1       313  3337   315   988  183   554  307  313\n",
       "2         0    73  2032   127    0    56  143  124\n",
       "3        46   235    55   267   47   159  158  191\n",
       "4         0    13     0     7  262     3    0    0\n",
       "5         0   209    86   135   24  3697  159  152\n",
       "6         2    27    33    63   12    22  149  125\n",
       "7         0    13    39    87    0    10  119  191"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_mClf, sma_mPred, sma_mResult = develop_clf(all_train, all_test, small_features, clf_name='sma_m', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "sma_mErr = int(len(all_test) * (1-sma_mResult.acc_multi))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {}\".format(sma_mErr, len(all_test)))\n",
    "sma_mResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Classifier: Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 9672 out of 115662 (Recall: 0.976951\tFPR: 0.093120)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90688</td>\n",
       "      <td>9312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>361</td>\n",
       "      <td>15301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     90688   9312\n",
       "1       361  15301"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## MULTI-CLASS CLASSIFIER - BINARY ##########\n",
    "sma_mPred_bin = np.copy(sma_mPred)\n",
    "sma_mPred_bin[sma_mPred_bin > 0] = 1\n",
    "sma_mResult.bin_results(all_test['Nature'], sma_mPred_bin)\n",
    "sma_mErr_bin = int(len(all_test) * (1-sma_mResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_mErr_bin, len(all_test), sma_mResult.rec, sma_mResult.fpr))\n",
    "sma_mResult.ctab_bin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial Attack against the Multiclass Classifier (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.993\n",
      "Adversarial Recall (attack): 0.993\n"
     ]
    }
   ],
   "source": [
    "adv_mPred_base = sma_mClf.predict(mal_base[small_features])\n",
    "adv_mPred_adv = sma_mClf.predict(mal_adv[small_features])\n",
    "\n",
    "adv_mPred_base_bin = np.copy(adv_mPred_base)\n",
    "adv_mPred_base_bin[adv_mPred_base_bin > 0] = 1\n",
    "\n",
    "adv_mPred_adv_bin = np.copy(adv_mPred_adv)\n",
    "adv_mPred_adv_bin[adv_mPred_adv_bin > 0] = 1\n",
    "\n",
    "\n",
    "adv_multi_base_rec =  recall_score(mal_base['Nature'], adv_mPred_base_bin)\n",
    "adv_multi_adv_rec = recall_score(mal_adv['Nature'], adv_mPred_adv_bin)\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_multi_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_multi_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Classifiers (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing sma_expl......done! Training time: 0.003992s\tInference time: 0.017864s\n",
      "Testing sma_expl...\n",
      "...done! \tInference time: 0.019932s\n",
      "Training and testing sma_recon......done! Training time: 0.003988s\tInference time: 0.013952s\n",
      "Testing sma_recon...\n",
      "...done! \tInference time: 0.018929s\n",
      "Training and testing sma_dos......done! Training time: 0.003987s\tInference time: 0.016934s\n",
      "Testing sma_dos...\n",
      "...done! \tInference time: 0.017939s\n",
      "Training and testing sma_shell......done! Training time: 0.003979s\tInference time: 0.014958s\n",
      "Testing sma_shell...\n",
      "...done! \tInference time: 0.017938s\n",
      "Training and testing sma_fuzz......done! Training time: 0.003990s\tInference time: 0.016016s\n",
      "Testing sma_fuzz...\n",
      "...done! \tInference time: 0.016943s\n",
      "Training and testing sma_bdoor......done! Training time: 0.003903s\tInference time: 0.013960s\n",
      "Testing sma_bdoor...\n",
      "...done! \tInference time: 0.016943s\n",
      "Training and testing sma_ana......done! Training time: 0.004001s\tInference time: 0.013935s\n",
      "Testing sma_ana...\n",
      "...done! \tInference time: 0.017867s\n",
      "Total training time: 0.027839s\t AvgFPR: 0.015476\t AvgTPR: 0.966543\tTotal inference time: 0.126491s (fake: 0.107619s)\n"
     ]
    }
   ],
   "source": [
    "sma_ensemble_df = pd.DataFrame()\n",
    "adv_ensemble_df_base = pd.DataFrame()\n",
    "adv_ensemble_df_adv = pd.DataFrame()\n",
    "\n",
    "benign_train = benign_df[benign_df['is_train']==True]\n",
    "benign_test = benign_df[benign_df['is_test']==True]\n",
    "\n",
    "\n",
    "sma_ens_time = 0\n",
    "sma_ens_avgFPR = 0\n",
    "sma_tot_TP = 0\n",
    "sma_tot_P = 0\n",
    "sma_ens_infer_time = 0\n",
    "sma_fakeEns_infer_time = 0\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_train = {a}_df[{a}_df['is_train']==True]\")\n",
    "    exec(f\"{a}_test = {a}_df[{a}_df['is_test']==True]\")\n",
    "\n",
    "    exec(f\"train = pd.concat([benign_train, {a}_train])\")\n",
    "    exec(f\"test = pd.concat([benign_test, {a}_test])\")\n",
    "\n",
    "    exec(f\"sma_{a}Clf, sma_{a}Pred, sma_{a}Result = develop_clf(train, test, small_features, clf_name='sma_{a}', clf_type=base_clf, verbose=1)\")\n",
    "    exec(f\"sma_fakeEns_infer_time += sma_{a}Result.infer_time\")\n",
    "    \n",
    "    exec(f\"sma_{a}_allPred, sma_{a}_allResults, sma_{a}Result.infer_time = evaluate_clf(sma_{a}Clf, all_test, small_features, clf_name='sma_{a}', time=sma_{a}Result.time, verbose=1)\")\n",
    "    exec(f\"sma_ensemble_df['{a}'] = sma_{a}_allPred\")\n",
    "\n",
    "    exec(f\"adv_{a}Pred_base = sma_{a}Clf.predict(mal_base[small_features])\")\n",
    "    exec(f\"adv_{a}Pred_adv = sma_{a}Clf.predict(mal_adv[small_features])\")\n",
    "    exec(f\"adv_ensemble_df_base['{a}'] = adv_{a}Pred_base\")\n",
    "    exec(f\"adv_ensemble_df_adv['{a}'] = adv_{a}Pred_adv\")\n",
    "\n",
    "\n",
    "\n",
    "    exec(f\"sma_tot_TP += (sma_{a}Result.rec * len({a}_test))\")\n",
    "    exec(f\"sma_tot_P += len({a}_test)\")\n",
    "\n",
    "\n",
    "    exec(f\"sma_ens_avgFPR += sma_{a}Result.fpr\")\n",
    "    exec(f\"sma_ens_time += sma_{a}Result.time\")\n",
    "    exec(f\"sma_ens_infer_time +=sma_{a}Result.infer_time\")\n",
    "\n",
    "\n",
    "\n",
    "sma_ens_avgFPR = sma_ens_avgFPR / len(attack_names)\n",
    "sma_ens_avgREC = sma_tot_TP/sma_tot_P\n",
    "\n",
    "\n",
    "print(\"Total training time: {:5f}s\\t AvgFPR: {:5f}\\t AvgTPR: {:5f}\\tTotal inference time: {:5f}s (fake: {:5f}s)\".\n",
    "      format(sma_ens_time, sma_ens_avgFPR, sma_ens_avgREC, sma_ens_infer_time, sma_fakeEns_infer_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing real Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma_ensemble_df[\"sum\"] = sma_ensemble_df.sum(axis=1)\n",
    "sma_ensemble_df[\"LOR\"] = (sma_ensemble_df[\"sum\"]>0)\n",
    "temp = all_test['Nature'] \n",
    "sma_ensemble_df['True'] = ((temp.reset_index(drop=True)) > 0)\n",
    "\n",
    "adv_ensemble_df_base[\"sum\"] = adv_ensemble_df_base.sum(axis=1)\n",
    "adv_ensemble_df_base[\"LOR\"] = (adv_ensemble_df_base[\"sum\"]>0)\n",
    "temp = mal_base['Nature']\n",
    "adv_ensemble_df_base['True'] = ((temp.reset_index(drop=True)) > 0)\n",
    "\n",
    "\n",
    "adv_ensemble_df_adv[\"sum\"] = adv_ensemble_df_adv.sum(axis=1)\n",
    "adv_ensemble_df_adv[\"LOR\"] = (adv_ensemble_df_adv[\"sum\"]>0)\n",
    "temp = mal_adv['Nature']\n",
    "adv_ensemble_df_adv['True'] = ((temp.reset_index(drop=True)) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Logical OR (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 6246 out of 115662 (Recall: 0.988124\tFPR: 0.060600)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>93940</td>\n",
       "      <td>6060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>186</td>\n",
       "      <td>15476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  93940   6060\n",
       "True     186  15476"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_enslorResult = Result(sma_ensemble_df['True'], sma_ensemble_df['LOR'], sma_ens_time, sma_ens_infer_time)\n",
    "sma_enslorErr= int(len(all_test) * (1-sma_enslorResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_enslorErr, len(all_test), sma_enslorResult.rec, sma_enslorResult.fpr))\n",
    "sma_enslorResult.ctab_bin # you can also try with sma_enslorResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logical OR: Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.993\n",
      "Adversarial Recall (attack): 0.992\n"
     ]
    }
   ],
   "source": [
    "adv_enslor_base_rec = recall_score(mal_base['Nature'], adv_ensemble_df_base[\"LOR\"])\n",
    "adv_enslor_adv_rec = recall_score(mal_adv['Nature'], adv_ensemble_df_adv[\"LOR\"])\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_enslor_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_enslor_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Majority Voting (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 5241 out of 115662 (Recall: 0.708530\tFPR: 0.006760)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>99324</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>4565</td>\n",
       "      <td>11097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  99324    676\n",
       "True    4565  11097"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_ensemble_df[\"MAJV\"] = (sma_ensemble_df[\"sum\"]>=min_agree)\n",
    "sma_ensvotResult = Result(sma_ensemble_df['True'], sma_ensemble_df['MAJV'], sma_ens_time, sma_ens_infer_time)\n",
    "sma_ensvotErr = int(len(all_test) * (1-sma_ensvotResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_ensvotErr, len(all_test), sma_ensvotResult.rec, sma_ensvotResult.fpr))\n",
    "sma_ensvotResult.ctab_bin # you can also try with sma_ensvotResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Majority Voting: Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.957\n",
      "Adversarial Recall (attack): 0.002\n"
     ]
    }
   ],
   "source": [
    "adv_ensemble_df_base[\"MAJV\"] = (adv_ensemble_df_base[\"sum\"]>=min_agree)\n",
    "adv_ensemble_df_adv[\"MAJV\"] = (adv_ensemble_df_adv[\"sum\"]>=min_agree)\n",
    "\n",
    "adv_ensvot_base_rec = recall_score(mal_base['Nature'], adv_ensemble_df_base[\"MAJV\"])\n",
    "adv_ensvot_adv_rec = recall_score(mal_adv['Nature'], adv_ensemble_df_adv[\"MAJV\"])\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_ensvot_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_ensvot_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Stacked Classifier (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 6187 out of 115662 (Recall: 0.985570\tFPR: 0.059620)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94038</td>\n",
       "      <td>5962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>226</td>\n",
       "      <td>15436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     94038   5962\n",
       "1       226  15436"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_list = []\n",
    "for a in attack_names:\n",
    "    exec(f\"clf_list.append(sma_{a}Clf)\")\n",
    "\n",
    "sma_sClf = StackingClassifier(classifiers=clf_list, meta_classifier=meta, fit_base_estimators=False, use_probas = False)\n",
    "s_timeStart = time.time()\n",
    "sma_sClf.fit(all_train[small_features], all_train['Nature'])\n",
    "sma_s_time = time.time() - s_timeStart + sma_ens_time\n",
    "s_timeStart = time.time()\n",
    "sma_sPred = sma_sClf.predict(all_test[small_features])\n",
    "sma_s_infer_time = time.time()-s_timeStart\n",
    "sma_sResult = Result(all_test['Nature'], sma_sPred, sma_s_time, sma_s_infer_time)\n",
    "if sma_sResult.acc < sma_sResult.acc_multi:\n",
    "    sma_sResult.acc = sma_sResult.acc_multi\n",
    "sma_sErr = int(len(all_test) * (1-sma_sResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_sErr, len(all_test), sma_sResult.rec, sma_sResult.fpr))\n",
    "\n",
    "sma_sResult.ctab_bin # you can also try with sma_sResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked Classifier: Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.987\n",
      "Adversarial Recall (attack): 0.866\n"
     ]
    }
   ],
   "source": [
    "adv_sPred_base = sma_sClf.predict(mal_base[small_features])\n",
    "adv_sPred_adv = sma_sClf.predict(mal_adv[small_features])\n",
    "\n",
    "adv_ensstk_base_rec =  recall_score(mal_base['Nature'], adv_sPred_base)\n",
    "adv_ensstk_adv_rec = recall_score(mal_adv['Nature'], adv_sPred_adv)\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_ensstk_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_ensstk_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now inspect the results by referring to the \"Result\" variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR 0.9983399310432894 0.9959136764142511 0.9980845358191802 0.999808453581918 0.9524964883156685 0.999808453581918 \n",
      "FPR 0.008879999999999999 0.018789999999999973 0.006981428571428551 0.009179999999999966 0.00814999999999999 0.008850000000000025 \n",
      "Training Time 0.0059814453125 0.008982419967651367 0.026004314422607422 0.026004314422607422 0.026004314422607422 0.038960933685302734 \n",
      "Inference Time 0.03196406364440918 0.035875797271728516 0.20421147346496582 0.21382498741149902 0.21382498741149902 0.1534864902496338 \n",
      "Accuracy 0.934360464110944 0.9832010513392471 0.6320030698388335 0.6320393308645128\n"
     ]
    }
   ],
   "source": [
    "## BASELINE RESULTS (on Complete Feature Set)\n",
    "print(\n",
    "    \"TPR\",\n",
    "    bResult.rec,\n",
    "    mResult.rec,\n",
    "    ens_avgREC,\n",
    "    enslorResult.rec,\n",
    "    ensvotResult.rec,\n",
    "    sResult.rec,\n",
    "    \"\\nFPR\",\n",
    "    bResult.fpr,\n",
    "    mResult.fpr,\n",
    "    ens_avgFPR,\n",
    "    enslorResult.fpr,\n",
    "    ensvotResult.fpr,\n",
    "    sResult.fpr,\n",
    "    \"\\nTraining Time\",\n",
    "    bResult.time,\n",
    "    mResult.time,\n",
    "    ens_time,\n",
    "    enslorResult.time,\n",
    "    ensvotResult.time,\n",
    "    sResult.time,\n",
    "    \"\\nInference Time\",\n",
    "    bResult.infer_time,\n",
    "    mResult.infer_time,\n",
    "    fakeEns_infer_time,\n",
    "    enslorResult.infer_time,\n",
    "    ensvotResult.infer_time,\n",
    "    sResult.infer_time, \n",
    "    \"\\nAccuracy\",\n",
    "    mResult.acc_multi,   # This is the accuracy on the multiclassification\n",
    "    mResult.acc,         # This is the accuracy on the binary classification\n",
    "    mcResult.acc_multic, # This is the accuracy on the multiclassification AFTER the output of the binary classifier (it does not account for benign samples, which are false positives)\n",
    "    mcResult.acc_multi   # This is the accuracy on the multiclassification on the whole test portion of the malicious dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR 0.985378623419742 0.9769505810241349 0.9665432256416805 0.9881241220789171 0.708530200485251 0.9855701698378241 \n",
      "FPR 0.07721 0.09311999999999998 0.015475714285714306 0.06059999999999999 0.006759999999999988 0.059620000000000006 \n",
      "Training Time 0.005980491638183594 0.006987333297729492 0.027839183807373047 0.027839183807373047 0.027839183807373047 0.04079079627990723 \n",
      "Inference Time 0.01904892921447754 0.021924257278442383 0.10761857032775879 0.12649106979370117 0.12649106979370117 0.09367752075195312 \n",
      "Accuracy 0.8699745811070188 0.9163683837388252 0.6416769260675177 0.6402758268420381\n"
     ]
    }
   ],
   "source": [
    "## BASELINE RESULTS (on Essential Feature Set)\n",
    "print(\n",
    "    \"TPR\",\n",
    "    sma_bResult.rec,\n",
    "    sma_mResult.rec,\n",
    "    sma_ens_avgREC,\n",
    "    sma_enslorResult.rec,\n",
    "    sma_ensvotResult.rec,\n",
    "    sma_sResult.rec,\n",
    "    \"\\nFPR\",\n",
    "    sma_bResult.fpr,\n",
    "    sma_mResult.fpr,\n",
    "    sma_ens_avgFPR,\n",
    "    sma_enslorResult.fpr,\n",
    "    sma_ensvotResult.fpr,\n",
    "    sma_sResult.fpr,\n",
    "    \"\\nTraining Time\",\n",
    "    sma_bResult.time,\n",
    "    sma_mResult.time,\n",
    "    sma_ens_time,\n",
    "    sma_enslorResult.time,\n",
    "    sma_ensvotResult.time,\n",
    "    sma_sResult.time,\n",
    "    \"\\nInference Time\",\n",
    "    sma_bResult.infer_time,\n",
    "    sma_mResult.infer_time,\n",
    "    sma_fakeEns_infer_time,\n",
    "    sma_enslorResult.infer_time,\n",
    "    sma_ensvotResult.infer_time,\n",
    "    sma_sResult.infer_time, \n",
    "    \"\\nAccuracy\",\n",
    "    sma_mResult.acc_multi,   # This is the accuracy on the multiclassification\n",
    "    sma_mResult.acc,         # This is the accuracy on the binary classification\n",
    "    sma_mcResult.acc_multic, # This is the accuracy on the multiclassification AFTER the output of the binary classifier (it does not account for benign samples, which are false positives)\n",
    "    sma_mcResult.acc_multi   # This is the accuracy on the multiclassification on the whole test portion of the malicious dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
      "      BD: TPR=0.996689\tFPR=0.009289\n",
      "      MD (binarized) CLF: TPR=0.996520\tFPR=0.009903\n",
      "      ED-o: TPR=0.999932\tFPR=0.009093\n",
      "      ED-v: TPR=0.837641\tFPR=0.006321\n",
      "      ED-s: TPR=0.999580\tFPR=0.008903\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "## Open World: One attack against all (averaged results)\n",
    "\n",
    "print('''Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
    "      BD: TPR={:5f}\\tFPR={:5f}\n",
    "      MD (binarized) CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      ED-o: TPR={:5f}\\tFPR={:5f}\n",
    "      ED-v: TPR={:5f}\\tFPR={:5f}\n",
    "      ED-s: TPR={:5f}\\tFPR={:5f}\n",
    "      '''.format(oaac_bin_rec, oaac_bin_fpr,\n",
    "                 oaac_multi_rec, oaac_multi_fpr,\n",
    "                 oaac_enslor_rec, oaac_enslor_fpr,\n",
    "                 oaac_ensvot_rec, oaac_ensvot_fpr,\n",
    "                 oaac_ensstk_rec, oaac_ensstk_fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BD (before, after): 0.9900432900432901 0.9939393939393939 \n",
      "MD  (before, after): 0.9926406926406927 0.9926406926406927 \n",
      "ED-o  (before, after): 0.9926406926406927 0.9917748917748918 \n",
      "ED-v  (before, after): 0.9571428571428572 0.0021645021645021645 \n",
      "ED-s  (before, after): 0.9874458874458875 0.8662337662337662\n"
     ]
    }
   ],
   "source": [
    "## Adversarial Attacks \n",
    "\n",
    "print(\n",
    "    \"BD (before, after):\",\n",
    "    adv_bin_base_rec,\n",
    "    adv_bin_adv_rec,\n",
    "    \"\\nMD  (before, after):\",\n",
    "    adv_multi_base_rec,\n",
    "    adv_multi_adv_rec,\n",
    "    \"\\nED-o  (before, after):\",\n",
    "    adv_enslor_base_rec,\n",
    "    adv_enslor_adv_rec, \n",
    "    \"\\nED-v  (before, after):\",\n",
    "    adv_ensvot_base_rec,\n",
    "    adv_ensvot_adv_rec, \n",
    "    \"\\nED-s  (before, after):\",\n",
    "    adv_ensstk_base_rec, \n",
    "    adv_ensstk_adv_rec,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
