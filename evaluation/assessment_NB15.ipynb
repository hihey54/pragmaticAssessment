{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from supportFunctions import *\n",
    "\n",
    "root_folder = \"..\\\\data\\\\NB15\\\\flows\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PARAMETERS #####\n",
    "\n",
    "## Generic parameters\n",
    "temporal = True # used to determine if this evaluation assumes a \"temporal\" dependency among its samples\n",
    "base_clf = 'dt' # name of the classifier used for this \"run\". Available names: ['dt', 'rf', 'hgb', 'lr']. You can add more by editing the supportFunctions file\n",
    "test_size = 0.2 # proportion of the dataset used for testing. We always kept it fixed to 0.2 for our paper\n",
    "train_size = 100 # proportion of the REMAINING data that are used for training (if >1, then it will take that exact amount). To reproduce the results of the paper, use: 100 (for \"limited\" training data) or 0.2 or 0.5 or 0.99 (for scarce, moderate, abundant training data, respectively) \n",
    "agreement = 0.5 # from 0 to 1. Proportion of classifiers that must agree on an attack (for the ensemble). This is fixed in our paper.\n",
    "max_size = 500000 ## maximum amount of samples to include when creating the initial dataframes. This is fixed in our paper\n",
    "max_size_atk = int(max_size / 3) # maximum amount of malicious samples per class. This is fixed in our paper\n",
    "\n",
    "## Adversarial Attacks parameters\n",
    "atk_intensity = 1 # send 100 packets, each of 100 bytes, over 100 seconds\n",
    "pkt_intensity = atk_intensity * 100 # \n",
    "byt_intensity = pkt_intensity * 100 # \n",
    "dur_intensity = atk_intensity * 100 # consider seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading input data\n",
    "\n",
    "malicious_folder = root_folder + \"malicious/\"\n",
    "\n",
    "benign_file = root_folder + \"benign.csv\"\n",
    "benign_df = pd.read_csv(benign_file, header='infer', index_col=0)\n",
    "benign_df = benign_df.sample(min(max_size, len(benign_df)))\n",
    "#sort by timestamp\n",
    "if temporal == True:\n",
    "    pass # already sorted\n",
    "benign_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# attack_names = ['expl', 'recon', 'dos', 'shell', 'fuzz', 'bdoor', 'ana', 'worm', 'other']\n",
    "attack_names = ['expl', 'recon', 'dos', 'shell', 'fuzz', 'bdoor', 'ana']\n",
    "\n",
    "expl_file = malicious_folder + \"expl.csv\"\n",
    "recon_file = malicious_folder + \"recon.csv\"\n",
    "dos_file = malicious_folder + \"dos.csv\"\n",
    "shell_file = malicious_folder + \"shell.csv\"\n",
    "fuzz_file = malicious_folder + \"fuzz.csv\"\n",
    "worm_file = malicious_folder + \"worm.csv\" # excluded, only 174 samples\n",
    "bdoor_file = malicious_folder + \"bdoor.csv\"\n",
    "ana_file = malicious_folder + \"ana.csv\"\n",
    "other_file = malicious_folder + \"other.csv\" # excluded due to mismatch\n",
    "\n",
    "\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_df = pd.read_csv({a}_file, header='infer', index_col=0)\")\n",
    "    exec(f\"{a}_df = {a}_df.sample(min(max_size_atk, len({a}_df)))\")\n",
    "    # sort by timestamp\n",
    "    if temporal == True:\n",
    "        pass\n",
    "    exec(f\"{a}_df.reset_index(inplace=True, drop=True)\")\n",
    "    exec(f\"{a}_df['Label'] = a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining Train and Test sets for each class\n",
    "\n",
    "df_list = [benign_df]\n",
    "for a in attack_names:\n",
    "    exec(f\"df_list.append({a}_df)\")\n",
    "\n",
    "if temporal == True:\n",
    "    for dummy_df in df_list:\n",
    "        if train_size <=1:\n",
    "            train_threshold = int(((1-test_size) * train_size) * len(dummy_df))\n",
    "        else:\n",
    "            train_threshold = int(100)\n",
    "        test_threshold = len(dummy_df) - int(test_size * len(dummy_df))\n",
    "        dummy_df['index'] = dummy_df.index\n",
    "        dummy_df['is_test'] = np.where(dummy_df['index'] >= test_threshold , True, False)\n",
    "        dummy_df['is_train'] = np.where(dummy_df['index'] <= train_threshold , True, False)\n",
    "else:\n",
    "    for dummy_df in df_list:\n",
    "        if train_size <= 1:\n",
    "            train_threshold = test_size + (1-test_size)*train_size\n",
    "        else:\n",
    "            train_threshold = test_size + ((train_size * 100) / (len(dummy_df)) / 100)       \n",
    "        dummy_df['seed'] = (np.random.uniform(0,1,len(dummy_df)))\n",
    "        dummy_df['is_test'] = np.where(dummy_df['seed'] <= test_size, True, False)\n",
    "        dummy_df['is_train'] = np.where((dummy_df['seed'] <= train_threshold) & (dummy_df['is_test']==False), True, False)\n",
    "\n",
    "# get all together\n",
    "all_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 0 & \\textit{Benign} & 500000 & \\\\ \\cline{2-4}\n",
      "& 1 & \\textit{expl} & 44525 \\\\ \\cline{2-4}\n",
      "& 2 & \\textit{recon} & 13987 \\\\ \\cline{2-4}\n",
      "& 3 & \\textit{dos} & 16353 \\\\ \\cline{2-4}\n",
      "& 4 & \\textit{shell} & 1511 \\\\ \\cline{2-4}\n",
      "& 5 & \\textit{fuzz} & 24246 \\\\ \\cline{2-4}\n",
      "& 6 & \\textit{bdoor} & 2329 \\\\ \\cline{2-4}\n",
      "& 7 & \\textit{ana} & 2677 \\\\ \\cline{2-4}\n"
     ]
    }
   ],
   "source": [
    "def handle_categorical(df):\n",
    "    ## Handling categorical data\n",
    "    df_dummy = df.copy(deep=True)\n",
    "    df_dummy['Nature'] = np.where(df_dummy['Label'].str.contains('BENIGN'),0,1)\n",
    "    \n",
    "    for column_name in df_dummy.columns:\n",
    "        if column_name == ('SrcPort_type'):\n",
    "            df_dummy[column_name+\"-f\"] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('DstPort_type'):\n",
    "            df_dummy[column_name+\"-f\"] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('Proto'):\n",
    "            df_dummy[column_name+\"-f\"] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('State'):\n",
    "            df_dummy[column_name+\"-f\"] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('Service'):\n",
    "            df_dummy[column_name+\"-f\"] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('ct_ftp_cmd'):\n",
    "            df_dummy[column_name+\"-f\"] = pd.factorize(df_dummy[column_name])[0]\n",
    "        else:\n",
    "            pass\n",
    "    return df_dummy\n",
    "\n",
    "all_df = handle_categorical(all_df)\n",
    "all_df['Label_cat'] = pd.factorize(all_df['Label'])[0]\n",
    "\n",
    "all_df['totPkt'] = all_df['SrcPkts'] + all_df['DstPkts']\n",
    "all_df['totByt'] = all_df['SrcBytes'] + all_df['DstBytes']\n",
    "\n",
    "all_df['int2int'] = np.where( ((all_df['SrcIP_internal']==True) & (all_df['DstIP_internal']==True)), True, False)\n",
    "all_train, all_test = all_df[all_df['is_train']==True], all_df[all_df['is_test']==True]\n",
    "\n",
    "### SPLITTING ALL BACK ####\n",
    "benign_df = all_df[all_df['Label']=='BENIGN']\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_df = all_df[all_df['Label']=='{a}']\")\n",
    "    \n",
    "malicious_df = all_df[all_df['Label']!='BENIGN']\n",
    "malicious_train, malicious_test = malicious_df[malicious_df['is_train']==True], malicious_df[malicious_df['is_test']==True]\n",
    "\n",
    "benign_train = benign_df[benign_df['is_train']==True]\n",
    "benign_test = benign_df[benign_df['is_test']==True]\n",
    "\n",
    "\n",
    "\n",
    "print(\"& 0 & \\\\textit{{Benign}} & {} & \\\\\\\\ \\\\cline{{2-4}}\".format(len(benign_df)))\n",
    "\n",
    "\n",
    "for i,a in enumerate(attack_names):\n",
    "    exec(f\"print('& {i+1} & \\\\\\\\textit{{{{{a}}}}} & {{}} \\\\\\\\\\\\\\\\ \\\\\\\\cline{{{{2-4}}}}'.format(len({a}_df)))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature sets\n",
    "\n",
    "# the following is the \"complete\" feature set\n",
    "\n",
    "features = ['Duration', 'SrcBytes', 'DstBytes',\n",
    "       'SrcTTL', 'DstTTL', 'SrcLoss', 'DstLoss', 'SrcBpS', 'DstBpS',\n",
    "       'SrcPkts', 'DstPkts', 'SrcWin', 'DstWin', 'SrcTcpb', 'DstTcpb',\n",
    "       'SrcMean', 'DstMean', 'Trans_Depth', 'res_bdy_len', 'SrcJit', 'DstJit',\n",
    "       'SrcAit', 'DstAit', 'TcpRrt', 'SynAck',\n",
    "       'AckDat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd',\n",
    "       'is_ftp_login', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm',\n",
    "       'ct_src_ltm', 'ct_src_dport', 'ct_dst_sport', 'ct_dst_src_ltm',\n",
    "       'DstIP_internal', 'SrcIP_internal', 'Proto-f', 'State-f',\n",
    "       'Service-f', 'ct_ftp_cmd-f', 'SrcPort_type-f', 'DstPort_type-f',\n",
    "       ]\n",
    "\n",
    "# this is for the \"essential\" feature set\n",
    "small_features = ['Duration', 'SrcBytes', 'DstBytes',\n",
    "       'SrcPkts', 'DstPkts', 'totPkt', 'totByt',\n",
    "       'SynAck',\n",
    "       'AckDat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd',\n",
    "       'is_ftp_login', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm',\n",
    "       'ct_src_ltm', \n",
    "       'int2int',\n",
    "        'Proto-f', 'State-f',\n",
    "       'Service-f', 'ct_ftp_cmd-f', 'SrcPort_type-f', 'DstPort_type-f',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2616\n"
     ]
    }
   ],
   "source": [
    "# creating adversarial dataset\n",
    "mal_base = malicious_df[((malicious_df['Proto']=='udp')) & (malicious_df['is_test']==True)]\n",
    "print(len(mal_base))\n",
    "mal_adv = mal_base.copy(deep=True)\n",
    "\n",
    "# attacking\n",
    "max_dur = mal_adv['Duration'].max()\n",
    "min_dur = mal_adv['Duration'].min()\n",
    "\n",
    "mal_adv['Duration'] = mal_adv['Duration'] + dur_intensity # we increase the duration in seconds\n",
    "mal_adv['Duration'] = np.where(mal_adv['Duration'] > max_dur, max_dur, mal_adv['Duration'])\n",
    "mal_adv['Duration'] = np.where(mal_adv['Duration'] < min_dur, min_dur, mal_adv['Duration'])\n",
    "\n",
    "\n",
    "mal_adv['DstPkts'] = mal_adv['DstPkts'] + pkt_intensity\n",
    "mal_adv['DstBytes'] = mal_adv['DstBytes'] + byt_intensity\n",
    "\n",
    "#mal_adv['SrcBytes'] = mal_adv['SrcBytes'] + pkt_intensity\n",
    "#mal_adv['SrcPkts'] = mal_adv['SrcPkts'] + byt_intensity\n",
    "\n",
    "mal_adv['totPkt'] = mal_adv['SrcPkts'] + mal_adv['DstPkts']\n",
    "mal_adv['totByt'] = mal_adv['SrcBytes'] + mal_adv['DstBytes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROM NOW ON, THE CODE IS ALWAYS THE SAME FOR EVERY DATASET!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Assessment on \"Complete\" feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BINARY CLASSIFIER (Complete features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing bin......done! Training time: 0.007973s\tInference time: 0.045846s\n",
      "Total Misclassifications: 2108 out of 121123 (Recall: 0.999101\tFPR: 0.020890)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97911</td>\n",
       "      <td>2089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>21104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     97911   2089\n",
       "1        19  21104"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bClf, bPred, bResult = develop_clf(all_train, all_test, features, clf_name='bin', label='Nature', clf_type=base_clf, verbose=1)\n",
    "\n",
    "if (bResult.acc == 0):\n",
    "    bErr = int(len(all_test) * (1-bResult.acc_multi))\n",
    "else:\n",
    "    bErr = int(len(all_test) * (1-bResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(bErr, len(all_test), bResult.rec, bResult.fpr))\n",
    "pd.crosstab(all_test['Nature'], bPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-CLASS CLASSIFIER - cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing mc......done! Training time: 0.010925s\tInference time: 0.009966s\n",
      "Total Misclassifications: 9710 out of 21123\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4111</td>\n",
       "      <td>586</td>\n",
       "      <td>1462</td>\n",
       "      <td>187</td>\n",
       "      <td>365</td>\n",
       "      <td>1159</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>136</td>\n",
       "      <td>2274</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>140</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>648</td>\n",
       "      <td>144</td>\n",
       "      <td>778</td>\n",
       "      <td>63</td>\n",
       "      <td>197</td>\n",
       "      <td>741</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>280</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>213</td>\n",
       "      <td>205</td>\n",
       "      <td>250</td>\n",
       "      <td>74</td>\n",
       "      <td>3584</td>\n",
       "      <td>281</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>160</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>45</td>\n",
       "      <td>16</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>138</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred     1     2     3    4     5     6     7\n",
       "True                                         \n",
       "1     4111   586  1462  187   365  1159  1035\n",
       "2      136  2274   109    0    35   140   103\n",
       "3      648   144   778   63   197   741   699\n",
       "4        0     0     2  280    18     2     0\n",
       "5      213   205   250   74  3584   281   242\n",
       "6       54    15    81    6    42   160   107\n",
       "7       45    16    96    0    14   138   226"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the classifier that analyzes ONLY the malicious samples that \"receives\" from the initial binary classifier\n",
    "# It is trained on the same training set---but without using the benign samples\n",
    "# It is tested on the malicious samples in the test set that are flagged as malicious by the binary classifier\n",
    "\n",
    "\n",
    "mcClf, mcPred, mcResult = develop_clf(malicious_train, malicious_test, features, clf_name='mc', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "mcErr = int(len(malicious_test) * (1-mcResult.acc_multi))\n",
    "print(\"Total Misclassifications: {} out of {}\".format(mcErr, len(malicious_test)))\n",
    "pd.crosstab(malicious_test['Label_cat'], mcPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications (among the malicious samples): 9702 out of 21104\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4109</td>\n",
       "      <td>586</td>\n",
       "      <td>1461</td>\n",
       "      <td>187</td>\n",
       "      <td>365</td>\n",
       "      <td>1159</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>136</td>\n",
       "      <td>2273</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>140</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>648</td>\n",
       "      <td>142</td>\n",
       "      <td>778</td>\n",
       "      <td>62</td>\n",
       "      <td>195</td>\n",
       "      <td>741</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>280</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>213</td>\n",
       "      <td>204</td>\n",
       "      <td>250</td>\n",
       "      <td>74</td>\n",
       "      <td>3576</td>\n",
       "      <td>281</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>160</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>45</td>\n",
       "      <td>16</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>138</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred     1     2     3    4     5     6     7\n",
       "True                                         \n",
       "1     4109   586  1461  187   365  1159  1035\n",
       "2      136  2273   109    0    34   140   103\n",
       "3      648   142   778   62   195   741   699\n",
       "4        0     0     2  280    18     2     0\n",
       "5      213   204   250   74  3576   281   242\n",
       "6       54    15    81    6    42   160   107\n",
       "7       45    16    96    0    14   138   226"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We select the samples flagged as malicious by the initial classifier.\n",
    "# Of course, samples flagged as malicious that are NOT actually malicious will always be misclassified\n",
    "\n",
    "all_test['bPred'] = bPred\n",
    "mc_test = all_test[(all_test['bPred']==1) & (all_test['Nature']==1)]\n",
    "if (len(mc_test)==0):\n",
    "    # in this case, this classifier receives nothing\n",
    "    print(\"There is no malicious sample flagged as malicious to analyze!\")\n",
    "\n",
    "mcPred_m = mcClf.predict(mc_test[features])\n",
    "mcResult.acc_multic = accuracy_score(mc_test['Label_cat'], mcPred_m, normalize=True, sample_weight=None)\n",
    "mcErr_m = int((1-mcResult.acc_multic) * len(mc_test))\n",
    "print(\"Total Misclassifications (among the malicious samples): {} out of {}\".format(mcErr_m, len(mc_test)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.crosstab(mc_test['Label_cat'], mcPred_m, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This classifier also analyzed 2088 benign samples that were incorrectly labelled as 'malicious' by the binary classifier\n",
      "Hence, this classifier was tested on 23192 samples, of which 11790 have been misclassified\n"
     ]
    }
   ],
   "source": [
    "## Note: We also accounted for the false positives of the first binary classifier (all of which have been considered as misclassifications)\n",
    "bin_falsePositives = int(bResult.fpr * len(benign_test))\n",
    "print(\"This classifier also analyzed {} benign samples that were incorrectly labelled as 'malicious' by the binary classifier\".format(bin_falsePositives))\n",
    "\n",
    "print(\"Hence, this classifier was tested on {} samples, of which {} have been misclassified\".format(len(mc_test)+bin_falsePositives, bin_falsePositives+mcErr_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-CLASS CLASSIFIER - stand-alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing m......done! Training time: 0.011974s\tInference time: 0.042842s\n",
      "Total Misclassifications: 12177 out of 121123\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97932</td>\n",
       "      <td>251</td>\n",
       "      <td>274</td>\n",
       "      <td>43</td>\n",
       "      <td>131</td>\n",
       "      <td>1153</td>\n",
       "      <td>32</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>3859</td>\n",
       "      <td>559</td>\n",
       "      <td>1691</td>\n",
       "      <td>185</td>\n",
       "      <td>374</td>\n",
       "      <td>1243</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>2240</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>139</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>642</td>\n",
       "      <td>135</td>\n",
       "      <td>799</td>\n",
       "      <td>61</td>\n",
       "      <td>192</td>\n",
       "      <td>748</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>279</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>65</td>\n",
       "      <td>247</td>\n",
       "      <td>332</td>\n",
       "      <td>211</td>\n",
       "      <td>74</td>\n",
       "      <td>3454</td>\n",
       "      <td>236</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>13</td>\n",
       "      <td>76</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>162</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>140</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0     1     2     3    4     5     6    7\n",
       "True                                               \n",
       "0     97932   251   274    43  131  1153    32  184\n",
       "1         5  3859   559  1691  185   374  1243  989\n",
       "2         1   161  2240   111    0    43   139  102\n",
       "3         1   642   135   799   61   192   748  692\n",
       "4         0     0     0     1  279    19     3    0\n",
       "5        65   247   332   211   74  3454   236  230\n",
       "6         0    56    13    76    6    43   162  109\n",
       "7         0    46    16    96    0    16   140  221"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first assess its multiclassification performance, and then its binary classification performance\n",
    "\n",
    "mClf, mPred, mResult = develop_clf(all_train, all_test, features, clf_name='m', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "mErr = int(len(all_test) * (1-mResult.acc_multi))\n",
    "print(\"Total Misclassifications: {} out of {}\".format(mErr, len(all_test)))\n",
    "mResult.ctab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 2139 out of 121123 (Recall: 0.996591\tFPR: 0.020680)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97932</td>\n",
       "      <td>2068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>21051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     97932   2068\n",
       "1        72  21051"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the binary classification performance, we use the previous predictions\n",
    "mPred_bin = np.copy(mPred)\n",
    "mPred_bin[mPred_bin > 0] = 1\n",
    "mResult.bin_results(all_test['Nature'], mPred_bin)\n",
    "mErr_bin = int(len(all_test) * (1-mResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(mErr_bin, len(all_test), mResult.rec, mResult.fpr))\n",
    "mResult.ctab_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSEMBLE CLASSIFIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \"individual\" binary classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing expl......done! Training time: 0.004995s\tInference time: 0.035875s\n",
      "Testing expl...\n",
      "...done! \tInference time: 0.038402s\n",
      "Training and testing recon......done! Training time: 0.004944s\tInference time: 0.032881s\n",
      "Testing recon...\n",
      "...done! \tInference time: 0.038872s\n",
      "Training and testing dos......done! Training time: 0.004967s\tInference time: 0.033863s\n",
      "Testing dos...\n",
      "...done! \tInference time: 0.035879s\n",
      "Training and testing shell......done! Training time: 0.004982s\tInference time: 0.031863s\n",
      "Testing shell...\n",
      "...done! \tInference time: 0.035387s\n",
      "Training and testing fuzz......done! Training time: 0.005053s\tInference time: 0.032867s\n",
      "Testing fuzz...\n",
      "...done! \tInference time: 0.035883s\n",
      "Training and testing bdoor......done! Training time: 0.004985s\tInference time: 0.031964s\n",
      "Testing bdoor...\n",
      "...done! \tInference time: 0.037874s\n",
      "Training and testing ana......done! Training time: 0.004984s\tInference time: 0.034883s\n",
      "Testing ana...\n",
      "...done! \tInference time: 0.040860s\n",
      "Total training time: 0.034909s\t AvgFPR: 0.012467\t AvgTPR: 0.990295\tTotal inference time: 0.263157s (fake: 0.234196s)\n"
     ]
    }
   ],
   "source": [
    "ensemble_df = pd.DataFrame()\n",
    "\n",
    "benign_train = benign_df[benign_df['is_train']==True]\n",
    "benign_test = benign_df[benign_df['is_test']==True]\n",
    "\n",
    "\n",
    "ens_time = 0\n",
    "ens_avgFPR = 0\n",
    "tot_TP = 0\n",
    "tot_P = 0\n",
    "ens_infer_time = 0\n",
    "fakeEns_infer_time = 0\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_train = {a}_df[{a}_df['is_train']==True]\")\n",
    "    exec(f\"{a}_test = {a}_df[{a}_df['is_test']==True]\")\n",
    "\n",
    "    exec(f\"train = pd.concat([benign_train, {a}_train])\")\n",
    "    exec(f\"test = pd.concat([benign_test, {a}_test])\")\n",
    "    \n",
    "    # We first train a classifier only on \"benign\" or on malicious samples of a specific attack. \n",
    "    # Afterwards, we immediately test it on a test-set having ONLY malicious samples of this specific attack\n",
    "    # Note: such \"testing\" is redundant, because it assumes that the classifier only receives the samples of the attack it is trained on!\n",
    "    exec(f\"{a}Clf, {a}Pred, {a}Result = develop_clf(train, test, features, clf_name='{a}', clf_type=base_clf, verbose=1)\")\n",
    "    exec(f\"fakeEns_infer_time += {a}Result.infer_time\")\n",
    "    # We now test the specific classifier on the ENTIRE test-set, thereby allowing to assess its performance also against malicious samples of different attacks\n",
    "    exec(f\"{a}_allPred, {a}_allResults, {a}Result.infer_time = evaluate_clf({a}Clf, all_test, features, clf_name='{a}', time={a}Result.time, verbose=1)\")\n",
    "\n",
    "    exec(f\"ensemble_df['{a}'] = {a}_allPred\")\n",
    "\n",
    "    exec(f\"tot_TP += ({a}Result.rec * len({a}_test))\")\n",
    "    exec(f\"tot_P += len({a}_test)\")\n",
    "\n",
    "\n",
    "    exec(f\"ens_avgFPR += {a}Result.fpr\")\n",
    "    exec(f\"ens_time += {a}Result.time\")\n",
    "    exec(f\"ens_infer_time +={a}Result.infer_time\")\n",
    "\n",
    "ens_avgFPR = ens_avgFPR / len(attack_names)\n",
    "ens_avgREC = tot_TP/tot_P\n",
    "\n",
    "print(\"Total training time: {:5f}s\\t AvgFPR: {:5f}\\t AvgTPR: {:5f}\\tTotal inference time: {:5f}s (fake: {:5f}s)\".\n",
    "      format(ens_time, ens_avgFPR, ens_avgREC, ens_infer_time, fakeEns_infer_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble (real assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we measure the combined performance of the entire ensemble\n",
    "# This is done with a logical or, or for majority voting (regulated by the \"agreement\" variable)\n",
    "\n",
    "ensemble_df[\"sum\"] = ensemble_df.sum(axis=1)\n",
    "#calculating \n",
    "ensemble_df[\"LOR\"] = (ensemble_df[\"sum\"]>0)\n",
    "\n",
    "#Appending Ground Truth\n",
    "temp = all_test['Nature'] #> 0)\n",
    "ensemble_df['True'] = ((temp.reset_index(drop=True)) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Logical OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 2381 out of 121123 (Recall: 0.999479\tFPR: 0.023700)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>97630</td>\n",
       "      <td>2370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>11</td>\n",
       "      <td>21112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  97630   2370\n",
       "True      11  21112"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enslorResult = Result(ensemble_df['True'], ensemble_df['LOR'], ens_time, ens_infer_time)\n",
    "enslorErr= int(len(all_test) * (1-enslorResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(enslorErr, len(all_test), enslorResult.rec, enslorResult.fpr))\n",
    "enslorResult.ctab_bin # you can also try with enslorResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting: at least 4 out of 7 classifiers must agree that a sample is malicious.\n",
      "Total Misclassifications: 3523 out of 121123 (Recall: 0.886332\tFPR: 0.011220)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>98878</td>\n",
       "      <td>1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>2401</td>\n",
       "      <td>18722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  98878   1122\n",
       "True    2401  18722"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_agree = math.ceil(agreement * len(attack_names))\n",
    "print(\"Voting: at least {} out of {} classifiers must agree that a sample is malicious.\".format(min_agree, len(attack_names)))\n",
    "ensemble_df[\"MAJV\"] = (ensemble_df[\"sum\"]>=min_agree)\n",
    "ensvotResult = Result(ensemble_df['True'], ensemble_df['MAJV'], ens_time, ens_infer_time)\n",
    "ensvotErr = int(len(all_test) * (1-ensvotResult.acc))\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(ensvotErr, len(all_test), ensvotResult.rec, ensvotResult.fpr))\n",
    "ensvotResult.ctab_bin # you can also try with ensvotResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Stacked Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 2127 out of 121123 (Recall: 0.998911\tFPR: 0.021050)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97895</td>\n",
       "      <td>2105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>21100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     97895   2105\n",
       "1        23  21100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "clf_list = []\n",
    "for a in attack_names:\n",
    "    exec(f\"clf_list.append({a}Clf)\")\n",
    "\n",
    "meta = choose_clf(clf_type=base_clf)\n",
    "sClf = StackingClassifier(classifiers=clf_list, meta_classifier=meta, fit_base_estimators=False, use_probas = False)\n",
    "s_timeStart = time.time()\n",
    "sClf.fit(all_train[features], all_train['Nature'])\n",
    "s_time = time.time() - s_timeStart + ens_time\n",
    "s_timeStart = time.time()\n",
    "sPred = sClf.predict(all_test[features])\n",
    "s_infer_time = time.time()-s_timeStart\n",
    "sResult = Result(all_test['Nature'], sPred, s_time, s_infer_time)\n",
    "if sResult.acc < sResult.acc_multi:\n",
    "    sResult.acc = sResult.acc_multi\n",
    "sErr = int(len(all_test) * (1-sResult.acc))\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sErr, len(all_test), sResult.rec, sResult.fpr))\n",
    "\n",
    "sResult.ctab_bin # you can also try with sResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open World Assessment: One attack against all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
      "      Binary CLF: TPR=0.980466\tFPR=0.019947\n",
      "      Multiclass (binarized) CLF: TPR=0.973689\tFPR=0.020686\n",
      "      EnsLOR CLF: TPR=0.996930\tFPR=0.023234\n",
      "      EnsVOT CLF: TPR=0.841133\tFPR=0.009054\n",
      "      EnsSTK CLF: TPR=0.984115\tFPR=0.020764\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "### The following code is a mixture of everything described insofar.\n",
    "### We are only focused on TPR and FPR here. We do not care about accuracy, adversarial robustness, or runtime.\n",
    "### These experiments are also done only on the \"Complete\" feature set\n",
    "\n",
    "\n",
    "oaac_bin_rec = 0\n",
    "oaac_bin_fpr = 0\n",
    "oaac_multi_rec = 0\n",
    "oaac_multi_fpr = 0\n",
    "oaac_enslor_rec = 0\n",
    "oaac_enslor_fpr = 0\n",
    "oaac_ensvot_rec = 0\n",
    "oaac_ensvot_fpr = 0\n",
    "oaac_ensstk_rec = 0\n",
    "oaac_ensstk_fpr = 0\n",
    "\n",
    "for u in attack_names: # u is the unknown attack\n",
    "    #print(u) # this is the unknown attack\n",
    "    #exec(f\"{u}_test = {u}_df[{u}_df['is_train']==False]\") # create test set by putting the \"test\" samples of u\n",
    "    exec(f\"{u}_test = pd.concat([benign_test, {u}_test])\") # add to the test set the \"benign\" test samples\n",
    "    exec(f\"{u}_train = benign_df[benign_df['is_train']==True]\") # compose the \"training\" set: start by putting the benign \"training\" samples\n",
    "    for a in attack_names: \n",
    "        # for every attack that is not u, add its training samples to the training set of u\n",
    "        if a==u:\n",
    "            continue\n",
    "        exec(f\"{u}_train = pd.concat([{u}_train, {a}_df[{a}_df['is_train']==True]])\")\n",
    "\n",
    "\n",
    "    # We have created the training and testing set. Now we must train and test a binary classifier by following the standard procedure\n",
    "    ########## BINARY CLASSIFIER ##########\n",
    "    exec(f\"{u}_oaac_bClf, {u}_oaac_bPred, {u}_oaac_bResult = develop_clf({u}_train, {u}_test, features, clf_name='{u}_oaac_bin', label='Nature', clf_type=base_clf)\")\n",
    "\n",
    "\n",
    "    ########## Multiclass CLASSIFIER ########## --> Train, then test only on binary\n",
    "    exec(f\"{u}_oaac_mClf, {u}_oaac_mPred, {u}_oaac_mResult = develop_clf({u}_train, {u}_test, features, clf_name='{u}_oaac_multi', label='Label_cat', clf_type=base_clf)\")\n",
    "    exec(f\"{u}_oaac_mPred_bin = np.copy({u}_oaac_mPred)\")\n",
    "    exec(f\"{u}_oaac_mPred_bin[{u}_oaac_mPred_bin > 0] = 1\")\n",
    "    exec(f\"{u}_oaac_mResult.bin_results({u}_test['Nature'], {u}_oaac_mPred_bin)\")\n",
    "\n",
    "\n",
    "    ######### Ensemble ##############\n",
    "    # send the samples in TEST to all the classifiers of the ensemble (which are already trained), aside from the one focusing on u\n",
    "    exec(f\"{u}_oaac_ens_df = pd.DataFrame()\")\n",
    "    for a in attack_names:    \n",
    "        if a==u:\n",
    "                continue\n",
    "        exec(f\"{a}_{u}Pred, {a}_{u}Results, {a}_{u}_infer_time = evaluate_clf({a}Clf, {u}_test, features, clf_name='{a}_{u}', time={a}Result.time)\")\n",
    "        exec(f\"{u}_oaac_ens_df['{a}'] = {a}_{u}Pred\")\n",
    "\n",
    "    # now we have the dataframe with all the predictions, let's see the aggregate results\n",
    "    exec(f\"{u}_oaac_ens_df['sum'] = {u}_oaac_ens_df.sum(axis=1)\")\n",
    "    exec(f\"{u}_oaac_ens_df['LOR'] = ({u}_oaac_ens_df['sum']>0)\")\n",
    "    exec(f\"temp = {u}_test['Nature'] #> 0)\")\n",
    "    exec(f\"{u}_oaac_ens_df['True'] = ((temp.reset_index(drop=True)) > 0)\")\n",
    "    exec(f\"{u}_oaac_enslorResult = Result({u}_oaac_ens_df['True'], {u}_oaac_ens_df['LOR'], (ens_time-{u}Result.time), (ens_infer_time-{u}Result.infer_time))\")\n",
    "\n",
    "    # now we consider the majority voting of the ensemble\n",
    "    exec(f\"{u}_oaac_ens_df['MAJV'] = ({u}_oaac_ens_df['sum']>=min_agree)\")\n",
    "    exec(f\"{u}_oaac_ensvotResult = Result({u}_oaac_ens_df['True'], {u}_oaac_ens_df['MAJV'], (ens_time-{u}Result.time), (ens_infer_time-{u}Result.infer_time))\")\n",
    "\n",
    "    # finally, let's use the stacking ensemble\n",
    "    exec(f\"{u}_clf_list = []\")\n",
    "    for a in attack_names:\n",
    "        if a==u:\n",
    "                continue\n",
    "        exec(f\"{u}_clf_list.append({a}Clf)\")\n",
    "    exec(f\"{u}_oaac_sClf = StackingClassifier(classifiers={u}_clf_list, meta_classifier=meta, fit_base_estimators=False, use_probas = False)\")\n",
    "    exec(f\"{u}_oaac_sClf.fit({u}_train[features], {u}_train['Nature'])\")\n",
    "    exec(f\"{u}_oaac_sPred = {u}_oaac_sClf.predict({u}_test[features])\")\n",
    "    exec(f\"{u}_oaac_sResult = Result({u}_test['Nature'], {u}_oaac_sPred, (ens_time-{u}Result.time), (ens_infer_time-{u}Result.infer_time))\")\n",
    "\n",
    "    # Updating results\n",
    "    exec(f\"oaac_bin_rec += {u}_oaac_bResult.rec\")\n",
    "    exec(f\"oaac_bin_fpr += {u}_oaac_bResult.fpr\")\n",
    "    exec(f\"oaac_multi_rec += {u}_oaac_mResult.rec\")\n",
    "    exec(f\"oaac_multi_fpr += {u}_oaac_mResult.fpr\")\n",
    "    exec(f\"oaac_enslor_rec += {u}_oaac_enslorResult.rec\")\n",
    "    exec(f\"oaac_enslor_fpr += {u}_oaac_enslorResult.fpr\")\n",
    "    exec(f\"oaac_ensvot_rec += {u}_oaac_ensvotResult.rec\")\n",
    "    exec(f\"oaac_ensvot_fpr += {u}_oaac_ensvotResult.fpr\")\n",
    "    exec(f\"oaac_ensstk_rec += {u}_oaac_sResult.rec\")\n",
    "    exec(f\"oaac_ensstk_fpr += {u}_oaac_sResult.fpr\")\n",
    "\n",
    "# Finalizing averages\n",
    "oaac_bin_rec /= len(attack_names)\n",
    "oaac_bin_fpr /= len(attack_names)\n",
    "oaac_multi_rec /= len(attack_names)\n",
    "oaac_multi_fpr /= len(attack_names)\n",
    "oaac_enslor_rec /= len(attack_names)\n",
    "oaac_enslor_fpr /= len(attack_names)\n",
    "oaac_ensvot_rec /= len(attack_names)\n",
    "oaac_ensvot_fpr /= len(attack_names)\n",
    "oaac_ensstk_rec /= len(attack_names)\n",
    "oaac_ensstk_fpr /= len(attack_names)\n",
    "\n",
    "\n",
    "print('''Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
    "      Binary CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      Multiclass (binarized) CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      EnsLOR CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      EnsVOT CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      EnsSTK CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      '''.format(oaac_bin_rec, oaac_bin_fpr,\n",
    "                 oaac_multi_rec, oaac_multi_fpr,\n",
    "                 oaac_enslor_rec, oaac_enslor_fpr,\n",
    "                 oaac_ensvot_rec, oaac_ensvot_fpr,\n",
    "                 oaac_ensstk_rec, oaac_ensstk_fpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment on Essential feature set (and Adversarial attacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BINARY CLASSIFIER (Essential features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing adv_bin......done! Training time: 0.005636s\tInference time: 0.030461s\n",
      "Total Misclassifications: 2107 out of 121123 (Recall: 0.995597\tFPR: 0.020140)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97986</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>21030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     97986   2014\n",
       "1        93  21030"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_bClf, sma_bPred, sma_bResult = develop_clf(all_train, all_test, small_features, clf_name='adv_bin', label='Nature', clf_type=base_clf, verbose=1)\n",
    "sma_bErr = int(len(all_test) * (1-sma_bResult.acc))\n",
    "if (sma_bResult.acc == 0):\n",
    "    sma_bErr = int(len(all_test) * (1-sma_bResult.acc_multi))\n",
    "else:\n",
    "    sma_bErr = int(len(all_test) * (1-sma_bResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_bErr, len(all_test), sma_bResult.rec, sma_bResult.fpr))\n",
    "pd.crosstab(all_test['Nature'], sma_bPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial Attack against Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.985\n",
      "Adversarial Recall (attack): 1.000\n"
     ]
    }
   ],
   "source": [
    "# Note that the adversarial attacks only affect a subset of the initial set of malicious samples\n",
    "# Hence, we compute the classification performance also on this subset for a far comparison\n",
    "\n",
    "\n",
    "\n",
    "adv_bPred_base = sma_bClf.predict(mal_base[small_features])\n",
    "adv_bPred_adv = sma_bClf.predict(mal_adv[small_features])\n",
    "adv_bin_base_rec =  recall_score(mal_base['Nature'], adv_bPred_base, pos_label=1)\n",
    "adv_bin_adv_rec = recall_score(mal_adv['Nature'], adv_bPred_adv, pos_label=1)\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_bin_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_bin_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classifier - cascade (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing sma_mc......done! Training time: 0.006935s\tInference time: 0.006977s\n",
      "Total Misclassifications: 9967 out of 21123\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3984</td>\n",
       "      <td>587</td>\n",
       "      <td>1571</td>\n",
       "      <td>181</td>\n",
       "      <td>392</td>\n",
       "      <td>1110</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>2186</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>121</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>585</td>\n",
       "      <td>208</td>\n",
       "      <td>821</td>\n",
       "      <td>57</td>\n",
       "      <td>201</td>\n",
       "      <td>695</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>286</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>282</td>\n",
       "      <td>425</td>\n",
       "      <td>163</td>\n",
       "      <td>19</td>\n",
       "      <td>3517</td>\n",
       "      <td>257</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54</td>\n",
       "      <td>26</td>\n",
       "      <td>86</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "      <td>143</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44</td>\n",
       "      <td>15</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>131</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred     1     2     3    4     5     6     7\n",
       "True                                         \n",
       "1     3984   587  1571  181   392  1110  1080\n",
       "2      119  2186   118    0   141   121   112\n",
       "3      585   208   821   57   201   695   703\n",
       "4        0     0     1  286    13     2     0\n",
       "5      282   425   163   19  3517   257   186\n",
       "6       54    26    86   14    30   143   112\n",
       "7       44    15   108    0    18   131   219"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_mcClf, sma_mcPred, sma_mcResult = develop_clf(malicious_train, malicious_test, small_features, clf_name='sma_mc', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "sma_mcErr = int(len(malicious_test) * (1-sma_mcResult.acc_multi))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {}\".format(sma_mcErr, len(malicious_test)))\n",
    "pd.crosstab(malicious_test['Label_cat'], sma_mcPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications (among the malicious samples): 9904 out of 21030\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3975</td>\n",
       "      <td>577</td>\n",
       "      <td>1561</td>\n",
       "      <td>181</td>\n",
       "      <td>385</td>\n",
       "      <td>1101</td>\n",
       "      <td>1079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117</td>\n",
       "      <td>2180</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>121</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>573</td>\n",
       "      <td>201</td>\n",
       "      <td>815</td>\n",
       "      <td>57</td>\n",
       "      <td>200</td>\n",
       "      <td>693</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>286</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>282</td>\n",
       "      <td>425</td>\n",
       "      <td>162</td>\n",
       "      <td>19</td>\n",
       "      <td>3511</td>\n",
       "      <td>257</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54</td>\n",
       "      <td>26</td>\n",
       "      <td>86</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "      <td>140</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44</td>\n",
       "      <td>15</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>131</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred     1     2     3    4     5     6     7\n",
       "True                                         \n",
       "1     3975   577  1561  181   385  1101  1079\n",
       "2      117  2180   118    0   140   121   112\n",
       "3      573   201   815   57   200   693   703\n",
       "4        0     0     1  286    13     2     0\n",
       "5      282   425   162   19  3511   257   186\n",
       "6       54    26    86   14    30   140   112\n",
       "7       44    15   108    0    18   131   219"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We select the samples flagged as malicious by the initial classifier.\n",
    "# Of course, samples flagged as malicious that are NOT actually malicious will always be misclassified\n",
    "\n",
    "all_test['sma_bPred'] = sma_bPred\n",
    "sma_mc_test = all_test[(all_test['sma_bPred']==1) & (all_test['Nature']==1)]\n",
    "if (len(sma_mc_test)==0):\n",
    "    # in this case, this classifier receives nothing\n",
    "    print(\"There is no malicious sample flagged as malicious to analyze!\")\n",
    "\n",
    "sma_mcPred_m = sma_mcClf.predict(sma_mc_test[small_features])\n",
    "sma_mcResult.acc_multic = accuracy_score(sma_mc_test['Label_cat'], sma_mcPred_m, normalize=True, sample_weight=None)\n",
    "sma_mcErr_m = int((1-sma_mcResult.acc_multic) * len(sma_mc_test))\n",
    "print(\"Total Misclassifications (among the malicious samples): {} out of {}\".format(sma_mcErr_m, len(sma_mc_test)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.crosstab(sma_mc_test['Label_cat'], sma_mcPred_m, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This classifier also analyzed 2014 benign samples that were incorrectly labelled as 'malicious' by the (small) binary classifier\n",
      "Hence, this (small) classifier was tested on 23044 samples, of which 11918 have been misclassified\n"
     ]
    }
   ],
   "source": [
    "## Note: We also accounted for the false positives of the first binary classifier (all of which have been considered as misclassifications)\n",
    "sma_bin_falsePositives = int(sma_bResult.fpr * len(benign_test))\n",
    "print(\"This classifier also analyzed {} benign samples that were incorrectly labelled as 'malicious' by the (small) binary classifier\".format(sma_bin_falsePositives))\n",
    "\n",
    "print(\"Hence, this (small) classifier was tested on {} samples, of which {} have been misclassified\".format(len(sma_mc_test)+sma_bin_falsePositives, sma_bin_falsePositives+sma_mcErr_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classifier - stand-alone (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing sma_m......done! Training time: 0.007981s\tInference time: 0.025906s\n",
      "Total Misclassifications: 12528 out of 121123\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97875</td>\n",
       "      <td>68</td>\n",
       "      <td>126</td>\n",
       "      <td>219</td>\n",
       "      <td>117</td>\n",
       "      <td>892</td>\n",
       "      <td>465</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>3673</td>\n",
       "      <td>549</td>\n",
       "      <td>1780</td>\n",
       "      <td>181</td>\n",
       "      <td>408</td>\n",
       "      <td>1146</td>\n",
       "      <td>1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>102</td>\n",
       "      <td>2176</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>138</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>535</td>\n",
       "      <td>189</td>\n",
       "      <td>813</td>\n",
       "      <td>57</td>\n",
       "      <td>194</td>\n",
       "      <td>731</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>287</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>105</td>\n",
       "      <td>278</td>\n",
       "      <td>421</td>\n",
       "      <td>172</td>\n",
       "      <td>19</td>\n",
       "      <td>3404</td>\n",
       "      <td>245</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>22</td>\n",
       "      <td>86</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>152</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>12</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>140</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0     1     2     3    4     5     6     7\n",
       "True                                                \n",
       "0     97875    68   126   219  117   892   465   238\n",
       "1        43  3673   549  1780  181   408  1146  1125\n",
       "2         6   102  2176   117    0   150   138   108\n",
       "3        32   535   189   813   57   194   731   719\n",
       "4         0     5     0     1  287     8     1     0\n",
       "5       105   278   421   172   19  3404   245   205\n",
       "6         4    46    22    86   14    40   152   101\n",
       "7         0    43    12   106    0    20   140   214"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_mClf, sma_mPred, sma_mResult = develop_clf(all_train, all_test, small_features, clf_name='sma_m', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "sma_mErr = int(len(all_test) * (1-sma_mResult.acc_multi))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {}\".format(sma_mErr, len(all_test)))\n",
    "sma_mResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Classifier: Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 2315 out of 121123 (Recall: 0.991005\tFPR: 0.021250)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97875</td>\n",
       "      <td>2125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>190</td>\n",
       "      <td>20933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     97875   2125\n",
       "1       190  20933"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## MULTI-CLASS CLASSIFIER - BINARY ##########\n",
    "sma_mPred_bin = np.copy(sma_mPred)\n",
    "sma_mPred_bin[sma_mPred_bin > 0] = 1\n",
    "sma_mResult.bin_results(all_test['Nature'], sma_mPred_bin)\n",
    "sma_mErr_bin = int(len(all_test) * (1-sma_mResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_mErr_bin, len(all_test), sma_mResult.rec, sma_mResult.fpr))\n",
    "sma_mResult.ctab_bin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial Attack against the Multiclass Classifier (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.981\n",
      "Adversarial Recall (attack): 0.998\n"
     ]
    }
   ],
   "source": [
    "adv_mPred_base = sma_mClf.predict(mal_base[small_features])\n",
    "adv_mPred_adv = sma_mClf.predict(mal_adv[small_features])\n",
    "\n",
    "adv_mPred_base_bin = np.copy(adv_mPred_base)\n",
    "adv_mPred_base_bin[adv_mPred_base_bin > 0] = 1\n",
    "\n",
    "adv_mPred_adv_bin = np.copy(adv_mPred_adv)\n",
    "adv_mPred_adv_bin[adv_mPred_adv_bin > 0] = 1\n",
    "\n",
    "\n",
    "adv_multi_base_rec =  recall_score(mal_base['Nature'], adv_mPred_base_bin)\n",
    "adv_multi_adv_rec = recall_score(mal_adv['Nature'], adv_mPred_adv_bin)\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_multi_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_multi_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Classifiers (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing sma_expl......done! Training time: 0.004978s\tInference time: 0.021939s\n",
      "Testing sma_expl...\n",
      "...done! \tInference time: 0.021993s\n",
      "Training and testing sma_recon......done! Training time: 0.003998s\tInference time: 0.018884s\n",
      "Testing sma_recon...\n",
      "...done! \tInference time: 0.023937s\n",
      "Training and testing sma_dos......done! Training time: 0.003951s\tInference time: 0.020927s\n",
      "Testing sma_dos...\n",
      "...done! \tInference time: 0.021996s\n",
      "Training and testing sma_shell......done! Training time: 0.003987s\tInference time: 0.019934s\n",
      "Testing sma_shell...\n",
      "...done! \tInference time: 0.020941s\n",
      "Training and testing sma_fuzz......done! Training time: 0.003996s\tInference time: 0.019928s\n",
      "Testing sma_fuzz...\n",
      "...done! \tInference time: 0.019971s\n",
      "Training and testing sma_bdoor......done! Training time: 0.004996s\tInference time: 0.018865s\n",
      "Testing sma_bdoor...\n",
      "...done! \tInference time: 0.021889s\n",
      "Training and testing sma_ana......done! Training time: 0.003993s\tInference time: 0.016943s\n",
      "Testing sma_ana...\n",
      "...done! \tInference time: 0.020931s\n",
      "Total training time: 0.029899s\t AvgFPR: 0.014979\t AvgTPR: 0.985324\tTotal inference time: 0.151659s (fake: 0.137418s)\n"
     ]
    }
   ],
   "source": [
    "sma_ensemble_df = pd.DataFrame()\n",
    "adv_ensemble_df_base = pd.DataFrame()\n",
    "adv_ensemble_df_adv = pd.DataFrame()\n",
    "\n",
    "benign_train = benign_df[benign_df['is_train']==True]\n",
    "benign_test = benign_df[benign_df['is_test']==True]\n",
    "\n",
    "\n",
    "sma_ens_time = 0\n",
    "sma_ens_avgFPR = 0\n",
    "sma_tot_TP = 0\n",
    "sma_tot_P = 0\n",
    "sma_ens_infer_time = 0\n",
    "sma_fakeEns_infer_time = 0\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_train = {a}_df[{a}_df['is_train']==True]\")\n",
    "    exec(f\"{a}_test = {a}_df[{a}_df['is_test']==True]\")\n",
    "\n",
    "    exec(f\"train = pd.concat([benign_train, {a}_train])\")\n",
    "    exec(f\"test = pd.concat([benign_test, {a}_test])\")\n",
    "\n",
    "    exec(f\"sma_{a}Clf, sma_{a}Pred, sma_{a}Result = develop_clf(train, test, small_features, clf_name='sma_{a}', clf_type=base_clf, verbose=1)\")\n",
    "    exec(f\"sma_fakeEns_infer_time += sma_{a}Result.infer_time\")\n",
    "    \n",
    "    exec(f\"sma_{a}_allPred, sma_{a}_allResults, sma_{a}Result.infer_time = evaluate_clf(sma_{a}Clf, all_test, small_features, clf_name='sma_{a}', time=sma_{a}Result.time, verbose=1)\")\n",
    "    exec(f\"sma_ensemble_df['{a}'] = sma_{a}_allPred\")\n",
    "\n",
    "    exec(f\"adv_{a}Pred_base = sma_{a}Clf.predict(mal_base[small_features])\")\n",
    "    exec(f\"adv_{a}Pred_adv = sma_{a}Clf.predict(mal_adv[small_features])\")\n",
    "    exec(f\"adv_ensemble_df_base['{a}'] = adv_{a}Pred_base\")\n",
    "    exec(f\"adv_ensemble_df_adv['{a}'] = adv_{a}Pred_adv\")\n",
    "\n",
    "\n",
    "\n",
    "    exec(f\"sma_tot_TP += (sma_{a}Result.rec * len({a}_test))\")\n",
    "    exec(f\"sma_tot_P += len({a}_test)\")\n",
    "\n",
    "\n",
    "    exec(f\"sma_ens_avgFPR += sma_{a}Result.fpr\")\n",
    "    exec(f\"sma_ens_time += sma_{a}Result.time\")\n",
    "    exec(f\"sma_ens_infer_time +=sma_{a}Result.infer_time\")\n",
    "\n",
    "\n",
    "\n",
    "sma_ens_avgFPR = sma_ens_avgFPR / len(attack_names)\n",
    "sma_ens_avgREC = sma_tot_TP/sma_tot_P\n",
    "\n",
    "\n",
    "print(\"Total training time: {:5f}s\\t AvgFPR: {:5f}\\t AvgTPR: {:5f}\\tTotal inference time: {:5f}s (fake: {:5f}s)\".\n",
    "      format(sma_ens_time, sma_ens_avgFPR, sma_ens_avgREC, sma_ens_infer_time, sma_fakeEns_infer_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing real Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma_ensemble_df[\"sum\"] = sma_ensemble_df.sum(axis=1)\n",
    "sma_ensemble_df[\"LOR\"] = (sma_ensemble_df[\"sum\"]>0)\n",
    "temp = all_test['Nature'] \n",
    "sma_ensemble_df['True'] = ((temp.reset_index(drop=True)) > 0)\n",
    "\n",
    "adv_ensemble_df_base[\"sum\"] = adv_ensemble_df_base.sum(axis=1)\n",
    "adv_ensemble_df_base[\"LOR\"] = (adv_ensemble_df_base[\"sum\"]>0)\n",
    "temp = mal_base['Nature']\n",
    "adv_ensemble_df_base['True'] = ((temp.reset_index(drop=True)) > 0)\n",
    "\n",
    "\n",
    "adv_ensemble_df_adv[\"sum\"] = adv_ensemble_df_adv.sum(axis=1)\n",
    "adv_ensemble_df_adv[\"LOR\"] = (adv_ensemble_df_adv[\"sum\"]>0)\n",
    "temp = mal_adv['Nature']\n",
    "adv_ensemble_df_adv['True'] = ((temp.reset_index(drop=True)) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Logical OR (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 3280 out of 121123 (Recall: 0.997065\tFPR: 0.032180)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>96782</td>\n",
       "      <td>3218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>62</td>\n",
       "      <td>21061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  96782   3218\n",
       "True      62  21061"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_enslorResult = Result(sma_ensemble_df['True'], sma_ensemble_df['LOR'], sma_ens_time, sma_ens_infer_time)\n",
    "sma_enslorErr= int(len(all_test) * (1-sma_enslorResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_enslorErr, len(all_test), sma_enslorResult.rec, sma_enslorResult.fpr))\n",
    "sma_enslorResult.ctab_bin # you can also try with sma_enslorResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logical OR: Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.980\n",
      "Adversarial Recall (attack): 0.976\n"
     ]
    }
   ],
   "source": [
    "adv_enslor_base_rec = recall_score(mal_base['Nature'], adv_ensemble_df_base[\"LOR\"])\n",
    "adv_enslor_adv_rec = recall_score(mal_adv['Nature'], adv_ensemble_df_adv[\"LOR\"])\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_enslor_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_enslor_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Majority Voting (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 2090 out of 121123 (Recall: 0.970601\tFPR: 0.014700)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>98530</td>\n",
       "      <td>1470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>621</td>\n",
       "      <td>20502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  98530   1470\n",
       "True     621  20502"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_ensemble_df[\"MAJV\"] = (sma_ensemble_df[\"sum\"]>=min_agree)\n",
    "sma_ensvotResult = Result(sma_ensemble_df['True'], sma_ensemble_df['MAJV'], sma_ens_time, sma_ens_infer_time)\n",
    "sma_ensvotErr = int(len(all_test) * (1-sma_ensvotResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_ensvotErr, len(all_test), sma_ensvotResult.rec, sma_ensvotResult.fpr))\n",
    "sma_ensvotResult.ctab_bin # you can also try with sma_ensvotResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Majority Voting: Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.961\n",
      "Adversarial Recall (attack): 0.961\n"
     ]
    }
   ],
   "source": [
    "adv_ensemble_df_base[\"MAJV\"] = (adv_ensemble_df_base[\"sum\"]>=min_agree)\n",
    "adv_ensemble_df_adv[\"MAJV\"] = (adv_ensemble_df_adv[\"sum\"]>=min_agree)\n",
    "\n",
    "adv_ensvot_base_rec = recall_score(mal_base['Nature'], adv_ensemble_df_base[\"MAJV\"])\n",
    "adv_ensvot_adv_rec = recall_score(mal_adv['Nature'], adv_ensemble_df_adv[\"MAJV\"])\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_ensvot_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_ensvot_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Stacked Classifier (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 2339 out of 121123 (Recall: 0.997017\tFPR: 0.022770)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97723</td>\n",
       "      <td>2277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>21060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     97723   2277\n",
       "1        63  21060"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_list = []\n",
    "for a in attack_names:\n",
    "    exec(f\"clf_list.append(sma_{a}Clf)\")\n",
    "\n",
    "sma_sClf = StackingClassifier(classifiers=clf_list, meta_classifier=meta, fit_base_estimators=False, use_probas = False)\n",
    "s_timeStart = time.time()\n",
    "sma_sClf.fit(all_train[small_features], all_train['Nature'])\n",
    "sma_s_time = time.time() - s_timeStart + sma_ens_time\n",
    "s_timeStart = time.time()\n",
    "sma_sPred = sma_sClf.predict(all_test[small_features])\n",
    "sma_s_infer_time = time.time()-s_timeStart\n",
    "sma_sResult = Result(all_test['Nature'], sma_sPred, sma_s_time, sma_s_infer_time)\n",
    "if sma_sResult.acc < sma_sResult.acc_multi:\n",
    "    sma_sResult.acc = sma_sResult.acc_multi\n",
    "sma_sErr = int(len(all_test) * (1-sma_sResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_sErr, len(all_test), sma_sResult.rec, sma_sResult.fpr))\n",
    "\n",
    "sma_sResult.ctab_bin # you can also try with sma_sResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked Classifier: Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.979\n",
      "Adversarial Recall (attack): 0.975\n"
     ]
    }
   ],
   "source": [
    "adv_sPred_base = sma_sClf.predict(mal_base[small_features])\n",
    "adv_sPred_adv = sma_sClf.predict(mal_adv[small_features])\n",
    "\n",
    "adv_ensstk_base_rec =  recall_score(mal_base['Nature'], adv_sPred_base)\n",
    "adv_ensstk_adv_rec = recall_score(mal_adv['Nature'], adv_sPred_adv)\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_ensstk_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_ensstk_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now inspect the results by referring to the \"Result\" variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR 0.9991005065568338 0.9965913932680017 0.9902949391658382 0.999479240638167 0.8863324338398901 0.9989111395161672 \n",
      "FPR 0.020889999999999964 0.02068000000000003 0.012467142857142852 0.023700000000000054 0.011220000000000008 0.021050000000000013 \n",
      "Training Time 0.00797271728515625 0.011973857879638672 0.03490853309631348 0.03490853309631348 0.03490853309631348 0.0478816032409668 \n",
      "Inference Time 0.045845985412597656 0.04284214973449707 0.23419570922851562 0.2631566524505615 0.2631566524505615 0.1954200267791748 \n",
      "Accuracy 0.8994658322531641 0.9823320096100658 0.5402767247915087 0.5403115087818965\n"
     ]
    }
   ],
   "source": [
    "## BASELINE RESULTS (on Complete Feature Set)\n",
    "print(\n",
    "    \"TPR\",\n",
    "    bResult.rec,\n",
    "    mResult.rec,\n",
    "    ens_avgREC,\n",
    "    enslorResult.rec,\n",
    "    ensvotResult.rec,\n",
    "    sResult.rec,\n",
    "    \"\\nFPR\",\n",
    "    bResult.fpr,\n",
    "    mResult.fpr,\n",
    "    ens_avgFPR,\n",
    "    enslorResult.fpr,\n",
    "    ensvotResult.fpr,\n",
    "    sResult.fpr,\n",
    "    \"\\nTraining Time\",\n",
    "    bResult.time,\n",
    "    mResult.time,\n",
    "    ens_time,\n",
    "    enslorResult.time,\n",
    "    ensvotResult.time,\n",
    "    sResult.time,\n",
    "    \"\\nInference Time\",\n",
    "    bResult.infer_time,\n",
    "    mResult.infer_time,\n",
    "    fakeEns_infer_time,\n",
    "    enslorResult.infer_time,\n",
    "    ensvotResult.infer_time,\n",
    "    sResult.infer_time, \n",
    "    \"\\nAccuracy\",\n",
    "    mResult.acc_multi,   # This is the accuracy on the multiclassification\n",
    "    mResult.acc,         # This is the accuracy on the binary classification\n",
    "    mcResult.acc_multic, # This is the accuracy on the multiclassification AFTER the output of the binary classifier (it does not account for benign samples, which are false positives)\n",
    "    mcResult.acc_multi   # This is the accuracy on the multiclassification on the whole test portion of the malicious dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR 0.9955972163045022 0.9910050655683378 0.9853240543483407 0.9970648108696681 0.9706007669365146 0.9970174691095015 \n",
      "FPR 0.020140000000000047 0.02124999999999999 0.014978571428571444 0.032179999999999986 0.014700000000000046 0.022769999999999957 \n",
      "Training Time 0.005635738372802734 0.007981300354003906 0.029898643493652344 0.029898643493652344 0.029898643493652344 0.04385995864868164 \n",
      "Inference Time 0.03046107292175293 0.025905847549438477 0.13741827011108398 0.1516585350036621 0.1516585350036621 0.12357854843139648 \n",
      "Accuracy 0.8965596955161282 0.9808871973118235 0.5290537327627199 0.5281446764190693\n"
     ]
    }
   ],
   "source": [
    "## BASELINE RESULTS (on Essential Feature Set)\n",
    "print(\n",
    "    \"TPR\",\n",
    "    sma_bResult.rec,\n",
    "    sma_mResult.rec,\n",
    "    sma_ens_avgREC,\n",
    "    sma_enslorResult.rec,\n",
    "    sma_ensvotResult.rec,\n",
    "    sma_sResult.rec,\n",
    "    \"\\nFPR\",\n",
    "    sma_bResult.fpr,\n",
    "    sma_mResult.fpr,\n",
    "    sma_ens_avgFPR,\n",
    "    sma_enslorResult.fpr,\n",
    "    sma_ensvotResult.fpr,\n",
    "    sma_sResult.fpr,\n",
    "    \"\\nTraining Time\",\n",
    "    sma_bResult.time,\n",
    "    sma_mResult.time,\n",
    "    sma_ens_time,\n",
    "    sma_enslorResult.time,\n",
    "    sma_ensvotResult.time,\n",
    "    sma_sResult.time,\n",
    "    \"\\nInference Time\",\n",
    "    sma_bResult.infer_time,\n",
    "    sma_mResult.infer_time,\n",
    "    sma_fakeEns_infer_time,\n",
    "    sma_enslorResult.infer_time,\n",
    "    sma_ensvotResult.infer_time,\n",
    "    sma_sResult.infer_time, \n",
    "    \"\\nAccuracy\",\n",
    "    sma_mResult.acc_multi,   # This is the accuracy on the multiclassification\n",
    "    sma_mResult.acc,         # This is the accuracy on the binary classification\n",
    "    sma_mcResult.acc_multic, # This is the accuracy on the multiclassification AFTER the output of the binary classifier (it does not account for benign samples, which are false positives)\n",
    "    sma_mcResult.acc_multi   # This is the accuracy on the multiclassification on the whole test portion of the malicious dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
      "      BD: TPR=0.980466\tFPR=0.019947\n",
      "      MD (binarized) CLF: TPR=0.973689\tFPR=0.020686\n",
      "      ED-o: TPR=0.996930\tFPR=0.023234\n",
      "      ED-v: TPR=0.841133\tFPR=0.009054\n",
      "      ED-s: TPR=0.984115\tFPR=0.020764\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "## Open World: One attack against all (averaged results)\n",
    "\n",
    "print('''Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
    "      BD: TPR={:5f}\\tFPR={:5f}\n",
    "      MD (binarized) CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      ED-o: TPR={:5f}\\tFPR={:5f}\n",
    "      ED-v: TPR={:5f}\\tFPR={:5f}\n",
    "      ED-s: TPR={:5f}\\tFPR={:5f}\n",
    "      '''.format(oaac_bin_rec, oaac_bin_fpr,\n",
    "                 oaac_multi_rec, oaac_multi_fpr,\n",
    "                 oaac_enslor_rec, oaac_enslor_fpr,\n",
    "                 oaac_ensvot_rec, oaac_ensvot_fpr,\n",
    "                 oaac_ensstk_rec, oaac_ensstk_fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BD (before, after): 0.985474006116208 1.0 \n",
      "MD  (before, after): 0.9805045871559633 0.9977064220183486 \n",
      "ED-o  (before, after): 0.9797400611620795 0.9759174311926605 \n",
      "ED-v  (before, after): 0.9613914373088684 0.9613914373088684 \n",
      "ED-s  (before, after): 0.9793577981651376 0.9751529051987767\n"
     ]
    }
   ],
   "source": [
    "## Adversarial Attacks \n",
    "\n",
    "print(\n",
    "    \"BD (before, after):\",\n",
    "    adv_bin_base_rec,\n",
    "    adv_bin_adv_rec,\n",
    "    \"\\nMD  (before, after):\",\n",
    "    adv_multi_base_rec,\n",
    "    adv_multi_adv_rec,\n",
    "    \"\\nED-o  (before, after):\",\n",
    "    adv_enslor_base_rec,\n",
    "    adv_enslor_adv_rec, \n",
    "    \"\\nED-v  (before, after):\",\n",
    "    adv_ensvot_base_rec,\n",
    "    adv_ensvot_adv_rec, \n",
    "    \"\\nED-s  (before, after):\",\n",
    "    adv_ensstk_base_rec, \n",
    "    adv_ensstk_adv_rec,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
