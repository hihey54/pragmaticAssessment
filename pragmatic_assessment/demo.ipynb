{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "\n",
    "\n",
    "root_folder = \"..\\\\data\\\\IDS17\\\\flows\\\\\"\n",
    "root_folder = \"I:\\\\Datasets\\\\NIDS-Datasets\\\\preprocessed\\\\GTCS\\\\flows\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PARAMETERS #####\n",
    "\n",
    "test_size = 0.2 # proportion of the dataset used for testing. We always kept it fixed to 0.2 for our paper\n",
    "train_size = 0.99 # proportion of the REMAINING data that are used for training (if >1, then it will take that exact amount). To reproduce the results of the paper, use: 100 (for \"limited\" training data) or 0.2 or 0.5 or 0.99 (for scarce, moderate, abundant training data, respectively) \n",
    "max_size = 500000 ## maximum amount of samples to include when creating the initial dataframes. This is fixed in our paper\n",
    "max_size_atk = int(max_size / 3) # maximum amount of malicious samples per class. This is fixed in our paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading input data\n",
    "\n",
    "malicious_folder = root_folder + \"malicious/\"\n",
    "\n",
    "benign_file = root_folder + \"benign.csv\"\n",
    "benign_df = pd.read_csv(benign_file, header='infer', index_col=0)\n",
    "benign_df = benign_df.sample(min(max_size, len(benign_df)))\n",
    "\n",
    "benign_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "attack_names = [\"ddos\", \"bot\", \"brute\", \"infi\"] # these are the attacks in the GTCS dataset\n",
    "\n",
    "ddos_file = malicious_folder + \"ddos.csv\"\n",
    "bot_file = malicious_folder + \"botnet.csv\"\n",
    "brute_file = malicious_folder + \"bruteforce.csv\"\n",
    "infi_file = malicious_folder + \"infiltration.csv\"\n",
    "\n",
    "\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_df = pd.read_csv({a}_file, header='infer', index_col=0)\")\n",
    "    exec(f\"{a}_df = {a}_df.sample(min(max_size_atk, len({a}_df)))\")\n",
    "    exec(f\"{a}_df.reset_index(inplace=True, drop=True)\")\n",
    "    exec(f\"{a}_df['Label'] = a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining Train and Test sets for each class\n",
    "\n",
    "df_list = [benign_df]\n",
    "for a in attack_names:\n",
    "    exec(f\"df_list.append({a}_df)\")\n",
    "\n",
    "\n",
    "for dummy_df in df_list:\n",
    "    if train_size <= 1:\n",
    "        train_threshold = test_size + (1-test_size)*train_size\n",
    "    else:\n",
    "        train_threshold = test_size + ((train_size * 100) / (len(dummy_df)) / 100)       \n",
    "    dummy_df['seed'] = (np.random.uniform(0,1,len(dummy_df)))\n",
    "    dummy_df['is_test'] = np.where(dummy_df['seed'] <= test_size, True, False)\n",
    "    dummy_df['is_train'] = np.where((dummy_df['seed'] <= train_threshold) & (dummy_df['is_test']==False), True, False)\n",
    "\n",
    "# get all together\n",
    "all_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 0 & \\textit{Benign} & 139186 & \\\\ \\cline{2-4}\n",
      "& 1 & \\textit{ddos} & 131211 \\\\ \\cline{2-4}\n",
      "& 2 & \\textit{bot} & 93021 \\\\ \\cline{2-4}\n",
      "& 3 & \\textit{brute} & 83857 \\\\ \\cline{2-4}\n",
      "& 4 & \\textit{infi} & 70202 \\\\ \\cline{2-4}\n"
     ]
    }
   ],
   "source": [
    "def handle_categorical(df):\n",
    "    ## Handling categorical data\n",
    "    df_dummy = df.copy(deep=True)\n",
    "    df_dummy['Nature'] = np.where(df_dummy['Label'].str.contains('BENIGN'),0,1)\n",
    "\n",
    "    for column_name in df_dummy.columns:\n",
    "        if column_name == ('SrcPort_type'):\n",
    "            df_dummy[column_name] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('DstPort_type'):\n",
    "            df_dummy[column_name] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('Protocol'):\n",
    "            df_dummy[column_name+'-f'] = pd.factorize(df_dummy[column_name])[0]\n",
    "        else:\n",
    "            pass\n",
    "    return df_dummy\n",
    "\n",
    "all_df = handle_categorical(all_df)\n",
    "all_df['Label_cat'] = pd.factorize(all_df['Label'])[0]\n",
    "all_df['int2int'] = np.where( ((all_df['SrcIP_internal']==True) & (all_df['DstIP_internal']==True)), True, False)\n",
    "all_df['Duration(s)'] = all_df['FlowDuration'] / 1000000\n",
    "all_df['DstPkt'] = all_df['BwdPkts/s'] * all_df['Duration(s)']\n",
    "all_df['SrcPkt'] = all_df['FwdPkts/s'] * all_df['Duration(s)']\n",
    "all_df['DstByt'] = all_df['DstPkt'] * all_df['BwdSegSizeAvg']\n",
    "all_df['SrcByt'] = all_df['SrcPkt'] * all_df['FwdSegSizeAvg']\n",
    "all_df['totPkt'] = all_df['SrcPkt'] + all_df['DstPkt']\n",
    "all_df['totByt'] = all_df['SrcByt'] + all_df['DstByt']\n",
    "\n",
    "all_train, all_test = all_df[all_df['is_train']==True], all_df[all_df['is_test']==True]\n",
    "\n",
    "### SPLITTING ALL BACK ####\n",
    "benign_df = all_df[all_df['Label']=='BENIGN']\n",
    "benign_train = benign_df[benign_df['is_train']==True]\n",
    "benign_test = benign_df[benign_df['is_test']==True]\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_df = all_df[all_df['Label']=='{a}']\")\n",
    "\n",
    "malicious_df = all_df[all_df['Label']!='BENIGN']\n",
    "malicious_train, malicious_test = malicious_df[malicious_df['is_train']==True], malicious_df[malicious_df['is_test']==True]\n",
    "\n",
    "print(\"& 0 & \\\\textit{{Benign}} & {} & \\\\\\\\ \\\\cline{{2-4}}\".format(len(benign_df)))\n",
    "\n",
    "\n",
    "for i,a in enumerate(attack_names):\n",
    "    exec(f\"print('& {i+1} & \\\\\\\\textit{{{{{a}}}}} & {{}} \\\\\\\\\\\\\\\\ \\\\\\\\cline{{{{2-4}}}}'.format(len({a}_df)))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature sets\n",
    "\n",
    "# the following is the \"complete\" feature set\n",
    "\n",
    "features = ['Protocol-f',\n",
    "       'FlowDuration', 'TotFwdPkts', 'TotBwdPkts',\n",
    "       'TotLenFwdPkts', 'TotLenBwdPkts', 'FwdPktLenMax', 'FwdPktLenMin',\n",
    "       'FwdPktLenMean', 'FwdPktLenStd', 'BwdPktLenMax', 'BwdPktLenMin',\n",
    "       'BwdPktLenMean', 'BwdPktLenStd', 'FlowByts/s', 'FlowPkts/s',\n",
    "       'FlowIATMean', 'FlowIATStd', 'FlowIATMax', 'FlowIATMin', 'FwdIATTot',\n",
    "       'FwdIATMean', 'FwdIATStd', 'FwdIATMax', 'FwdIATMin', 'BwdIATTot',\n",
    "       'BwdIATMean', 'BwdIATStd', 'BwdIATMax', 'BwdIATMin', 'FwdPSHFlags',\n",
    "       'BwdPSHFlags', 'FwdURGFlags', 'BwdURGFlags', 'FwdHeaderLen',\n",
    "       'BwdHeaderLen', 'FwdPkts/s', 'BwdPkts/s', 'PktLenMin', 'PktLenMax',\n",
    "       'PktLenMean', 'PktLenStd', 'PktLenVar', 'FINFlagCnt', 'SYNFlagCnt',\n",
    "       'RSTFlagCnt', 'PSHFlagCnt', 'ACKFlagCnt', 'URGFlagCnt', 'CWEFlagCount',\n",
    "       'ECEFlagCnt', 'Down/UpRatio', 'PktSizeAvg', 'FwdSegSizeAvg',\n",
    "       'BwdSegSizeAvg', 'FwdByts/bAvg', 'FwdPkts/bAvg', 'FwdBlkRateAvg',\n",
    "       'BwdByts/bAvg', 'BwdPkts/bAvg', 'BwdBlkRateAvg', 'SubflowFwdPkts',\n",
    "       'SubflowFwdByts', 'SubflowBwdPkts', 'SubflowBwdByts', 'InitFwdWinByts',\n",
    "       'InitBwdWinByts', 'FwdActDataPkts', 'FwdSegSizeMin', 'ActiveMean',\n",
    "       'ActiveStd', 'ActiveMax', 'ActiveMin', 'IdleMean', 'IdleStd', 'IdleMax',\n",
    "       'IdleMin', 'SrcPort_type',\n",
    "       'DstPort_type', 'int2int'\n",
    "       ]\n",
    "\n",
    "# this is for the \"essential\" feature set\n",
    "small_features = ['Protocol-f', 'Duration(s)', 'totPkt', 'totByt',\n",
    "                'DstPkt', 'SrcPkt', 'DstByt', 'SrcByt', 'SrcPort_type', \n",
    "                  'DstPort_type', 'FwdPSHFlags', 'BwdPSHFlags', 'FwdURGFlags', 'BwdURGFlags',\n",
    "                  'FINFlagCnt',\n",
    "       'SYNFlagCnt', 'RSTFlagCnt', 'PSHFlagCnt', 'ACKFlagCnt',\n",
    "       'URGFlagCnt', 'ECEFlagCnt', \n",
    "                  #'int2int'\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TRAIN:\t 409978\n",
      "Size of TEST:\t 103330\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of TRAIN:\\t\", len(all_train))\n",
    "print(\"Size of TEST:\\t\", len(all_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb = HistGradientBoostingClassifier(loss='log_loss', learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, \n",
    "                                    min_samples_leaf=20, l2_regularization=0.0, max_bins=255,\n",
    "                                    monotonic_cst=None, warm_start=False, early_stopping='auto', scoring='loss', \n",
    "                                    validation_fraction=0.1, n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HGB......done! Training runtime: 10.61s\n",
      "Testing HGB......done! Inference runtime: 0.39s\n",
      "HGB performance: \tRecall=0.997\tFPR=0.041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_y = all_train[\"Nature\"]\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Training HGB...\", end=\"\", flush=True)\n",
    "hgb.fit(all_train[features], train_y)\n",
    "hgb_trainTime = time.time() - start_time\n",
    "print(\"...done! Training runtime: {:.2f}s\".format(hgb_trainTime))\n",
    "\n",
    "print(\"Testing HGB...\", end=\"\", flush=True)\n",
    "start_time = time.time()\n",
    "hgb_pred = hgb.predict(all_test[features])\n",
    "hgb_inferTime = time.time() - start_time\n",
    "print(\"...done! Inference runtime: {:.2f}s\".format(hgb_inferTime))\n",
    "hgb_tpr = recall_score(all_test['Nature'], hgb_pred, zero_division=0, pos_label=1)\n",
    "hgb_fpr = 1-recall_score(all_test['Nature'], hgb_pred, zero_division=0, pos_label=0)\n",
    "\n",
    "print(\"HGB performance: \\tRecall={:.3f}\\tFPR={:.3f}\\n\".format(hgb_tpr, hgb_fpr))\n",
    "# pd.crosstab(all_test['Nature'], hgb_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, \n",
    "                                     min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                     random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                     class_weight=None, ccp_alpha=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DT......done! Training runtime: 6.87s\n",
      "Testing DT......done! Inference runtime: 0.05s\n",
      "DT performance: \tRecall=0.9902\tFPR=0.0271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_y = all_train[\"Nature\"]\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Training DT...\", end=\"\", flush=True)\n",
    "dt.fit(all_train[features], train_y)\n",
    "dt_trainTime = time.time() - start_time\n",
    "print(\"...done! Training runtime: {:.2f}s\".format(dt_trainTime))\n",
    "\n",
    "print(\"Testing DT...\", end=\"\", flush=True)\n",
    "start_time = time.time()\n",
    "dt_pred = dt.predict(all_test[features])\n",
    "dt_inferTime = time.time() - start_time\n",
    "print(\"...done! Inference runtime: {:.2f}s\".format(dt_inferTime))\n",
    "dt_tpr = recall_score(all_test['Nature'], dt_pred, zero_division=0, pos_label=1)\n",
    "dt_fpr = 1-recall_score(all_test['Nature'], dt_pred, zero_division=0, pos_label=0)\n",
    "\n",
    "print(\"DT performance: \\tRecall={:.4f}\\tFPR={:.4f}\\n\".format(dt_tpr, dt_fpr))\n",
    "# pd.crosstab(all_test['Nature'], dt_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                                 min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', \n",
    "                                 max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, \n",
    "                                 n_jobs=-1, random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "                                 ccp_alpha=0.0, max_samples=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RF......done! Training runtime: 10.01s\n",
      "Testing RF......done! Inference runtime: 0.28s\n",
      "RF performance: \tRecall=0.9942\tFPR=0.0275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_y = all_train[\"Nature\"]\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Training RF...\", end=\"\", flush=True)\n",
    "rf.fit(all_train[features], train_y)\n",
    "rf_trainTime = time.time() - start_time\n",
    "print(\"...done! Training runtime: {:.2f}s\".format(rf_trainTime))\n",
    "\n",
    "print(\"Testing RF...\", end=\"\", flush=True)\n",
    "start_time = time.time()\n",
    "rf_pred = rf.predict(all_test[features])\n",
    "rf_inferTime = time.time() - start_time\n",
    "print(\"...done! Inference runtime: {:.2f}s\".format(rf_inferTime))\n",
    "rf_tpr = recall_score(all_test['Nature'], rf_pred, zero_division=0, pos_label=1)\n",
    "rf_fpr = 1-recall_score(all_test['Nature'], rf_pred, zero_division=0, pos_label=0)\n",
    "\n",
    "print(\"RF performance: \\tRecall={:.4f}\\tFPR={:.4f}\\n\".format(rf_tpr, rf_fpr))\n",
    "# pd.crosstab(all_test['Nature'], rf_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about Deep Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', alpha=0.0001, \n",
    "    batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, \n",
    "    max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, \n",
    "    warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n",
    "    validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, \n",
    "    n_iter_no_change=20, max_fun=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = all_train[\"Nature\"]\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Training DNN...\", end=\"\", flush=True)\n",
    "dnn.fit(all_train[features], train_y)\n",
    "dnn_trainTime = time.time() - start_time\n",
    "print(\"...done! Training time: {}\".format(dnn_trainTime))\n",
    "\n",
    "print(\"Testing DNN...\", end=\"\", flush=True)\n",
    "start_time = time.time()\n",
    "dnn_pred = dnn.predict(all_test[features])\n",
    "dnn_inferTime = time.time() - start_time\n",
    "print(\"...done! Inference time: {}\".format(dnn_inferTime))\n",
    "dnn_tpr = recall_score(all_test['Nature'], dnn_pred, zero_division=0, pos_label=1)\n",
    "dnn_fpr = 1-recall_score(all_test['Nature'], dnn_pred, zero_division=0, pos_label=0)\n",
    "\n",
    "print(\"DNN performance: \\tRecall={:.4f}\\tFPR={:.4f}\\n\".format(dnn_tpr, dnn_fpr))\n",
    "# pd.crosstab(all_test['Nature'], dnn_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See for yourself how long it takes to train a \"Deep\" neural network that performs equally well to the three \"shallow\" ML algorithms used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
