{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from supportFunctions import *\n",
    "\n",
    "root_folder = \"..\\\\data\\\\GTCS\\\\flows\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PARAMETERS #####\n",
    "\n",
    "## Generic parameters\n",
    "temporal = True # used to determine if this evaluation assumes a \"temporal\" dependency among its samples\n",
    "dataset_name = 'GTCS' # used to create the output\n",
    "base_clf = 'dt' # name of the classifier used for this \"run\". Available names: ['dt', 'rf', 'hgb', 'lr']. You can add more by editing the supportFunctions file\n",
    "test_size = 0.2 # proportion of the dataset used for testing. We always kept it fixed to 0.2 for our paper\n",
    "train_size = 100 # proportion of the REMAINING data that are used for training (if >1, then it will take that exact amount). To reproduce the results of the paper, use: 100 (for \"limited\" training data) or 0.2 or 0.5 or 0.99 (for scarce, moderate, abundant training data, respectively) \n",
    "agreement = 0.5 # from 0 to 1. Proportion of classifiers that must agree on an attack (for the ensemble). This is fixed in our paper.\n",
    "max_size = 500000 ## maximum amount of samples to include when creating the initial dataframes. This is fixed in our paper\n",
    "max_size_atk = int(max_size / 3) # maximum amount of malicious samples per class. This is fixed in our paper\n",
    "\n",
    "## Adversarial Attacks parameters\n",
    "atk_intensity = 10 # send 100 packets of 100 bytes each, over 100 seconds\n",
    "pkt_intensity = atk_intensity * 10 # \n",
    "byt_intensity = pkt_intensity * 10 # \n",
    "dur_intensity = atk_intensity * 10 # consider seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading input data\n",
    "\n",
    "#malicious_folder = root_folder + \"malicious\\\\\"\n",
    "malicious_folder = root_folder + \"malicious/\"\n",
    "\n",
    "benign_file = root_folder + \"benign.csv\"\n",
    "benign_df = pd.read_csv(benign_file, header='infer', index_col=0)\n",
    "benign_df = benign_df.sample(min(max_size, len(benign_df)))\n",
    "#sort by timestamp\n",
    "if temporal == True:\n",
    "    benign_df = benign_df.sort_values(by=['Timestamp'])\n",
    "    \n",
    "benign_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "attack_names = [\"ddos\", \"bot\", \"brute\", \"infi\"] # these are the attacks in the GTCS dataset\n",
    "\n",
    "ddos_file = malicious_folder + \"ddos.csv\"\n",
    "bot_file = malicious_folder + \"botnet.csv\"\n",
    "brute_file = malicious_folder + \"bruteforce.csv\"\n",
    "infi_file = malicious_folder + \"infiltration.csv\"\n",
    "\n",
    "\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_df = pd.read_csv({a}_file, header='infer', index_col=0)\")\n",
    "    exec(f\"{a}_df = {a}_df.sample(min(max_size_atk, len({a}_df)))\")\n",
    "    # sort by timestamp\n",
    "    if temporal == True:\n",
    "        exec(f\"{a}_df = {a}_df.sort_values(by=['Timestamp'])\")\n",
    "    exec(f\"{a}_df.reset_index(inplace=True, drop=True)\")\n",
    "    exec(f\"{a}_df['Label'] = a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining Train and Test sets for each class\n",
    "\n",
    "df_list = [benign_df]\n",
    "for a in attack_names:\n",
    "    exec(f\"df_list.append({a}_df)\")\n",
    "\n",
    "if temporal == True:\n",
    "    for dummy_df in df_list:\n",
    "        if train_size <=1:\n",
    "            train_threshold = int(((1-test_size) * train_size) * len(dummy_df))\n",
    "        else:\n",
    "            train_threshold = int(100)\n",
    "        test_threshold = len(dummy_df) - int(test_size * len(dummy_df))\n",
    "        dummy_df['index'] = dummy_df.index\n",
    "        dummy_df['is_test'] = np.where(dummy_df['index'] >= test_threshold , True, False)\n",
    "        dummy_df['is_train'] = np.where(dummy_df['index'] <= train_threshold , True, False)\n",
    "else:\n",
    "    for dummy_df in df_list:\n",
    "        if train_size <= 1:\n",
    "            train_threshold = test_size + (1-test_size)*train_size\n",
    "        else:\n",
    "            train_threshold = test_size + ((train_size * 100) / (len(dummy_df)) / 100)       \n",
    "        dummy_df['seed'] = (np.random.uniform(0,1,len(dummy_df)))\n",
    "        dummy_df['is_test'] = np.where(dummy_df['seed'] <= test_size, True, False)\n",
    "        dummy_df['is_train'] = np.where((dummy_df['seed'] <= train_threshold) & (dummy_df['is_test']==False), True, False)\n",
    "\n",
    "# get all together\n",
    "all_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 0 & \\textit{Benign} & 139186 & \\\\ \\cline{2-4}\n",
      "& 1 & \\textit{ddos} & 131211 \\\\ \\cline{2-4}\n",
      "& 2 & \\textit{bot} & 93021 \\\\ \\cline{2-4}\n",
      "& 3 & \\textit{brute} & 83857 \\\\ \\cline{2-4}\n",
      "& 4 & \\textit{infi} & 70202 \\\\ \\cline{2-4}\n"
     ]
    }
   ],
   "source": [
    "def handle_categorical(df):\n",
    "    ## Handling categorical data\n",
    "    df_dummy = df.copy(deep=True)\n",
    "    df_dummy['Nature'] = np.where(df_dummy['Label'].str.contains('BENIGN'),0,1)\n",
    "\n",
    "    for column_name in df_dummy.columns:\n",
    "        if column_name == ('SrcPort_type'):\n",
    "            df_dummy[column_name] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('DstPort_type'):\n",
    "            df_dummy[column_name] = pd.factorize(df_dummy[column_name])[0]\n",
    "        elif column_name == ('Protocol'):\n",
    "            df_dummy[column_name+'-f'] = pd.factorize(df_dummy[column_name])[0]\n",
    "        else:\n",
    "            pass\n",
    "    return df_dummy\n",
    "\n",
    "all_df = handle_categorical(all_df)\n",
    "all_df['Label_cat'] = pd.factorize(all_df['Label'])[0]\n",
    "all_df['int2int'] = np.where( ((all_df['SrcIP_internal']==True) & (all_df['DstIP_internal']==True)), True, False)\n",
    "all_df['Duration(s)'] = all_df['FlowDuration'] / 1000000\n",
    "all_df['DstPkt'] = all_df['BwdPkts/s'] * all_df['Duration(s)']\n",
    "all_df['SrcPkt'] = all_df['FwdPkts/s'] * all_df['Duration(s)']\n",
    "all_df['DstByt'] = all_df['DstPkt'] * all_df['BwdSegSizeAvg']\n",
    "all_df['SrcByt'] = all_df['SrcPkt'] * all_df['FwdSegSizeAvg']\n",
    "all_df['totPkt'] = all_df['SrcPkt'] + all_df['DstPkt']\n",
    "all_df['totByt'] = all_df['SrcByt'] + all_df['DstByt']\n",
    "\n",
    "all_train, all_test = all_df[all_df['is_train']==True], all_df[all_df['is_test']==True]\n",
    "\n",
    "### SPLITTING ALL BACK ####\n",
    "benign_df = all_df[all_df['Label']=='BENIGN']\n",
    "benign_train = benign_df[benign_df['is_train']==True]\n",
    "benign_test = benign_df[benign_df['is_test']==True]\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_df = all_df[all_df['Label']=='{a}']\")\n",
    "\n",
    "malicious_df = all_df[all_df['Label']!='BENIGN']\n",
    "malicious_train, malicious_test = malicious_df[malicious_df['is_train']==True], malicious_df[malicious_df['is_test']==True]\n",
    "\n",
    "print(\"& 0 & \\\\textit{{Benign}} & {} & \\\\\\\\ \\\\cline{{2-4}}\".format(len(benign_df)))\n",
    "\n",
    "\n",
    "for i,a in enumerate(attack_names):\n",
    "    exec(f\"print('& {i+1} & \\\\\\\\textit{{{{{a}}}}} & {{}} \\\\\\\\\\\\\\\\ \\\\\\\\cline{{{{2-4}}}}'.format(len({a}_df)))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature sets\n",
    "\n",
    "# the following is the \"complete\" feature set\n",
    "\n",
    "features = ['Protocol-f',\n",
    "       'FlowDuration', 'TotFwdPkts', 'TotBwdPkts',\n",
    "       'TotLenFwdPkts', 'TotLenBwdPkts', 'FwdPktLenMax', 'FwdPktLenMin',\n",
    "       'FwdPktLenMean', 'FwdPktLenStd', 'BwdPktLenMax', 'BwdPktLenMin',\n",
    "       'BwdPktLenMean', 'BwdPktLenStd', 'FlowByts/s', 'FlowPkts/s',\n",
    "       'FlowIATMean', 'FlowIATStd', 'FlowIATMax', 'FlowIATMin', 'FwdIATTot',\n",
    "       'FwdIATMean', 'FwdIATStd', 'FwdIATMax', 'FwdIATMin', 'BwdIATTot',\n",
    "       'BwdIATMean', 'BwdIATStd', 'BwdIATMax', 'BwdIATMin', 'FwdPSHFlags',\n",
    "       'BwdPSHFlags', 'FwdURGFlags', 'BwdURGFlags', 'FwdHeaderLen',\n",
    "       'BwdHeaderLen', 'FwdPkts/s', 'BwdPkts/s', 'PktLenMin', 'PktLenMax',\n",
    "       'PktLenMean', 'PktLenStd', 'PktLenVar', 'FINFlagCnt', 'SYNFlagCnt',\n",
    "       'RSTFlagCnt', 'PSHFlagCnt', 'ACKFlagCnt', 'URGFlagCnt', 'CWEFlagCount',\n",
    "       'ECEFlagCnt', 'Down/UpRatio', 'PktSizeAvg', 'FwdSegSizeAvg',\n",
    "       'BwdSegSizeAvg', 'FwdByts/bAvg', 'FwdPkts/bAvg', 'FwdBlkRateAvg',\n",
    "       'BwdByts/bAvg', 'BwdPkts/bAvg', 'BwdBlkRateAvg', 'SubflowFwdPkts',\n",
    "       'SubflowFwdByts', 'SubflowBwdPkts', 'SubflowBwdByts', 'InitFwdWinByts',\n",
    "       'InitBwdWinByts', 'FwdActDataPkts', 'FwdSegSizeMin', 'ActiveMean',\n",
    "       'ActiveStd', 'ActiveMax', 'ActiveMin', 'IdleMean', 'IdleStd', 'IdleMax',\n",
    "       'IdleMin', 'SrcPort_type',\n",
    "       'DstPort_type', 'int2int'\n",
    "       ]\n",
    "\n",
    "# this is for the \"essential\" feature set\n",
    "small_features = ['Protocol-f', 'Duration(s)', 'totPkt', 'totByt',\n",
    "                'DstPkt', 'SrcPkt', 'DstByt', 'SrcByt', 'SrcPort_type', \n",
    "                  'DstPort_type', 'FwdPSHFlags', 'BwdPSHFlags', 'FwdURGFlags', 'BwdURGFlags',\n",
    "                  'FINFlagCnt',\n",
    "       'SYNFlagCnt', 'RSTFlagCnt', 'PSHFlagCnt', 'ACKFlagCnt',\n",
    "       'URGFlagCnt', 'ECEFlagCnt', \n",
    "                  #'int2int'\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the dataset for adversarial attacks\n",
    "mal_base = malicious_df[((malicious_df['Protocol']==17)) & (malicious_df['is_test']==True)]\n",
    "mal_adv = mal_base.copy(deep=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# attacking\n",
    "max_dur = mal_adv['Duration(s)'].max()\n",
    "min_dur = mal_adv['Duration(s)'].min()\n",
    "mal_adv['Duration(s)'] = mal_adv['Duration(s)'] + dur_intensity # we increase the duration in seconds\n",
    "mal_adv['Duration(s)'] = np.where(mal_adv['Duration(s)'] > max_dur, max_dur, mal_adv['Duration(s)'])\n",
    "mal_adv['Duration(s)'] = np.where(mal_adv['Duration(s)'] < min_dur, min_dur, mal_adv['Duration(s)'])\n",
    "\n",
    "\n",
    "# mal_adv['DstPkt'] = mal_adv['DstPkt'] + pkt_intensity\n",
    "mal_adv['SrcPkt'] = mal_adv['SrcPkt'] + pkt_intensity\n",
    "#mal_adv['DstByt'] = mal_adv['DstByt'] + byt_intensity\n",
    "mal_adv['SrcByt'] = mal_adv['SrcByt'] + byt_intensity \n",
    "\n",
    "#mal_adv['SYNFlagCount'] = mal_adv['SYNFlagCount'] + pkt_intensity\n",
    "#mal_adv['ACKFlagCount'] = mal_adv['SYNFlagCount'] + pkt_intensity\n",
    "mal_adv['totPkt'] = mal_adv['SrcPkt'] + mal_adv['DstPkt']\n",
    "mal_adv['totByt'] = mal_adv['SrcByt'] + mal_adv['DstByt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROM NOW ON, THE CODE IS ALWAYS THE SAME FOR EVERY DATASET!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Assessment on \"Complete\" feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BINARY CLASSIFIER (Complete features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing bin......done! Training time: 0.009557s\tInference time: 0.064732s\n",
      "Total Misclassifications: 5533 out of 103494 (Recall: 0.976367\tFPR: 0.134569)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24091</td>\n",
       "      <td>3746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1788</td>\n",
       "      <td>73869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     24091   3746\n",
       "1      1788  73869"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bClf, bPred, bResult = develop_clf(all_train, all_test, features, clf_name='bin', label='Nature', clf_type=base_clf, verbose=1)\n",
    "\n",
    "if (bResult.acc == 0):\n",
    "    bErr = int(len(all_test) * (1-bResult.acc_multi))\n",
    "else:\n",
    "    bErr = int(len(all_test) * (1-bResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(bErr, len(all_test), bResult.rec, bResult.fpr))\n",
    "pd.crosstab(all_test['Nature'], bPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-CLASS CLASSIFIER - cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing mc......done! Training time: 0.006079s\tInference time: 0.039288s\n",
      "Total Misclassifications: 939 out of 75657\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>18534</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>508</td>\n",
       "      <td>331</td>\n",
       "      <td>13182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      1      2      3      4\n",
       "True                            \n",
       "1     26230      0      0     12\n",
       "2         0  18534      0     70\n",
       "3         0      0  16771      0\n",
       "4        19    508    331  13182"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the classifier that analyzes ONLY the malicious samples that \"receives\" from the initial binary classifier\n",
    "# It is trained on the same training set---but without using the benign samples\n",
    "# It is tested on the malicious samples in the test set that are flagged as malicious by the binary classifier\n",
    "\n",
    "\n",
    "mcClf, mcPred, mcResult = develop_clf(malicious_train, malicious_test, features, clf_name='mc', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "mcErr = int(len(malicious_test) * (1-mcResult.acc_multi))\n",
    "print(\"Total Misclassifications: {} out of {}\".format(mcErr, len(malicious_test)))\n",
    "pd.crosstab(malicious_test['Label_cat'], mcPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications (among the malicious samples): 777 out of 73869\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>18504</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>385</td>\n",
       "      <td>323</td>\n",
       "      <td>11587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      1      2      3      4\n",
       "True                            \n",
       "1     26230      0      0     12\n",
       "2         0  18504      0     38\n",
       "3         0      0  16771      0\n",
       "4        19    385    323  11587"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We select the samples flagged as malicious by the initial classifier.\n",
    "# Of course, samples flagged as malicious that are NOT actually malicious will always be misclassified\n",
    "\n",
    "all_test['bPred'] = bPred\n",
    "mc_test = all_test[(all_test['bPred']==1) & (all_test['Nature']==1)]\n",
    "if (len(mc_test)==0):\n",
    "    # in this case, this classifier receives nothing\n",
    "    print(\"There is no malicious sample flagged as malicious to analyze!\")\n",
    "\n",
    "mcPred_m = mcClf.predict(mc_test[features])\n",
    "mcResult.acc_multic = accuracy_score(mc_test['Label_cat'], mcPred_m, normalize=True, sample_weight=None)\n",
    "mcErr_m = int((1-mcResult.acc_multic) * len(mc_test))\n",
    "print(\"Total Misclassifications (among the malicious samples): {} out of {}\".format(mcErr_m, len(mc_test)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.crosstab(mc_test['Label_cat'], mcPred_m, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This classifier also analyzed 3746 benign samples that were incorrectly labelled as 'malicious' by the binary classifier\n",
      "Hence, this classifier was tested on 77615 samples, of which 4523 have been misclassified\n"
     ]
    }
   ],
   "source": [
    "## Note: We also accounted for the false positives of the first binary classifier (all of which have been considered as misclassifications)\n",
    "bin_falsePositives = int(bResult.fpr * len(benign_test))\n",
    "print(\"This classifier also analyzed {} benign samples that were incorrectly labelled as 'malicious' by the binary classifier\".format(bin_falsePositives))\n",
    "\n",
    "print(\"Hence, this classifier was tested on {} samples, of which {} have been misclassified\".format(len(mc_test)+bin_falsePositives, bin_falsePositives+mcErr_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-CLASS CLASSIFIER - stand-alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing m......done! Training time: 0.007973s\tInference time: 0.057808s\n",
      "Total Misclassifications: 7277 out of 103494\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24401</td>\n",
       "      <td>66</td>\n",
       "      <td>108</td>\n",
       "      <td>18</td>\n",
       "      <td>3244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>26230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>18401</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3251</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>331</td>\n",
       "      <td>10413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1      2      3      4\n",
       "True                                   \n",
       "0     24401     66    108     18   3244\n",
       "1        10  26230      0      0      2\n",
       "2       164      0  18401      0     39\n",
       "3         0      0      0  16771      0\n",
       "4      3251     19     26    331  10413"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first assess its multiclassification performance, and then its binary classification performance\n",
    "\n",
    "mClf, mPred, mResult = develop_clf(all_train, all_test, features, clf_name='m', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "mErr = int(len(all_test) * (1-mResult.acc_multi))\n",
    "print(\"Total Misclassifications: {} out of {}\".format(mErr, len(all_test)))\n",
    "mResult.ctab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 6861 out of 103494 (Recall: 0.954730\tFPR: 0.123433)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24401</td>\n",
       "      <td>3436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3425</td>\n",
       "      <td>72232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     24401   3436\n",
       "1      3425  72232"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the binary classification performance, we use the previous predictions\n",
    "mPred_bin = np.copy(mPred)\n",
    "mPred_bin[mPred_bin > 0] = 1\n",
    "mResult.bin_results(all_test['Nature'], mPred_bin)\n",
    "mErr_bin = int(len(all_test) * (1-mResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(mErr_bin, len(all_test), mResult.rec, mResult.fpr))\n",
    "mResult.ctab_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSEMBLE CLASSIFIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \"individual\" binary classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing ddos......done! Training time: 0.005946s\tInference time: 0.031929s\n",
      "Testing ddos...\n",
      "...done! \tInference time: 0.054805s\n",
      "Training and testing bot......done! Training time: 0.006017s\tInference time: 0.026911s\n",
      "Testing bot...\n",
      "...done! \tInference time: 0.058769s\n",
      "Training and testing brute......done! Training time: 0.004993s\tInference time: 0.026875s\n",
      "Testing brute...\n",
      "...done! \tInference time: 0.054850s\n",
      "Training and testing infi......done! Training time: 0.006982s\tInference time: 0.028866s\n",
      "Testing infi...\n",
      "...done! \tInference time: 0.058773s\n",
      "Total training time: 0.023938s\t AvgFPR: 0.036624\t AvgTPR: 0.972058\tTotal inference time: 0.227197s (fake: 0.114582s)\n"
     ]
    }
   ],
   "source": [
    "ensemble_df = pd.DataFrame()\n",
    "\n",
    "benign_train = benign_df[benign_df['is_train']==True]\n",
    "benign_test = benign_df[benign_df['is_test']==True]\n",
    "\n",
    "\n",
    "ens_time = 0\n",
    "ens_avgFPR = 0\n",
    "tot_TP = 0\n",
    "tot_P = 0\n",
    "ens_infer_time = 0\n",
    "fakeEns_infer_time = 0\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_train = {a}_df[{a}_df['is_train']==True]\")\n",
    "    exec(f\"{a}_test = {a}_df[{a}_df['is_test']==True]\")\n",
    "\n",
    "    exec(f\"train = pd.concat([benign_train, {a}_train])\")\n",
    "    exec(f\"test = pd.concat([benign_test, {a}_test])\")\n",
    "    \n",
    "    # We first train a classifier only on \"benign\" or on malicious samples of a specific attack. \n",
    "    # Afterwards, we immediately test it on a test-set having ONLY malicious samples of this specific attack\n",
    "    # Note: such \"testing\" is redundant, because it assumes that the classifier only receives the samples of the attack it is trained on!\n",
    "    exec(f\"{a}Clf, {a}Pred, {a}Result = develop_clf(train, test, features, clf_name='{a}', clf_type=base_clf, verbose=1)\")\n",
    "    exec(f\"fakeEns_infer_time += {a}Result.infer_time\")\n",
    "    # We now test the specific classifier on the ENTIRE test-set, thereby allowing to assess its performance also against malicious samples of different attacks\n",
    "    exec(f\"{a}_allPred, {a}_allResults, {a}Result.infer_time = evaluate_clf({a}Clf, all_test, features, clf_name='{a}', time={a}Result.time, verbose=1)\")\n",
    "\n",
    "    exec(f\"ensemble_df['{a}'] = {a}_allPred\")\n",
    "\n",
    "    exec(f\"tot_TP += ({a}Result.rec * len({a}_test))\")\n",
    "    exec(f\"tot_P += len({a}_test)\")\n",
    "\n",
    "\n",
    "    exec(f\"ens_avgFPR += {a}Result.fpr\")\n",
    "    exec(f\"ens_time += {a}Result.time\")\n",
    "    exec(f\"ens_infer_time +={a}Result.infer_time\")\n",
    "\n",
    "ens_avgFPR = ens_avgFPR / len(attack_names)\n",
    "ens_avgREC = tot_TP/tot_P\n",
    "\n",
    "print(\"Total training time: {:5f}s\\t AvgFPR: {:5f}\\t AvgTPR: {:5f}\\tTotal inference time: {:5f}s (fake: {:5f}s)\".\n",
    "      format(ens_time, ens_avgFPR, ens_avgREC, ens_infer_time, fakeEns_infer_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble (real assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we measure the combined performance of the entire ensemble\n",
    "# This is done with a logical or, or for majority voting (regulated by the \"agreement\" variable)\n",
    "\n",
    "ensemble_df[\"sum\"] = ensemble_df.sum(axis=1)\n",
    "#calculating \n",
    "ensemble_df[\"LOR\"] = (ensemble_df[\"sum\"]>0)\n",
    "\n",
    "#Appending Ground Truth\n",
    "temp = all_test['Nature'] #> 0)\n",
    "ensemble_df['True'] = ((temp.reset_index(drop=True)) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Logical OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 5528 out of 103494 (Recall: 0.973816\tFPR: 0.127420)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>24290</td>\n",
       "      <td>3547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1981</td>\n",
       "      <td>73676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  24290   3547\n",
       "True    1981  73676"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enslorResult = Result(ensemble_df['True'], ensemble_df['LOR'], ens_time, ens_infer_time)\n",
    "enslorErr= int(len(all_test) * (1-enslorResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(enslorErr, len(all_test), enslorResult.rec, enslorResult.fpr))\n",
    "enslorResult.ctab_bin # you can also try with enslorResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting: at least 2 out of 4 classifiers must agree that a sample is malicious.\n",
      "Total Misclassifications: 13552 out of 103494 (Recall: 0.827524\tFPR: 0.018069)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>27334</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>13049</td>\n",
       "      <td>62608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  27334    503\n",
       "True   13049  62608"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_agree = math.ceil(agreement * len(attack_names))\n",
    "print(\"Voting: at least {} out of {} classifiers must agree that a sample is malicious.\".format(min_agree, len(attack_names)))\n",
    "ensemble_df[\"MAJV\"] = (ensemble_df[\"sum\"]>=min_agree)\n",
    "ensvotResult = Result(ensemble_df['True'], ensemble_df['MAJV'], ens_time, ens_infer_time)\n",
    "ensvotErr = int(len(all_test) * (1-ensvotResult.acc))\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(ensvotErr, len(all_test), ensvotResult.rec, ensvotResult.fpr))\n",
    "ensvotResult.ctab_bin # you can also try with ensvotResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Stacked Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 5516 out of 103494 (Recall: 0.973605\tFPR: 0.126414)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24318</td>\n",
       "      <td>3519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997</td>\n",
       "      <td>73660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     24318   3519\n",
       "1      1997  73660"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "clf_list = []\n",
    "for a in attack_names:\n",
    "    exec(f\"clf_list.append({a}Clf)\")\n",
    "\n",
    "meta = choose_clf(clf_type=base_clf)\n",
    "sClf = StackingClassifier(classifiers=clf_list, meta_classifier=meta, fit_base_estimators=False, use_probas = False)\n",
    "s_timeStart = time.time()\n",
    "sClf.fit(all_train[features], all_train['Nature'])\n",
    "s_time = time.time() - s_timeStart + ens_time\n",
    "s_timeStart = time.time()\n",
    "sPred = sClf.predict(all_test[features])\n",
    "s_infer_time = time.time()-s_timeStart\n",
    "sResult = Result(all_test['Nature'], sPred, s_time, s_infer_time)\n",
    "if sResult.acc < sResult.acc_multi:\n",
    "    sResult.acc = sResult.acc_multi\n",
    "sErr = int(len(all_test) * (1-sResult.acc))\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sErr, len(all_test), sResult.rec, sResult.fpr))\n",
    "\n",
    "sResult.ctab_bin # you can also try with sResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open World Assessment: One attack against all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
      "      Binary CLF: TPR=0.561091\tFPR=0.109090\n",
      "      Multiclass (binarized) CLF: TPR=0.517770\tFPR=0.088632\n",
      "      EnsLOR CLF: TPR=0.770541\tFPR=0.100083\n",
      "      EnsVOT CLF: TPR=0.131183\tFPR=0.009538\n",
      "      EnsSTK CLF: TPR=0.767888\tFPR=0.098601\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "### The following code is a mixture of everything described insofar.\n",
    "### We are only focused on TPR and FPR here. We do not care about accuracy, adversarial robustness, or runtime.\n",
    "### These experiments are also done only on the \"Complete\" feature set\n",
    "\n",
    "\n",
    "oaac_bin_rec = 0\n",
    "oaac_bin_fpr = 0\n",
    "oaac_multi_rec = 0\n",
    "oaac_multi_fpr = 0\n",
    "oaac_enslor_rec = 0\n",
    "oaac_enslor_fpr = 0\n",
    "oaac_ensvot_rec = 0\n",
    "oaac_ensvot_fpr = 0\n",
    "oaac_ensstk_rec = 0\n",
    "oaac_ensstk_fpr = 0\n",
    "\n",
    "for u in attack_names: # u is the unknown attack\n",
    "    #print(u) # this is the unknown attack\n",
    "    #exec(f\"{u}_test = {u}_df[{u}_df['is_train']==False]\") # create test set by putting the \"test\" samples of u\n",
    "    exec(f\"{u}_test = pd.concat([benign_test, {u}_test])\") # add to the test set the \"benign\" test samples\n",
    "    exec(f\"{u}_train = benign_df[benign_df['is_train']==True]\") # compose the \"training\" set: start by putting the benign \"training\" samples\n",
    "    for a in attack_names: \n",
    "        # for every attack that is not u, add its training samples to the training set of u\n",
    "        if a==u:\n",
    "            continue\n",
    "        exec(f\"{u}_train = pd.concat([{u}_train, {a}_df[{a}_df['is_train']==True]])\")\n",
    "\n",
    "\n",
    "    # We have created the training and testing set. Now we must train and test a binary classifier by following the standard procedure\n",
    "    ########## BINARY CLASSIFIER ##########\n",
    "    exec(f\"{u}_oaac_bClf, {u}_oaac_bPred, {u}_oaac_bResult = develop_clf({u}_train, {u}_test, features, clf_name='{u}_oaac_bin', label='Nature', clf_type=base_clf)\")\n",
    "\n",
    "\n",
    "    ########## Multiclass CLASSIFIER ########## --> Train, then test only on binary\n",
    "    exec(f\"{u}_oaac_mClf, {u}_oaac_mPred, {u}_oaac_mResult = develop_clf({u}_train, {u}_test, features, clf_name='{u}_oaac_multi', label='Label_cat', clf_type=base_clf)\")\n",
    "    exec(f\"{u}_oaac_mPred_bin = np.copy({u}_oaac_mPred)\")\n",
    "    exec(f\"{u}_oaac_mPred_bin[{u}_oaac_mPred_bin > 0] = 1\")\n",
    "    exec(f\"{u}_oaac_mResult.bin_results({u}_test['Nature'], {u}_oaac_mPred_bin)\")\n",
    "\n",
    "\n",
    "    ######### Ensemble ##############\n",
    "    # send the samples in TEST to all the classifiers of the ensemble (which are already trained), aside from the one focusing on u\n",
    "    exec(f\"{u}_oaac_ens_df = pd.DataFrame()\")\n",
    "    for a in attack_names:    \n",
    "        if a==u:\n",
    "                continue\n",
    "        exec(f\"{a}_{u}Pred, {a}_{u}Results, {a}_{u}_infer_time = evaluate_clf({a}Clf, {u}_test, features, clf_name='{a}_{u}', time={a}Result.time)\")\n",
    "        exec(f\"{u}_oaac_ens_df['{a}'] = {a}_{u}Pred\")\n",
    "\n",
    "    # now we have the dataframe with all the predictions, let's see the aggregate results\n",
    "    exec(f\"{u}_oaac_ens_df['sum'] = {u}_oaac_ens_df.sum(axis=1)\")\n",
    "    exec(f\"{u}_oaac_ens_df['LOR'] = ({u}_oaac_ens_df['sum']>0)\")\n",
    "    exec(f\"temp = {u}_test['Nature'] #> 0)\")\n",
    "    exec(f\"{u}_oaac_ens_df['True'] = ((temp.reset_index(drop=True)) > 0)\")\n",
    "    exec(f\"{u}_oaac_enslorResult = Result({u}_oaac_ens_df['True'], {u}_oaac_ens_df['LOR'], (ens_time-{u}Result.time), (ens_infer_time-{u}Result.infer_time))\")\n",
    "\n",
    "    # now we consider the majority voting of the ensemble\n",
    "    exec(f\"{u}_oaac_ens_df['MAJV'] = ({u}_oaac_ens_df['sum']>=min_agree)\")\n",
    "    exec(f\"{u}_oaac_ensvotResult = Result({u}_oaac_ens_df['True'], {u}_oaac_ens_df['MAJV'], (ens_time-{u}Result.time), (ens_infer_time-{u}Result.infer_time))\")\n",
    "\n",
    "    # finally, let's use the stacking ensemble\n",
    "    exec(f\"{u}_clf_list = []\")\n",
    "    for a in attack_names:\n",
    "        if a==u:\n",
    "                continue\n",
    "        exec(f\"{u}_clf_list.append({a}Clf)\")\n",
    "    exec(f\"{u}_oaac_sClf = StackingClassifier(classifiers={u}_clf_list, meta_classifier=meta, fit_base_estimators=False, use_probas = False)\")\n",
    "    exec(f\"{u}_oaac_sClf.fit({u}_train[features], {u}_train['Nature'])\")\n",
    "    exec(f\"{u}_oaac_sPred = {u}_oaac_sClf.predict({u}_test[features])\")\n",
    "    exec(f\"{u}_oaac_sResult = Result({u}_test['Nature'], {u}_oaac_sPred, (ens_time-{u}Result.time), (ens_infer_time-{u}Result.infer_time))\")\n",
    "\n",
    "    # Updating results\n",
    "    exec(f\"oaac_bin_rec += {u}_oaac_bResult.rec\")\n",
    "    exec(f\"oaac_bin_fpr += {u}_oaac_bResult.fpr\")\n",
    "    exec(f\"oaac_multi_rec += {u}_oaac_mResult.rec\")\n",
    "    exec(f\"oaac_multi_fpr += {u}_oaac_mResult.fpr\")\n",
    "    exec(f\"oaac_enslor_rec += {u}_oaac_enslorResult.rec\")\n",
    "    exec(f\"oaac_enslor_fpr += {u}_oaac_enslorResult.fpr\")\n",
    "    exec(f\"oaac_ensvot_rec += {u}_oaac_ensvotResult.rec\")\n",
    "    exec(f\"oaac_ensvot_fpr += {u}_oaac_ensvotResult.fpr\")\n",
    "    exec(f\"oaac_ensstk_rec += {u}_oaac_sResult.rec\")\n",
    "    exec(f\"oaac_ensstk_fpr += {u}_oaac_sResult.fpr\")\n",
    "\n",
    "# Finalizing averages\n",
    "oaac_bin_rec /= len(attack_names)\n",
    "oaac_bin_fpr /= len(attack_names)\n",
    "oaac_multi_rec /= len(attack_names)\n",
    "oaac_multi_fpr /= len(attack_names)\n",
    "oaac_enslor_rec /= len(attack_names)\n",
    "oaac_enslor_fpr /= len(attack_names)\n",
    "oaac_ensvot_rec /= len(attack_names)\n",
    "oaac_ensvot_fpr /= len(attack_names)\n",
    "oaac_ensstk_rec /= len(attack_names)\n",
    "oaac_ensstk_fpr /= len(attack_names)\n",
    "\n",
    "\n",
    "print('''Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
    "      Binary CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      Multiclass (binarized) CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      EnsLOR CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      EnsVOT CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      EnsSTK CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      '''.format(oaac_bin_rec, oaac_bin_fpr,\n",
    "                 oaac_multi_rec, oaac_multi_fpr,\n",
    "                 oaac_enslor_rec, oaac_enslor_fpr,\n",
    "                 oaac_ensvot_rec, oaac_ensvot_fpr,\n",
    "                 oaac_ensstk_rec, oaac_ensstk_fpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment on Essential feature set (and Adversarial attacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BINARY CLASSIFIER (Essential features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing adv_bin......done! Training time: 0.004984s\tInference time: 0.023929s\n",
      "Total Misclassifications: 9846 out of 103494 (Recall: 0.959422\tFPR: 0.243453)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21060</td>\n",
       "      <td>6777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3070</td>\n",
       "      <td>72587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     21060   6777\n",
       "1      3070  72587"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_bClf, sma_bPred, sma_bResult = develop_clf(all_train, all_test, small_features, clf_name='adv_bin', label='Nature', clf_type=base_clf, verbose=1)\n",
    "sma_bErr = int(len(all_test) * (1-sma_bResult.acc))\n",
    "if (sma_bResult.acc == 0):\n",
    "    sma_bErr = int(len(all_test) * (1-sma_bResult.acc_multi))\n",
    "else:\n",
    "    sma_bErr = int(len(all_test) * (1-sma_bResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_bErr, len(all_test), sma_bResult.rec, sma_bResult.fpr))\n",
    "pd.crosstab(all_test['Nature'], sma_bPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial Attack against Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.784\n",
      "Adversarial Recall (attack): 0.979\n"
     ]
    }
   ],
   "source": [
    "# Note that the adversarial attacks only affect a subset of the initial set of malicious samples\n",
    "# Hence, we compute the classification performance also on this subset for a far comparison\n",
    "\n",
    "\n",
    "\n",
    "adv_bPred_base = sma_bClf.predict(mal_base[small_features])\n",
    "adv_bPred_adv = sma_bClf.predict(mal_adv[small_features])\n",
    "adv_bin_base_rec =  recall_score(mal_base['Nature'], adv_bPred_base, pos_label=1)\n",
    "adv_bin_adv_rec = recall_score(mal_adv['Nature'], adv_bPred_adv, pos_label=1)\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_bin_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_bin_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classifier - cascade (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing sma_mc......done! Training time: 0.004996s\tInference time: 0.016930s\n",
      "Total Misclassifications: 3462 out of 75657\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25248</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>18568</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>1119</td>\n",
       "      <td>1253</td>\n",
       "      <td>11607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      1      2      3      4\n",
       "True                            \n",
       "1     25248      0      0    994\n",
       "2         3  18568      0     33\n",
       "3         0      0  16771      0\n",
       "4        61   1119   1253  11607"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_mcClf, sma_mcPred, sma_mcResult = develop_clf(malicious_train, malicious_test, small_features, clf_name='sma_mc', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "sma_mcErr = int(len(malicious_test) * (1-sma_mcResult.acc_multi))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {}\".format(sma_mcErr, len(malicious_test)))\n",
    "pd.crosstab(malicious_test['Label_cat'], sma_mcPred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications (among the malicious samples): 3061 out of 72587\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>18463</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16747</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>966</td>\n",
       "      <td>1243</td>\n",
       "      <td>9083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      1      2      3     4\n",
       "True                           \n",
       "1     25233      0      0   787\n",
       "2         3  18463      0     8\n",
       "3         0      0  16747     0\n",
       "4        54    966   1243  9083"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We select the samples flagged as malicious by the initial classifier.\n",
    "# Of course, samples flagged as malicious that are NOT actually malicious will always be misclassified\n",
    "\n",
    "all_test['sma_bPred'] = sma_bPred\n",
    "sma_mc_test = all_test[(all_test['sma_bPred']==1) & (all_test['Nature']==1)]\n",
    "if (len(sma_mc_test)==0):\n",
    "    # in this case, this classifier receives nothing\n",
    "    print(\"There is no malicious sample flagged as malicious to analyze!\")\n",
    "\n",
    "sma_mcPred_m = sma_mcClf.predict(sma_mc_test[small_features])\n",
    "sma_mcResult.acc_multic = accuracy_score(sma_mc_test['Label_cat'], sma_mcPred_m, normalize=True, sample_weight=None)\n",
    "sma_mcErr_m = int((1-sma_mcResult.acc_multic) * len(sma_mc_test))\n",
    "print(\"Total Misclassifications (among the malicious samples): {} out of {}\".format(sma_mcErr_m, len(sma_mc_test)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.crosstab(sma_mc_test['Label_cat'], sma_mcPred_m, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This classifier also analyzed 6776 benign samples that were incorrectly labelled as 'malicious' by the (small) binary classifier\n",
      "Hence, this (small) classifier was tested on 79363 samples, of which 9837 have been misclassified\n"
     ]
    }
   ],
   "source": [
    "## Note: We also accounted for the false positives of the first binary classifier (all of which have been considered as misclassifications)\n",
    "sma_bin_falsePositives = int(sma_bResult.fpr * len(benign_test))\n",
    "print(\"This classifier also analyzed {} benign samples that were incorrectly labelled as 'malicious' by the (small) binary classifier\".format(sma_bin_falsePositives))\n",
    "\n",
    "print(\"Hence, this (small) classifier was tested on {} samples, of which {} have been misclassified\".format(len(sma_mc_test)+sma_bin_falsePositives, sma_bin_falsePositives+sma_mcErr_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classifier - stand-alone (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing sma_m......done! Training time: 0.004992s\tInference time: 0.024917s\n",
      "Total Misclassifications: 14080 out of 103494\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19966</td>\n",
       "      <td>279</td>\n",
       "      <td>182</td>\n",
       "      <td>484</td>\n",
       "      <td>6926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74</td>\n",
       "      <td>25231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>350</td>\n",
       "      <td>3</td>\n",
       "      <td>18244</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3653</td>\n",
       "      <td>84</td>\n",
       "      <td>12</td>\n",
       "      <td>1067</td>\n",
       "      <td>9224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1      2      3     4\n",
       "True                                  \n",
       "0     19966    279    182    484  6926\n",
       "1        74  25231      0      0   937\n",
       "2       350      3  18244      0     7\n",
       "3        23      0      0  16748     0\n",
       "4      3653     84     12   1067  9224"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_mClf, sma_mPred, sma_mResult = develop_clf(all_train, all_test, small_features, clf_name='sma_m', label='Label_cat', clf_type=base_clf, verbose=1)\n",
    "sma_mErr = int(len(all_test) * (1-sma_mResult.acc_multi))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {}\".format(sma_mErr, len(all_test)))\n",
    "sma_mResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Classifier: Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 11970 out of 103494 (Recall: 0.945808\tFPR: 0.282753)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19966</td>\n",
       "      <td>7871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4100</td>\n",
       "      <td>71557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     19966   7871\n",
       "1      4100  71557"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## MULTI-CLASS CLASSIFIER - BINARY ##########\n",
    "sma_mPred_bin = np.copy(sma_mPred)\n",
    "sma_mPred_bin[sma_mPred_bin > 0] = 1\n",
    "sma_mResult.bin_results(all_test['Nature'], sma_mPred_bin)\n",
    "sma_mErr_bin = int(len(all_test) * (1-sma_mResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_mErr_bin, len(all_test), sma_mResult.rec, sma_mResult.fpr))\n",
    "sma_mResult.ctab_bin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial Attack against the Multiclass Classifier (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.796\n",
      "Adversarial Recall (attack): 0.011\n"
     ]
    }
   ],
   "source": [
    "adv_mPred_base = sma_mClf.predict(mal_base[small_features])\n",
    "adv_mPred_adv = sma_mClf.predict(mal_adv[small_features])\n",
    "\n",
    "adv_mPred_base_bin = np.copy(adv_mPred_base)\n",
    "adv_mPred_base_bin[adv_mPred_base_bin > 0] = 1\n",
    "\n",
    "adv_mPred_adv_bin = np.copy(adv_mPred_adv)\n",
    "adv_mPred_adv_bin[adv_mPred_adv_bin > 0] = 1\n",
    "\n",
    "\n",
    "adv_multi_base_rec =  recall_score(mal_base['Nature'], adv_mPred_base_bin)\n",
    "adv_multi_adv_rec = recall_score(mal_adv['Nature'], adv_mPred_adv_bin)\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_multi_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_multi_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Classifiers (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing sma_ddos......done! Training time: 0.003978s\tInference time: 0.009974s\n",
      "Testing sma_ddos...\n",
      "...done! \tInference time: 0.019971s\n",
      "Training and testing sma_bot......done! Training time: 0.003985s\tInference time: 0.009011s\n",
      "Testing sma_bot...\n",
      "...done! \tInference time: 0.020892s\n",
      "Training and testing sma_brute......done! Training time: 0.002961s\tInference time: 0.008964s\n",
      "Testing sma_brute...\n",
      "...done! \tInference time: 0.019968s\n",
      "Training and testing sma_infi......done! Training time: 0.004023s\tInference time: 0.009931s\n",
      "Testing sma_infi...\n",
      "...done! \tInference time: 0.024917s\n",
      "Total training time: 0.014947s\t AvgFPR: 0.067383\t AvgTPR: 0.933595\tTotal inference time: 0.085748s (fake: 0.037879s)\n"
     ]
    }
   ],
   "source": [
    "sma_ensemble_df = pd.DataFrame()\n",
    "adv_ensemble_df_base = pd.DataFrame()\n",
    "adv_ensemble_df_adv = pd.DataFrame()\n",
    "\n",
    "benign_train = benign_df[benign_df['is_train']==True]\n",
    "benign_test = benign_df[benign_df['is_test']==True]\n",
    "\n",
    "\n",
    "sma_ens_time = 0\n",
    "sma_ens_avgFPR = 0\n",
    "sma_tot_TP = 0\n",
    "sma_tot_P = 0\n",
    "sma_ens_infer_time = 0\n",
    "sma_fakeEns_infer_time = 0\n",
    "\n",
    "for a in attack_names:\n",
    "    exec(f\"{a}_train = {a}_df[{a}_df['is_train']==True]\")\n",
    "    exec(f\"{a}_test = {a}_df[{a}_df['is_test']==True]\")\n",
    "\n",
    "    exec(f\"train = pd.concat([benign_train, {a}_train])\")\n",
    "    exec(f\"test = pd.concat([benign_test, {a}_test])\")\n",
    "\n",
    "    exec(f\"sma_{a}Clf, sma_{a}Pred, sma_{a}Result = develop_clf(train, test, small_features, clf_name='sma_{a}', clf_type=base_clf, verbose=1)\")\n",
    "    exec(f\"sma_fakeEns_infer_time += sma_{a}Result.infer_time\")\n",
    "    \n",
    "    exec(f\"sma_{a}_allPred, sma_{a}_allResults, sma_{a}Result.infer_time = evaluate_clf(sma_{a}Clf, all_test, small_features, clf_name='sma_{a}', time=sma_{a}Result.time, verbose=1)\")\n",
    "    exec(f\"sma_ensemble_df['{a}'] = sma_{a}_allPred\")\n",
    "\n",
    "    exec(f\"adv_{a}Pred_base = sma_{a}Clf.predict(mal_base[small_features])\")\n",
    "    exec(f\"adv_{a}Pred_adv = sma_{a}Clf.predict(mal_adv[small_features])\")\n",
    "    exec(f\"adv_ensemble_df_base['{a}'] = adv_{a}Pred_base\")\n",
    "    exec(f\"adv_ensemble_df_adv['{a}'] = adv_{a}Pred_adv\")\n",
    "\n",
    "\n",
    "\n",
    "    exec(f\"sma_tot_TP += (sma_{a}Result.rec * len({a}_test))\")\n",
    "    exec(f\"sma_tot_P += len({a}_test)\")\n",
    "\n",
    "\n",
    "    exec(f\"sma_ens_avgFPR += sma_{a}Result.fpr\")\n",
    "    exec(f\"sma_ens_time += sma_{a}Result.time\")\n",
    "    exec(f\"sma_ens_infer_time +=sma_{a}Result.infer_time\")\n",
    "\n",
    "\n",
    "\n",
    "sma_ens_avgFPR = sma_ens_avgFPR / len(attack_names)\n",
    "sma_ens_avgREC = sma_tot_TP/sma_tot_P\n",
    "\n",
    "\n",
    "print(\"Total training time: {:5f}s\\t AvgFPR: {:5f}\\t AvgTPR: {:5f}\\tTotal inference time: {:5f}s (fake: {:5f}s)\".\n",
    "      format(sma_ens_time, sma_ens_avgFPR, sma_ens_avgREC, sma_ens_infer_time, sma_fakeEns_infer_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing real Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma_ensemble_df[\"sum\"] = sma_ensemble_df.sum(axis=1)\n",
    "sma_ensemble_df[\"LOR\"] = (sma_ensemble_df[\"sum\"]>0)\n",
    "temp = all_test['Nature'] \n",
    "sma_ensemble_df['True'] = ((temp.reset_index(drop=True)) > 0)\n",
    "\n",
    "adv_ensemble_df_base[\"sum\"] = adv_ensemble_df_base.sum(axis=1)\n",
    "adv_ensemble_df_base[\"LOR\"] = (adv_ensemble_df_base[\"sum\"]>0)\n",
    "temp = mal_base['Nature']\n",
    "adv_ensemble_df_base['True'] = ((temp.reset_index(drop=True)) > 0)\n",
    "\n",
    "\n",
    "adv_ensemble_df_adv[\"sum\"] = adv_ensemble_df_adv.sum(axis=1)\n",
    "adv_ensemble_df_adv[\"LOR\"] = (adv_ensemble_df_adv[\"sum\"]>0)\n",
    "temp = mal_adv['Nature']\n",
    "adv_ensemble_df_adv['True'] = ((temp.reset_index(drop=True)) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Logical OR (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 10356 out of 103494 (Recall: 0.950236\tFPR: 0.236807)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>21245</td>\n",
       "      <td>6592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>3765</td>\n",
       "      <td>71892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  21245   6592\n",
       "True    3765  71892"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_enslorResult = Result(sma_ensemble_df['True'], sma_ensemble_df['LOR'], sma_ens_time, sma_ens_infer_time)\n",
    "sma_enslorErr= int(len(all_test) * (1-sma_enslorResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_enslorErr, len(all_test), sma_enslorResult.rec, sma_enslorResult.fpr))\n",
    "sma_enslorResult.ctab_bin # you can also try with sma_enslorResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logical OR: Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.773\n",
      "Adversarial Recall (attack): 0.002\n"
     ]
    }
   ],
   "source": [
    "adv_enslor_base_rec = recall_score(mal_base['Nature'], adv_ensemble_df_base[\"LOR\"])\n",
    "adv_enslor_adv_rec = recall_score(mal_adv['Nature'], adv_ensemble_df_adv[\"LOR\"])\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_enslor_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_enslor_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Majority Voting (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 44393 out of 103494 (Recall: 0.425275\tFPR: 0.032726)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>26926</td>\n",
       "      <td>911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>43482</td>\n",
       "      <td>32175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred   False  True \n",
       "True               \n",
       "False  26926    911\n",
       "True   43482  32175"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_ensemble_df[\"MAJV\"] = (sma_ensemble_df[\"sum\"]>=min_agree)\n",
    "sma_ensvotResult = Result(sma_ensemble_df['True'], sma_ensemble_df['MAJV'], sma_ens_time, sma_ens_infer_time)\n",
    "sma_ensvotErr = int(len(all_test) * (1-sma_ensvotResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_ensvotErr, len(all_test), sma_ensvotResult.rec, sma_ensvotResult.fpr))\n",
    "sma_ensvotResult.ctab_bin # you can also try with sma_ensvotResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Majority Voting: Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.000\n",
      "Adversarial Recall (attack): 0.000\n"
     ]
    }
   ],
   "source": [
    "adv_ensemble_df_base[\"MAJV\"] = (adv_ensemble_df_base[\"sum\"]>=min_agree)\n",
    "adv_ensemble_df_adv[\"MAJV\"] = (adv_ensemble_df_adv[\"sum\"]>=min_agree)\n",
    "\n",
    "adv_ensvot_base_rec = recall_score(mal_base['Nature'], adv_ensemble_df_base[\"MAJV\"])\n",
    "adv_ensvot_adv_rec = recall_score(mal_adv['Nature'], adv_ensemble_df_adv[\"MAJV\"])\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_ensvot_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_ensvot_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: Stacked Classifier (essential feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Misclassifications: 10356 out of 103494 (Recall: 0.950236\tFPR: 0.236807)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21245</td>\n",
       "      <td>6592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3765</td>\n",
       "      <td>71892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0      1\n",
       "True              \n",
       "0     21245   6592\n",
       "1      3765  71892"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_list = []\n",
    "for a in attack_names:\n",
    "    exec(f\"clf_list.append(sma_{a}Clf)\")\n",
    "\n",
    "sma_sClf = StackingClassifier(classifiers=clf_list, meta_classifier=meta, fit_base_estimators=False, use_probas = False)\n",
    "s_timeStart = time.time()\n",
    "sma_sClf.fit(all_train[small_features], all_train['Nature'])\n",
    "sma_s_time = time.time() - s_timeStart + sma_ens_time\n",
    "s_timeStart = time.time()\n",
    "sma_sPred = sma_sClf.predict(all_test[small_features])\n",
    "sma_s_infer_time = time.time()-s_timeStart\n",
    "sma_sResult = Result(all_test['Nature'], sma_sPred, sma_s_time, sma_s_infer_time)\n",
    "if sma_sResult.acc < sma_sResult.acc_multi:\n",
    "    sma_sResult.acc = sma_sResult.acc_multi\n",
    "sma_sErr = int(len(all_test) * (1-sma_sResult.acc))\n",
    "\n",
    "print(\"Total Misclassifications: {} out of {} (Recall: {:5f}\\tFPR: {:5f})\".format(sma_sErr, len(all_test), sma_sResult.rec, sma_sResult.fpr))\n",
    "\n",
    "sma_sResult.ctab_bin # you can also try with sma_sResult.ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked Classifier: Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Recall (baseline): 0.773\n",
      "Adversarial Recall (attack): 0.002\n"
     ]
    }
   ],
   "source": [
    "adv_sPred_base = sma_sClf.predict(mal_base[small_features])\n",
    "adv_sPred_adv = sma_sClf.predict(mal_adv[small_features])\n",
    "\n",
    "adv_ensstk_base_rec =  recall_score(mal_base['Nature'], adv_sPred_base)\n",
    "adv_ensstk_adv_rec = recall_score(mal_adv['Nature'], adv_sPred_adv)\n",
    "\n",
    "print(\"Adversarial Recall (baseline): {:.3f}\".format(adv_ensstk_base_rec))\n",
    "print(\"Adversarial Recall (attack): {:.3f}\".format(adv_ensstk_adv_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now inspect the results by referring to the \"Result\" variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR 0.9763670248622071 0.9547298994144626 0.9720581043393208 0.9738160381722775 0.8275242211560068 0.9736045574104181 \n",
      "FPR 0.13456909868161082 0.12343284118259867 0.03662391780723498 0.12742033983547074 0.018069475877429353 0.12641448431943092 \n",
      "Training Time 0.009556770324707031 0.007973432540893555 0.023938417434692383 0.023938417434692383 0.023938417434692383 0.03489351272583008 \n",
      "Inference Time 0.06473207473754883 0.05780768394470215 0.11458158493041992 0.22719693183898926 0.22719693183898926 0.16245579719543457 \n",
      "Accuracy 0.9296770827294336 0.9337063018145979 0.9894813791983105 0.9875755052407577\n"
     ]
    }
   ],
   "source": [
    "## BASELINE RESULTS (on Complete Feature Set)\n",
    "print(\n",
    "    \"TPR\",\n",
    "    bResult.rec,\n",
    "    mResult.rec,\n",
    "    ens_avgREC,\n",
    "    enslorResult.rec,\n",
    "    ensvotResult.rec,\n",
    "    sResult.rec,\n",
    "    \"\\nFPR\",\n",
    "    bResult.fpr,\n",
    "    mResult.fpr,\n",
    "    ens_avgFPR,\n",
    "    enslorResult.fpr,\n",
    "    ensvotResult.fpr,\n",
    "    sResult.fpr,\n",
    "    \"\\nTraining Time\",\n",
    "    bResult.time,\n",
    "    mResult.time,\n",
    "    ens_time,\n",
    "    enslorResult.time,\n",
    "    ensvotResult.time,\n",
    "    sResult.time,\n",
    "    \"\\nInference Time\",\n",
    "    bResult.infer_time,\n",
    "    mResult.infer_time,\n",
    "    fakeEns_infer_time,\n",
    "    enslorResult.infer_time,\n",
    "    ensvotResult.infer_time,\n",
    "    sResult.infer_time, \n",
    "    \"\\nAccuracy\",\n",
    "    mResult.acc_multi,   # This is the accuracy on the multiclassification\n",
    "    mResult.acc,         # This is the accuracy on the binary classification\n",
    "    mcResult.acc_multic, # This is the accuracy on the multiclassification AFTER the output of the binary classifier (it does not account for benign samples, which are false positives)\n",
    "    mcResult.acc_multi   # This is the accuracy on the multiclassification on the whole test portion of the malicious dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR 0.9594221288182191 0.9458080547735174 0.9335950407761344 0.9502359332249495 0.4252745945517269 0.9502359332249495 \n",
      "FPR 0.24345295829291946 0.28275317024104607 0.06738333872184502 0.23680712720479935 0.03272622768258071 0.23680712720479935 \n",
      "Training Time 0.004983663558959961 0.0049915313720703125 0.014947175979614258 0.014947175979614258 0.014947175979614258 0.021923065185546875 \n",
      "Inference Time 0.02392888069152832 0.024917125701904297 0.037878990173339844 0.08574819564819336 0.08574819564819336 0.07175970077514648 \n",
      "Accuracy 0.8639438035055172 0.8843314588285311 0.9578299144474905 0.9542276326050465\n"
     ]
    }
   ],
   "source": [
    "## BASELINE RESULTS (on Essential Feature Set)\n",
    "print(\n",
    "    \"TPR\",\n",
    "    sma_bResult.rec,\n",
    "    sma_mResult.rec,\n",
    "    sma_ens_avgREC,\n",
    "    sma_enslorResult.rec,\n",
    "    sma_ensvotResult.rec,\n",
    "    sma_sResult.rec,\n",
    "    \"\\nFPR\",\n",
    "    sma_bResult.fpr,\n",
    "    sma_mResult.fpr,\n",
    "    sma_ens_avgFPR,\n",
    "    sma_enslorResult.fpr,\n",
    "    sma_ensvotResult.fpr,\n",
    "    sma_sResult.fpr,\n",
    "    \"\\nTraining Time\",\n",
    "    sma_bResult.time,\n",
    "    sma_mResult.time,\n",
    "    sma_ens_time,\n",
    "    sma_enslorResult.time,\n",
    "    sma_ensvotResult.time,\n",
    "    sma_sResult.time,\n",
    "    \"\\nInference Time\",\n",
    "    sma_bResult.infer_time,\n",
    "    sma_mResult.infer_time,\n",
    "    sma_fakeEns_infer_time,\n",
    "    sma_enslorResult.infer_time,\n",
    "    sma_ensvotResult.infer_time,\n",
    "    sma_sResult.infer_time, \n",
    "    \"\\nAccuracy\",\n",
    "    sma_mResult.acc_multi,   # This is the accuracy on the multiclassification\n",
    "    sma_mResult.acc,         # This is the accuracy on the binary classification\n",
    "    sma_mcResult.acc_multic, # This is the accuracy on the multiclassification AFTER the output of the binary classifier (it does not account for benign samples, which are false positives)\n",
    "    sma_mcResult.acc_multi   # This is the accuracy on the multiclassification on the whole test portion of the malicious dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
      "      BD: TPR=0.561091\tFPR=0.109090\n",
      "      MD (binarized) CLF: TPR=0.517770\tFPR=0.088632\n",
      "      ED-o: TPR=0.770541\tFPR=0.100083\n",
      "      ED-v: TPR=0.131183\tFPR=0.009538\n",
      "      ED-s: TPR=0.767888\tFPR=0.098601\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "## Open World: One attack against all (averaged results)\n",
    "\n",
    "print('''Open World assessment: performance against one unknown attack (averaged for all attacks in the dataset)\n",
    "      BD: TPR={:5f}\\tFPR={:5f}\n",
    "      MD (binarized) CLF: TPR={:5f}\\tFPR={:5f}\n",
    "      ED-o: TPR={:5f}\\tFPR={:5f}\n",
    "      ED-v: TPR={:5f}\\tFPR={:5f}\n",
    "      ED-s: TPR={:5f}\\tFPR={:5f}\n",
    "      '''.format(oaac_bin_rec, oaac_bin_fpr,\n",
    "                 oaac_multi_rec, oaac_multi_fpr,\n",
    "                 oaac_enslor_rec, oaac_enslor_fpr,\n",
    "                 oaac_ensvot_rec, oaac_ensvot_fpr,\n",
    "                 oaac_ensstk_rec, oaac_ensstk_fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BD (before, after): 0.7837155480749001 0.9787502629917947 \n",
      "MD  (before, after): 0.7963391542183884 0.011361245529139492 \n",
      "ED-o  (before, after): 0.7734062697243846 0.0023143277929728594 \n",
      "ED-v  (before, after): 0.0 0.0 \n",
      "ED-s  (before, after): 0.7734062697243846 0.0023143277929728594\n"
     ]
    }
   ],
   "source": [
    "## Adversarial Attacks \n",
    "\n",
    "print(\n",
    "    \"BD (before, after):\",\n",
    "    adv_bin_base_rec,\n",
    "    adv_bin_adv_rec,\n",
    "    \"\\nMD  (before, after):\",\n",
    "    adv_multi_base_rec,\n",
    "    adv_multi_adv_rec,\n",
    "    \"\\nED-o  (before, after):\",\n",
    "    adv_enslor_base_rec,\n",
    "    adv_enslor_adv_rec, \n",
    "    \"\\nED-v  (before, after):\",\n",
    "    adv_ensvot_base_rec,\n",
    "    adv_ensvot_adv_rec, \n",
    "    \"\\nED-s  (before, after):\",\n",
    "    adv_ensstk_base_rec, \n",
    "    adv_ensstk_adv_rec,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
